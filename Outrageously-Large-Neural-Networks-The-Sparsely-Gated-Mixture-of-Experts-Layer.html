<!DOCTYPE html>
<html lang="en-us">
  
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Outrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer &middot; Papers I Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/poole.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/syntax.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/lanyon.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/style.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/font-awesome-4.7.0/css/font-awesome.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://shagunsodhani.github.io/papers-I-read/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://shagunsodhani.github.io/papers-I-read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68140113-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68140113-4');
</script>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>I am trying a new initiative - <i>A Paper A Week</i>. This blog will hold all the notes and summaries.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/archieve">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/tags">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read/archive/v1.0.0.zip">Download</a> -->
    <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read">GitHub project</a>
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/atom.xml">Feed</a>
    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://shagunsodhani.github.io/papers-I-read/" title="Home">Papers I Read</a>
            <small>Notes and Summaries</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Outrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer</h1>
  <p class="entry-tags"><a href="https://shagunsodhani.github.io/papers-I-read/tags.html#2017" title="Pages tagged 2017" rel="tag">2017</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Conditional+Computation" title="Pages tagged Conditional Computation" rel="tag">Conditional Computation</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Distributed+Computing" title="Pages tagged Distributed Computing" rel="tag">Distributed Computing</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#ICLR+2017" title="Pages tagged ICLR 2017" rel="tag">ICLR 2017</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Mixture+of+Experts" title="Pages tagged Mixture of Experts" rel="tag">Mixture of Experts</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#AI" title="Pages tagged AI" rel="tag">AI</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Gating" title="Pages tagged Gating" rel="tag">Gating</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#ICLR" title="Pages tagged ICLR" rel="tag">ICLR</a></p>
  <span class="post-date">14 Aug 2020</span>
  <h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Conditional computation is a technique to increase a model’s capacity (without a proportional increase in computation) by activating parts of the network on a per example basis.</p>
  </li>
  <li>
    <p>The paper describes (and address) the computational and algorithmic challenges in conditional computation. It introduces a sparsely-gated Mixture-of-Experts layer (MoE) with 1000s of feed-forward sub-networks.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1701.06538">Link to the paper</a></p>
  </li>
</ul>

<h2 id="practical-challenges">Practical Challenges</h2>

<ul>
  <li>
    <p>GPUs are fast at matrix arithmetic but slow at branching.</p>
  </li>
  <li>
    <p>Large batch sizes amortizes the cost of updates. Conditional computation reduces the effective batch size for different components of the model.</p>
  </li>
  <li>
    <p>Network bandwidth can be a bottleneck with the network demand overshadowing the computational demand.</p>
  </li>
  <li>
    <p>Additional losses may be needed to achieve the desired level of sparsity.</p>
  </li>
  <li>
    <p>Conditional computation is most useful for large datasets.</p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p><em>n</em> Expert Networks - $E_1$, …, $E_n$.</p>
  </li>
  <li>
    <p>Gating Network $G$ to select a sparse combination of experts.</p>
  </li>
  <li>
    <p>Output of the MoE module is the weighted sum of predictions of experts (weighted by the output of the gate).</p>
  </li>
  <li>
    <p>If the gating network’s output is sparse, then some of the experts’ value does not have to be computed.</p>
  </li>
  <li>
    <p>In theory, one could use a hierarchical mixture of experts where a mixture of experts is trained at each level.</p>
  </li>
</ul>

<h3 id="choices-for-the-gating-network">Choices for the Gating Network</h3>

<ul>
  <li>
    <p>Softmax Gating</p>
  </li>
  <li>
    <p>Noisy top-k gating - Add tunable Gaussian noise to the output of softmax gating and retain only the top-k values. A second trainable weight matrix controls the amount of noise per component.</p>
  </li>
</ul>

<h2 id="addressing-performance-challenge">Addressing Performance Challenge</h2>

<ul>
  <li>
    <p>Shrinking Batch Problem</p>

    <ul>
      <li>
        <p>If the MoE selects <em>k</em> out of <em>n</em> experts, the effective batch size reduces by a factor of <em>k</em> / <em>n</em>.</p>
      </li>
      <li>
        <p>This reduction in batch size is accounted for by combining data parallelism (for standard layers and gasting networks) and model parallelism (for experts in MoE). Thus, with <em>d</em> devices, the batch size changes by a factor of (<em>k</em> x <em>d</em> ) / <em>n</em>.</p>
      </li>
      <li>
        <p>For hierarchical MoE, the primary gating network uses data parallelism while secondary MoEs use model parallelism.</p>
      </li>
      <li>
        <p>The paper considers LSTM models where the MoE is applied once the previous layer has finished. This increases the batch size (for the current MoE layer) by a factor equal to the number of unrolling timesteps.</p>
      </li>
      <li>
        <p>Network Bandwith limitations can be overcome by ensuring that the ratio of computation (of each expert) to the input and output size is greater than (or equal to) the ratio of computational to network capacity.</p>
      </li>
      <li>
        <p>Computational efficiency can be improved by using larger hidden layers (or more hidden layers).</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Balancing Expert Utilization</p>

    <ul>
      <li>
        <p>Importance of an expert (relative to a batch of training examples) is defined as the batchwise sum of the expert’s goal values.</p>
      </li>
      <li>
        <p>An additional loss, called importance loss, is added to encourage the experts to have equal importance.</p>
      </li>
      <li>
        <p>The importance loss is defined as the square of the coefficient of variation (of a set of importance values) multiplied by a (hand-tuned) scaling factor $w_{importance}$.</p>
      </li>
      <li>
        <p>In practice, an additional loss called $L_{load}$ might be needed to ensure that the different experts get equal load (along with equal importance).</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Datasets</p>

    <ul>
      <li>
        <p>Billon Word Language modeling Benchmark</p>
      </li>
      <li>
        <p>100 Billion word Google News Corpus</p>
      </li>
      <li>
        <p>Machine Translation datasets</p>

        <ul>
          <li>
            <p>Single Language Pairs - WMT’14 En to Fr (36M sentence pairs) and En to De (5M sentence pairs).</p>
          </li>
          <li>
            <p>Multilingual Machine Translation - large combine dataset of twelve language pairs.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>In all the setups, the proposed MoE models achieve significantly better results than the baseline models, at a lower computational cost.</p>
  </li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/2021-01-25-HyperNetworks">
            
            <small>11 Apr 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Energy-based-Models-for-Continual-Learning">
            Energy-based Models for Continual Learning
            <small>18 Jan 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism">
            GPipe - Easy Scaling with Micro-Batch Pipeline Parallelism
            <small>11 Jan 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://shagunsodhani.github.io/papers-I-read/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer"  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://papers-i-read.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

  </body>
</html>
