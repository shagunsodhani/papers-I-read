<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data.</p>
  </li>
  <li>
    <p>The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.07595">Paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/oval-group/smooth-topk">Code</a></p>
  </li>
</ul>

<h2 id="idea">Idea</h2>

<ul>
  <li>Inspired by the multi-loss SVMs, a surrogate loss (l<sub>k</sub>) is introduced that creates a margin between the ground truth and the kth largest score.</li>
</ul>

<p><img src="https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq1.png" alt="Equation 1" /></p>

<ul>
  <li>
    <p>Here <strong>s</strong> denotes the output of the classifier model to be learnt, <em>y</em> is the ground truth label, <em>s[p]</em> denotes the kth largest element of <strong>s</strong> and <strong>s\p</strong> denotes the vector <strong>s</strong> without <em>p</em>th element.</p>
  </li>
  <li>
    <p>This l<sub>k</sub> loss has two limitations:</p>

    <ul>
      <li>
        <p>It is continous but not differentiable in <em>s</em>.</p>
      </li>
      <li>
        <p>Its weak derivatives have at most 2-nonzero elements.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The loss can be reformulated by adding and subtracting the k-1 largest scores of <strong>s\y</strong> and <em>s<sub>y</sub></em> and by introducing a temperature parameter τ.</p>
  </li>
</ul>

<p><img src="https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq2.png" alt="Equation 2" /></p>

<h2 id="properties-of-lkτ">Properties of L<sub>kτ</sub></h2>

<ul>
  <li>
    <p>For any τ &gt; 0, L<sub>kτ</sub> is infinite-differentiable and has non-sparse gradients.</p>
  </li>
  <li>
    <p>Under mild conditions, L<sub>kτ</sub> apporachs l<sub>k</sub> (in a pointwise sense) as τ approaches to 0+<sup>+</sup>.</p>
  </li>
  <li>
    <p>It is an upper bound on the actual loss (up to a constant factor).</p>
  </li>
  <li>
    <p>It is a generalization of the cross-entropy loss for different values of k, and τ and higher margins.</p>
  </li>
</ul>

<h2 id="computational-challenges">Computational Challenges</h2>

<ul>
  <li>
    <p><em>nCk</em> number of terms needs to be evaluated for computing the loss for one sample (n is number of classes).</p>
  </li>
  <li>
    <p>Loss L<sub>kτ</sub> can be expressed in terms of elementary symmetric polynomials σ<sub>i</sub>(<strong>e</strong>) (sum of all products of i distinct elements of vector e). Thus the challenge is to compute σ<sub>k</sub> efficiently.</p>
  </li>
</ul>

<h3 id="forward-computation">Forward Computation</h3>

<ul>
  <li>
    <p>Compute σ<sub>k</sub>(<strong>e</strong>) where <strong>e</strong> is a n-dimensional vector and k« n and e[i]!=0 for all i.</p>
  </li>
  <li>
    <p>σ<sub>i</sub>(<em>e</em>) can be computed using the coefficients of the polynomial (X+e<sub>1</sub>)(X+e<sub>2</sub>)…(X+e<sub>n</sub>) by divide and conquer approach with polynomial multiplication.</p>
  </li>
  <li>
    <p>With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU.</p>
  </li>
  <li>
    <p>Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision.</p>
  </li>
</ul>

<h3 id="backward-computation">Backward computation</h3>

<ul>
  <li>
    <p>The backward pass uses optimizations like computing derivative of σ<sub>j</sub> with respect to e<sub>i</sub> in a recursive manner.</p>
  </li>
  <li>
    <p>Appendix of the paper describes these techniques in detail.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Experiments are performed on CIFAR-100 (with noise) and Imagenet.</p>
  </li>
  <li>
    <p>For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class).</p>
  </li>
  <li>
    <p>The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance.</p>
  </li>
</ul>
