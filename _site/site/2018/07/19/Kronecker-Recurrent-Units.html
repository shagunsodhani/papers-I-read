<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Recurrent Neural Networks have two key issues:</p>

    <ul>
      <li>
        <p><strong>Over parameterization</strong> which increases the time for training and inference.</p>
      </li>
      <li>
        <p><strong>Ill conditioned</strong> recurrent weight matrix which makes training difficult due to vanishing or exploding gradients.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1705.10142">Link to the paper</a></p>
  </li>
</ul>

<h2 id="related-work">Related Work</h2>

<h3 id="existing-solutions-for-overparameterization">Existing solutions for overparameterization</h3>

<ul>
  <li>
    <p>Low-rank decomposition.</p>
  </li>
  <li>
    <p>Training a neural network on the soft targets predicted by a big pre-trained network.</p>
  </li>
  <li>
    <p>Low-bit precision training.</p>
  </li>
  <li>
    <p>Hashing.</p>
  </li>
</ul>

<h3 id="existing-solutions-for-vanishing-and-exploding-gradients">Existing solutions for vanishing and exploding gradients</h3>

<ul>
  <li>
    <p>Gating mechanism like in LSTMs.</p>
  </li>
  <li>
    <p>Gradient Clipping.</p>
  </li>
  <li>
    <p>Orthogonal Weight Initialization.</p>
  </li>
  <li>
    <p>Parameterizing recurrent weight matrix.</p>
  </li>
</ul>

<h2 id="kru">KRU</h2>

<ul>
  <li>
    <p>Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.</p>
  </li>
  <li>
    <p>Vanishing and exploding gradients are taken care of by using a soft unitary constraint.</p>
  </li>
  <li>
    <p>Why not use strict unitary constraint:</p>

    <ul>
      <li>
        <p>Restricts the search space and makes learning process unstable.</p>

        <ul>
          <li>
            <p>Makes forgetting (irrelevant) information difficult.</p>
          </li>
          <li>
            <p>Relaxing the strict constraint has shown to improve the convergence speed and generalization performance.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>KRU can be easily plugged into RNNs, LSTMs and other variants.</p>
  </li>
  <li>
    <p>The recurrent matrix <em>W</em> is paramterized as a kronecker product of <em>F</em> matrices <em>W<sub>0</sub>, …, W<sub>F-1</sub></em> where each <em>W<sub>f</sub></em> is a complex matrix of shape <em>P<sub>f</sub> x Q<sub>f</sub></em> and the product of all <em>P<sub>f</sub></em> and producto of all <em>Q<sub>f</sub></em> are both equal to <em>N</em>.</p>
  </li>
  <li>
    <p>Why is <em>W</em> a complex matrix?</p>

    <ul>
      <li>
        <p>In the real space, the set of all unitary matrices have the determinant as 1 or -1.</p>

        <ul>
          <li>
            <p>Given that determinant is a continuous function, the unitary set in the real space is disconnected.</p>
          </li>
          <li>
            <p>The unitary set in the complex space is connected as its determinants are points on the unit circle.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="soft-unitary-constraint">Soft Unitary Constraint</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>A soft unitary constraint is introduced in the form of regularization term</td>
          <td> </td>
          <td>W<sub>f</sub><sup>H</sup>W<sub>f</sub> - I</td>
          <td> </td>
          <td><sup>2</sup> (per kronecker factored recurrent matrix).</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>If each of the Kronecker factors is unitary, the resulting matrix <em>W</em> would also be unitary.</p>
  </li>
  <li>
    <p>It is computationally inefficient to apply this constraint over the recurrent matrix <em>W</em> itself as the complexity of the regularizer is given as <em>O(N<sup>3</sup>)</em>.</p>
  </li>
  <li>Use of Kronecker factorisation makes it computationally feasible to use this regulariser.</li>
</ul>

<h2 id="experiment">Experiment</h2>

<ul>
  <li>
    <p>The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.</p>
  </li>
  <li>
    <p>For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.</p>
  </li>
  <li>
    <p>Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).</p>
  </li>
  <li>
    <p>Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.</p>
  </li>
  <li>
    <p>The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output. The recurrent dynamics should be implemented via a low capacity model.s per task.</p>
  </li>
</ul>
