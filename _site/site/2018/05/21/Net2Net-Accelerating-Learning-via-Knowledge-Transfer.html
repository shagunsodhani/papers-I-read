<h2 id="notes">Notes</h2>

<ul>
  <li>
    <p>The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).</p>
  </li>
  <li>
    <p>The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1511.05641">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/paengs/Net2Net">Link to an implementation</a></p>
  </li>
  <li>
    <p>The approach works as follows - Let us say that the teacher network was represented by the transformation <em>y = f(x, θ)</em> where <em>θ</em> refer to the parameters of the network. The task is to choose a new set of parameters <em>θ’</em> for the student network <em>g(x, θ’)</em> such that for all <em>x, f(x, θ) = g(x, θ’)</em></p>
  </li>
  <li>
    <p>To start, we can assume that <em>f</em> and <em>g</em> are composed of standard linear layers. Layer <em>i</em> and <em>i+1</em> are represented by weights <em>W<sub>mxn</sub><sup>i</sup></em> and <em>W<sub>nxp</sub><sup>i+1</sup></em></p>
  </li>
  <li>
    <p>We want to grow layer <em>i</em> to have <em>q</em> output units (where <em>q</em> &gt; <em>n</em>) and layer <em>i+1</em> to have <em>q</em> input units. The new weight matrix would be <em>U<sub>mxq</sub><sup>i</sup></em> and <em>U<sub>qxp</sub><sup>i+1</sup></em></p>
  </li>
  <li>
    <p>The first <em>q</em> columns (rows) of <em>W<sup>i</sup></em> (<em>W<sup>i+1</sup></em>) would be copied as it is into <em>U<sup>i</sup></em>(<em>U<sup>i+1</sup></em>).</p>
  </li>
  <li>
    <p>For filling the remaining <em>n-q</em> slots, columns (rows) would be sampled randomly from <em>W<sup>i</sup></em> (<em>W<sup>i+1</sup></em>).</p>
  </li>
  <li>
    <p>Finally, each layer in <em>U<sup>i</sup></em> is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.</p>
  </li>
  <li>
    <p>Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.</p>
  </li>
  <li>
    <p>The benefits of using this approach are the following:</p>

    <ul>
      <li>The newly created student network performs at least as good as the teacher network.</li>
      <li>Any changes to the network are guaranteed to be an improvement.</li>
      <li>It is safe to optimize all the parameters in the network.</li>
    </ul>
  </li>
  <li>
    <p>The variant discussed above is called the <strong>Net2WiderNet</strong> variant. There is another variant called<strong>Net2DeeperNet</strong> that enables the network to grow in depth.</p>
  </li>
  <li>
    <p>In that case, a new matrix, <em>U</em>, initialized as the identity matrix, is added to the network. Note that unlike the <strong>Net2WiderNet</strong>, this approach would not work with arbitrary activation function between the layers.</p>
  </li>
</ul>

<h2 id="strengths">Strengths</h2>

<ul>
  <li>
    <p>The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.</p>
  </li>
  <li>
    <p>The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.</p>
  </li>
</ul>

<h2 id="limitations">Limitations</h2>

<ul>
  <li>The function preserving transformations need to be worked out manually. Extra care needs to be taken when operations like concatenation or batch norm are present.</li>
</ul>
