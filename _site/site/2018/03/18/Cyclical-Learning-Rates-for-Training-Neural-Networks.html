<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Conventional wisdom says that when training neural networks, learning rate should monotonically decrease. This insight forms the basis of the different type of adaptive learning rates.</p>
  </li>
  <li>
    <p>Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.</p>
  </li>
  <li>
    <p>The paper also provides a simple approach to estimate the lower and upper bound for CLR.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1506.01186">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/bckenstler/CLR">Link to the implementation</a></p>
  </li>
</ul>

<h2 id="intution">Intution</h2>

<ul>
  <li>
    <p>Difficulty in minimizing the loss arises from saddle points and not from local minima. <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">[Ref]</a></p>
  </li>
  <li>
    <p>Increasing the learning rate allows for rapid traversal of saddle points.</p>
  </li>
  <li>
    <p>Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.</p>
  </li>
</ul>

<h2 id="parameter-estimation">Parameter Estimation</h2>

<ul>
  <li>
    <p>Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size</p>
  </li>
  <li>
    <p>step_size should be set to 2-10 times the number of iterations in an epoch.</p>
  </li>
  <li>
    <p>Estimating the CLR boundary values:</p>

    <ul>
      <li>
        <p>Run the model for several epochs while increasing the learning rate between the allowed low and high values.</p>
      </li>
      <li>
        <p>Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.</p>
      </li>
      <li>
        <p>This gives a good candidate value for upper and lower bound. Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound. But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="notes">Notes</h2>

<ul>
  <li>The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.</li>
  <li>The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results.</li>
</ul>
