<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p><strong>Problem Statement</strong>: Given an image, answer a given question about the image.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1505.02074">Link to the paper</a></p>
  </li>
  <li>
    <p><strong>Assumptions</strong>:</p>
    <ul>
      <li>The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.</li>
    </ul>
  </li>
</ul>

<h2 id="vis-lstm-model">VIS-LSTM Model</h2>

<ul>
  <li>Treat the input image as the first word in the question.</li>
  <li>Obtain the vector representation (skip-gram) for words in the question.</li>
  <li>Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.</li>
  <li>Keep image embedding frozen during training and use an LSTM to combine the word vectors.</li>
  <li>LSTM outputs are fed into a softmax layer which generates the answer.</li>
</ul>

<h2 id="dataset">Dataset</h2>

<ul>
  <li>DAtaset for QUestion Ansering on Real-world images (DAQUAR)
    <ul>
      <li>1300 images and 7000 questions with 37 object classes.</li>
      <li>Downside is that even guess work can yield good results.</li>
    </ul>
  </li>
  <li>The paper proposed an algorithm for generating questions using MS-COCO dataset.
    <ul>
      <li>Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.</li>
      <li><em>object</em> questions, <em>number</em> questions, <em>colour</em> questions and <em>location</em> questions can be generated by searching for nouns, numbers, colours and prepositions respectively.</li>
      <li>Resulting dataset has ~120K questions across above 4 semantic types.</li>
    </ul>
  </li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li>VIS+LSTM - explained above</li>
  <li>2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM</li>
  <li>IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).</li>
  <li>FULL - Simple average of above 2 models.</li>
</ul>

<h3 id="baseline">Baseline</h3>

<ul>
  <li>Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.</li>
  <li>Also includes a KNN model where the system finds the nearest (image, question) pair.</li>
</ul>

<h3 id="metrics">Metrics</h3>

<ul>
  <li>Accuracy</li>
  <li>Wu-Palmer similarity measure</li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.</li>
  <li>Some useful information seems to be lost when downsizing the VGG vectors.</li>
  <li>Fine tuning the word vectors helps with performance.</li>
  <li>Normalising CNN hidden image features into zero mean and unit variance leads to faster training.</li>
  <li>Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present</li>
</ul>
