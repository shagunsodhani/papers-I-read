<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p><a href="https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f">Sequence-to-Sequence models</a> have made abstract summarization viable but they still suffer from issues like <em>out of vocabulary</em> words and repetitive sentences.</p>
  </li>
  <li>
    <p>The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a <em>coverage</em> vector that keeps track of content that has already been summarized so as to discourage repetition.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1704.04368">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/abisee/pointer-generator">Code</a></p>
  </li>
</ul>

<h2 id="model">Model</h2>

<h3 id="pointer-generator-network">Pointer Generator Network</h3>

<ul>
  <li>
    <p>It is a hybrid model between the Sequence-to-Sequence network and <a href="https://shagunsodhani.in/papers-I-read/Pointer-Networks">Pointer Network</a> such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).</p>
  </li>
  <li>
    <p>Since the model can choose a word from the source vocabulary, the issue of <em>out of vocabulary</em> words is handled.</p>
  </li>
</ul>

<h3 id="coverage-mechanism">Coverage Mechanism</h3>

<ul>
  <li>
    <p>The model maintains a <em>coverage</em> vector which is the sum of attention distributions over all previous decoder timesteps.</p>
  </li>
  <li>
    <p>This <em>coverage</em> vector is fed as an input to the attention mechanism.</p>
  </li>
  <li>
    <p>A <em>coverage loss</em> is added to prevent the model from repeatedly attending to the same word.</p>
  </li>
  <li>
    <p>The idea is to capture how much coverage different words have already received from the attention mechanism.</p>
  </li>
</ul>

<h2 id="observation">Observation</h2>

<ul>
  <li>
    <p>Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.</p>
  </li>
  <li>
    <p>Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.</p>
  </li>
  <li>
    <p>The model is initially trained without coverage and then finetuned with the coverage loss.</p>
  </li>
  <li>
    <p>During training, the model first learns how to copy words and then how to generate words (p<sup>gen</sup> starts from 0.3 and converges to 0.53).</p>
  </li>
  <li>
    <p>During testing, the model strongly prefers copying over generating (p<sup>gen</sup> = 0.17).</p>
  </li>
  <li>
    <p>Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.</p>
  </li>
  <li>
    <p>The overall model is very simple, neat and interpretable and also performs well in practice.</p>
  </li>
</ul>
