<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.</p>
  </li>
  <li>
    <p>The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1806.08804">Link to the paper</a></p>
  </li>
</ul>

<h2 id="key-idea">Key Idea</h2>

<ul>
  <li>
    <p>CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.</p>
  </li>
  <li>
    <p>This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.</p>
  </li>
  <li>
    <p>DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.</p>
  </li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>Given a graph <em>G(A, F)</em>, where <em>A</em> is the adjacency matrix and <em>F</em> is the feature matrix.</p>
  </li>
  <li>
    <p>Given a permutation invariant GNN that follows the message passing architecture. The output of this GNN can be expressed as <em>Z = GNN(A, X)</em> where <em>X</em> is the current feature matrix.</p>
  </li>
  <li>
    <p>Goal is to stack <em>L</em> GNN layers on top of each other such that the <em>l<sup>th</sup></em> layer uses coarsened output from the  <em>(l-1)<sup>th</sup></em> layer.</p>
  </li>
  <li>
    <p>This coarsening operation uses a cluster assignment matrix <em>S</em>.</p>
  </li>
  <li>
    <p>The learned cluster assignment matrix at layer <em>l</em> is denoted at <em>S<sup>l</sup></em></p>
  </li>
  <li>
    <p>Given <em>S<sup>l</sup></em>, the embedding matrix for the <em>(l+1)<sup>th</sup></em> layer is given as <em>transpose(S<sub>l</sub>)Z<sub>l</sub></em> and adjancecy matrix is given by <em>transpose(S<sub>l</sub>)A<sub>l</sub>S<sub>l</sub></em></p>
  </li>
  <li>
    <p>A new GNN, called as GNN<sub>pool</sub> is used to produce the assignment matrix <em>S</em> by taking a softmax over <em>GNN<sub>pool</sub>(A<sup>l</sup>, X<sup>l</sup>)</em></p>
  </li>
  <li>
    <p>As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.</p>
  </li>
</ul>

<h2 id="auxiliary-losses">Auxiliary Losses</h2>

<ul>
  <li>
    <p>The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.</p>
  </li>
  <li>
    <p>Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.</p>
  </li>
  <li>
    <p>Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.</p>
  </li>
</ul>

<h2 id="baselines">Baselines</h2>

<ul>
  <li>GNN based models
    <ul>
      <li>GraphSage
        <ul>
          <li>Mean pooling</li>
          <li>Set2Set pooling</li>
          <li>Sort pooling</li>
        </ul>
      </li>
      <li>Structure2vec</li>
      <li>Edge conditioned filters in CNN</li>
      <li>PatchySan</li>
    </ul>
  </li>
  <li>Kernel based models
    <ul>
      <li>Graphlet, shortest path etc</li>
    </ul>
  </li>
</ul>

<h2 id="model-variants">Model Variants</h2>

<ul>
  <li>GraphSage
    <ul>
      <li>Mean pool + Diff pool (3 or 2 layers)</li>
    </ul>
  </li>
  <li>Structure2Vec + Diffpool</li>
  <li>Diffpool-Det
    <ul>
      <li>The assignment matrix <em>S</em> are generated using graph clustering algorithms.</li>
    </ul>
  </li>
  <li>Diffpool-NoLP
    <ul>
      <li>The link prediction objective function is turned off.</li>
    </ul>
  </li>
  <li>At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.</li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.</p>
  </li>
  <li>
    <p>In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time. But given that now there are 2 GNN models per layer, the size of the model should increase.</p>
  </li>
  <li>
    <p>DiffPool can capture hierarchical community structure even when trained on just the graph classification loss.</p>
  </li>
  <li>
    <p>One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity.</p>
  </li>
</ul>
