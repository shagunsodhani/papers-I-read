<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.</p>
  </li>
  <li>
    <p>This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.</p>
  </li>
  <li>
    <p>Here, MANN refers to neural networks that have an external memory. This includes Neural Turning Machines (NTMs) and excludes LSTMs.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1605.06065">Link to the paper</a></p>
  </li>
</ul>

<h2 id="meta-learning">Meta-Learning</h2>

<ul>
  <li>
    <p>In meta-learning, a learner is learning at two levels.</p>
  </li>
  <li>
    <p>The learner is shown a sequence of tasks D<sub>1</sub>, D<sub>2</sub>, …, D<sub>T</sub>.</p>
  </li>
  <li>
    <p>When it is training on one of the datasets (say D<sub>T</sub>), it learns to solve the current dataset.</p>
  </li>
  <li>
    <p>At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).</p>
  </li>
</ul>

<h2 id="mann--meta-learning">MANN + Meta Learning</h2>

<ul>
  <li>
    <p>Following are the desirable characteristics for a scalable, combined architecture:</p>

    <ul>
      <li>
        <p>Memory representation should be both stable and element-wise accessible.</p>
      </li>
      <li>
        <p>Number of model parameters should not be tied to the size of the memory.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="task-setup">Task Setup</h2>

<ul>
  <li>
    <p>In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).</p>
  </li>
  <li>
    <p>Each dataset is presented to the model in the form (x<sub>1</sub>, null), (x<sub>1</sub>, y<sub>0</sub>), …, (x<sub>t+1</sub>, y<sub>t</sub>) where y<sub>t</sub> is the correct label (or value) corresponding to the inpuit x<sub>t</sub>.</p>
  </li>
  <li>
    <p>Further, the data labels are shuffled from dataset to dataset.</p>
  </li>
  <li>
    <p>The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.</p>
  </li>
  <li>
    <p>The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.</p>
  </li>
  <li>
    <p>The paper uses NTM as the MANN with one modification.</p>
  </li>
  <li>
    <p>In the original formulation, the memories were addressed by both context and location. Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.</p>
  </li>
  <li>
    <p>A new access module - LRUA - Least Recent Used Access - is used to write to memory.</p>
  </li>
  <li>
    <p>LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information). This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.</p>
  </li>
</ul>

<h2 id="datasets">Datasets</h2>

<ul>
  <li>
    <p>Omniglot (classification)</p>
  </li>
  <li>
    <p>Sampled functions from Gaussian Processes</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.</p>
  </li>
  <li>
    <p>As baselines, following models were considered:</p>

    <ul>
      <li>Regular NTM</li>
      <li>LSTM</li>
      <li>Feedforward RNN</li>
      <li>Nearest Neighbour Classifier</li>
    </ul>
  </li>
  <li>
    <p>Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.</p>
  </li>
  <li>
    <p>For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.</p>
  </li>
  <li>
    <p>For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs.</p>
  </li>
</ul>

