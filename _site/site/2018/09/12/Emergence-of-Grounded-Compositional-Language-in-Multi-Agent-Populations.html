<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.</p>
  </li>
  <li>
    <p>The language is quite rudimentary and is essentially a sequence of abstract discrete symbols. But it does comprise of a defined vocabulary and syntax.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1703.04908">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>Cooperative, partially observable Markov game (multi-agent extension of MDP).</p>
  </li>
  <li>
    <p>All agents have identical action and observation spaces, use the same policy and receive a shared reward.</p>
  </li>
</ul>

<h3 id="grounded-communication-environment">Grounded Communication Environment</h3>

<ul>
  <li>
    <p>Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.</p>
  </li>
  <li>
    <p>The agents and the landmarks would occupy some location and would have some attributes (colour, shape).</p>
  </li>
  <li>
    <p>Within the environment, the agents can <em>go to</em> a location, <em>look</em> at a location or <em>do nothing</em>. Additionally, they can utter communication symbols c (from a shared vocabulary C). Agents themselves learn to assign a meaning to the symbols.</p>
  </li>
  <li>
    <p>Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.</p>
  </li>
  <li>
    <p>Goal for agent <em>i</em> consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.</p>
  </li>
  <li>
    <p>Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.</p>
  </li>
  <li>
    <p>For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).</p>
  </li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>A model of all agent and environment state dynamics is created over time and the return gradient is computed.</p>
  </li>
  <li>
    <p>Gumbel-Softmax distribution is used to obtain categorical word emission c.</p>
  </li>
  <li>
    <p>A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.</p>
  </li>
  <li>
    <p>Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.</p>
  </li>
  <li>
    <p>The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations. These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.</p>
  </li>
  <li>
    <p>In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.</p>
  </li>
</ul>

<h3 id="compositionality-and-vocabulary-size">Compositionality and Vocabulary Size</h3>

<ul>
  <li>
    <p>Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words. This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.</p>
  </li>
  <li>
    <p>Along the lines of rich gets richer dynamics, the communication symbol c’s are modelled as being generated by a Dirichlet process. The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.</p>
  </li>
  <li>
    <p>Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.</p>
  </li>
</ul>

<h2 id="case-i---agents-can-not-see-each-other">Case I - Agents can not see each other</h2>

<ul>
  <li>
    <p>Non-verbal communication is not possible.</p>
  </li>
  <li>
    <p>When trained with just 2 agents, symbols are assigned for each landmark and action.</p>
  </li>
  <li>
    <p>As the number of agents is increased, additional symbols are used to refer to agents.</p>
  </li>
  <li>
    <p>If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks. If distractor locations are added, the agents learn to ignore them.</p>
  </li>
</ul>

<h2 id="non-verbal-communication">Non-verbal communication</h2>

<ul>
  <li>
    <p>Agents are allowed to observe other agents’ position, gaze etc.</p>
  </li>
  <li>
    <p>Now the location can be pointed to using gaze.</p>
  </li>
  <li>
    <p>If gaze is disabled, the agent could indicate the goal landmark by moving to it.</p>
  </li>
  <li>
    <p>Basically even when the communication is disabled the agents can come up with strategies to complete the task.</p>
  </li>
</ul>
