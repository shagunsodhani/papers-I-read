<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes a simple and dataset-agnostic data augmentation mechanism called <em>mixup</em>.</p>
  </li>
  <li>
    <p><a href="">Link to the paper</a></p>
  </li>
  <li>
    <p>Consider two training examples, $(x_1, y_1)$ and $(y_1, y_2)$, where $x_1$ and $x_2$ are the datapoints and $y_1$ and $y_2$ are the labels.</p>
  </li>
  <li>
    <p>New training examples of the form $(\lambda \times x_1 + (1-\lambda) \times x_2, \lambda \times y_1 + (1-\lambda) \times y_2)$ are constructured by considering the linear interpolation of the datapoints and the labels. Here $\lambda \in [0, 1]$.</p>
  </li>
  <li>
    <p>$\lambda$ is sampled from a Beta distribution $Beta(\alpha, \alpha)$ where $\alpha \in (0, \infty)$.</p>
  </li>
  <li>
    <p>Setting $\lambda$ to 0 or 1 eliminates the effect of <em>mixup</em>.</p>
  </li>
  <li>
    <p>Mixup encourages the neural network to favor linear behavior between the training examples.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p><strong>Supervised Learning</strong></p>

    <ul>
      <li>
        <p>ImageNet for ResNet-50, ResNet-101 and ResNext-101.</p>
      </li>
      <li>
        <p>CIFAR10/CIFAR100 for PreAct ResNet-18, WideResNet-28-10 and DenseNet.</p>
      </li>
      <li>
        <p>Google command dataset for LeNet and VGG.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In all these setups, adding <em>mixup</em> improves the performance of the model.</p>
  </li>
  <li>
    <p><em>Mixup</em> makes the model more robust to noisy labels. Moreover, <em>mixup</em> + dropout improves over <em>mixup</em> alone. This hints that <em>mixup</em>’s benefits are complementary to those of dropout.</p>
  </li>
  <li>
    <p><em>Mixup</em> makes the network more robust to adversarial examples in both white-box and black-box settings (ImageNet + Resnet101).</p>
  </li>
  <li>
    <p><em>Mixup</em> also stabilizes the training of GANs by acting as a regularizer for the gradient of the discriminator.</p>
  </li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>
    <p>Convex combination of three or more examples (with weights sampled from a Dirichlet distribution) does not provide gains over the case of two examples.</p>
  </li>
  <li>
    <p>In the authors’ implementation, <em>mixup</em> is applied between images of the same batch (after shuffling).</p>
  </li>
  <li>
    <p>Interpolating only between inputs, with the same labels, did not lead to the same kind of gains as <em>mixup</em>.</p>
  </li>
</ul>
