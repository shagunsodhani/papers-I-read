<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposed a framework for joint modeling of labels and data by interpreting a discriminative classifier <em>p(y|x)</em> as an energy-based model <em>p(x, y)</em>.</p>
  </li>
  <li>
    <p>Joint modeling provides benefits like improved calibration (i.e., the predictive confidence should align with the miss classification rate), robustness, and out of order distribution.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1912.03263">Link to the paper</a></p>
  </li>
</ul>

<h2 id="motivation">Motivation</h2>

<ul>
  <li>
    <p>Consider a standard classifier $f_{\theta}(x)$ which produces a k-dimensional vector of logits.</p>
  </li>
  <li>
    <p>$p_{\theta}(y | x) = softmax(f_{\theta}(x)[y])$</p>
  </li>
  <li>
    <p>Uisng concepts from energy based models, we write $p_{\theta}(x, y) = \frac{exp(-E_{\theta}(x, y))}{Z_{\theta}}$ where $E_{\theta}(x, y) = -f_{\theta}(x)[y]$</p>
  </li>
  <li>
    <p>$p_{\theta}(x) = \sum_{y}{ \frac{exp(-E_{\theta}(x, y))}{Z_{\theta}}}$</p>
  </li>
  <li>
    <p>$E_{\theta}(x) = -LogSumExp_y(f_{\theta}(x)[y])$</p>
  </li>
  <li>
    <p>Note that in the standard discriminative setup, shiting the logits $f_{\theta}(x)$ does not affect the model but it affects $p_{\theta}(x)$.</p>
  </li>
  <li>
    <p>Computing $p_{\theta}(y | x)$ using $p_{\theta}(x, y)$ and $p_{\theta}(x)$ gives back the same softmax parameterization as before.</p>
  </li>
  <li>
    <p>This reinterpreted classifier is referred to as a Joint Energy-based Model (JEM).</p>
  </li>
</ul>

<h2 id="optimization">Optimization</h2>

<ul>
  <li>
    <p>The log-liklihood of the data can be factoized as $log p_{\theta}(x, y) = log p_{\theta}(x) + log p_{\theta}(y | x)$.</p>
  </li>
  <li>
    <p>The second factor can be trained using the standard CE loss. In contrast, the first factor can be trained using a sampler based on Stochastic Gradient Langevin Dynamics.</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<h3 id="hybrid-modelling">Hybrid Modelling</h3>

<ul>
  <li>
    <p>Datasets: CIFAR10, CIFAR100, SVHN.</p>
  </li>
  <li>
    <p>Metrics: Inception Score, Frechet Inception Distance</p>
  </li>
  <li>
    <p>JEM outperforms generative, discriminative, and hybrid models on both generative and discriminative tasks.</p>
  </li>
</ul>

<h3 id="calibration">Calibration</h3>

<ul>
  <li>
    <p>A calibrated classifier is the one where the predictive confidence aligns with the misclassification rate.</p>
  </li>
  <li>
    <p>Dataset: CIFAR100</p>
  </li>
  <li>
    <p>JEM improves calibration while retaining high accuracy.</p>
  </li>
</ul>

<h3 id="out-of-distribution-ood-detection">Out of Distribution (OOD) Detection</h3>

<ul>
  <li>
    <p>One way to detect OOD samples is to learn a density model that assigns a higher likelihood to in-distribution examples and lower likelihood to out of distribution examples.</p>
  </li>
  <li>
    <p>JEM consistently assigns a higher likelihood to in-distribution examples.</p>
  </li>
  <li>
    <p>The paper also proposes an alternate metric called <em>approximate mass</em> to detect OOD examples.</p>
  </li>
  <li>
    <p>The intuition is that a point could have likelihood but be impossible to sample because its surroundings have a very low density.</p>
  </li>
  <li>
    <p>On the other hand, the in-distribution data points would lie in a region of high probability mass.</p>
  </li>
  <li>
    <p>Hence the norm of the gradient of log density could provide a useful signal to detect OOD examples.</p>
  </li>
</ul>

<h3 id="robustness">Robustness</h3>

<ul>
  <li>JEM is more robust to adversarial attacks as compared to discriminative classifiers.</li>
</ul>
