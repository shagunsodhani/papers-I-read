<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes a contrastive learning approach, called CURL, for performing off-policy control from raw pixel observations (by transforming them into high dimensional features).</p>
  </li>
  <li>
    <p>The idea is motivated by the application of contrastive losses in computer vision. But there are additional challenges:</p>

    <ul>
      <li>
        <p>The learning agent has to perform both unsupervised and reinforcement learning.</p>
      </li>
      <li>
        <p>The “dataset” for unsupervised learning is not fixed and keeps changing with the policy of the agent.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Unlike prior work, CURL introduces fewer changes in the underlying RL pipeline and provides more significant sample efficiency gains. For example, CURL (trained on pixels) nearly matches the performance of SAC policy (trained on state-based features).</p>
  </li>
  <li>
    <p><a href="https://github.com/MishaLaskin/curl">Link to the paper</a></p>
  </li>
</ul>

<h2 id="implementation">Implementation</h2>

<ul>
  <li>
    <p>CURL uses instance discrimination. Deep RL algorithms commonly use a stack of temporally consecutive frames as input to the policy. In such cases, instance discrimination is applied to all the images in the stack.</p>
  </li>
  <li>
    <p>For generating the positive and negative samples, random crop data augmentation is used.</p>
  </li>
  <li>
    <p>Bilinear inner product is used as the similarity metric as it outperforms the commonly used normalized dot product.</p>
  </li>
  <li>
    <p>For encoding the anchors and the samples, InfoNCE is used. It learns two encoders $f_q$ and $f_k$ that transform the query (base input) and the key (positive/negative samples) into latent representations. The similarity loss is applied to these latents.</p>
  </li>
  <li>
    <p>Momentum contrast is used to update the parameters ($\theta_k$) of the $f_k$ network. ie $\theta_k = m \theta_k + (1-m) \theta_q$. $\theta_q$ are the parameters of the $f_q$ network and are updated in the usual way, using both the contrastive loss and the RL loss.</p>
  </li>
</ul>

<h2 id="experiment">Experiment</h2>

<ul>
  <li>
    <p>DMControl100K and Atart100K refer to the setups where the agent is trained for 100K steps on DMControl and Atari, respectively.</p>
  </li>
  <li>
    <p>Metrics:</p>

    <ul>
      <li>
        <p>Sample Efficiency - How many steps does the baseline need to match CURL’s performance after 100K steps.</p>
      </li>
      <li>
        <p>Performance - Ratio of episodic returns by CURL vs. the baseline after 100K steps.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Baselines:</p>

    <ul>
      <li>
        <p>DMControl</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1910.01741">SAC-AE</a></li>
          <li><a href="https://arxiv.org/abs/1907.00953">SLAC</a></li>
          <li><a href="https://planetrl.github.io/">PlaNet</a></li>
          <li><a href="https://openreview.net/forum?id=S1lOTC4tDS">Dreamer</a></li>
          <li><a href="https://arxiv.org/abs/1812.05905">Pixel SAC</a></li>
          <li>SAC trained on state-space observations</li>
        </ul>
      </li>
      <li>
        <p>Atari</p>

        <ul>
          <li><a href="https://arxiv.org/abs/1903.00374">SimPLe</a></li>
          <li><a href="https://arxiv.org/abs/1710.02298">RainbowDQN</a></li>
          <li><a href="https://openreview.net/forum?id=Bke9u1HFwB">OTRainbow (Over Trained Rainbow)</a></li>
          <li><a href="https://arxiv.org/abs/1906.05243">Efficient Rainbow</a></li>
          <li>Random Agent</li>
          <li>Human Performance</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Results</p>

    <ul>
      <li>
        <p>DM Control</p>

        <ul>
          <li>
            <p>CURL outperforms all pixel-based RL algorithms by a significant margin for all environments on DMControl and most environments on Atari.</p>
          </li>
          <li>
            <p>On DMControl, it closely matches the performance of the SAC agent trained on state-space observations.</p>
          </li>
          <li>
            <p>On Atari, it achieves better median human normalizes score (HNS) than the other baselines and close to human efficiency in three environments.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
