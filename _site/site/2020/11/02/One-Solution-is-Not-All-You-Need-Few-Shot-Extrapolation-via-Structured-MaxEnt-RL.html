<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Key idea: Practicing and remembering diverse solutions to a task can lead to robustness to that taskâ€™s variations.</p>
  </li>
  <li>
    <p>The paper proposes a framework to implement this idea - train multiple policies such that they are <em>collectively</em> robust to a new distribution over environments while using a single training environment.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.14484">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>During training, the agent has access to only one MDP.</p>
  </li>
  <li>
    <p>During the evaluation, the agent encounters a new MDP which has the same state and action space but may have a different reward and transition function.</p>
  </li>
  <li>
    <p>The agent is allowed some interactions (say <em>k</em>) with the test MDP and is then evaluated on the test MDP. The setup is referred to as <em>few-shot robustness</em>.</p>
  </li>
</ul>

<h2 id="structured-maximum-entropy-reinforcement-learning-smerl">Structured Maximum Entropy Reinforcement Learning (SMERL)</h2>

<ul>
  <li>
    <p>Represent a set of policies using a latent variable policy (i.e., a policy conditioned on a latent variable <em>z</em>).</p>
  </li>
  <li>
    <p>This has two benefits: (i) Multiple policies can be represented by the same object, and (ii) diverse behaviors can be learned by encouraging the trajectories, corresponding to different <em>z</em> to be different, while being able to solve the task.</p>
  </li>
  <li>
    <p>A diversity-inducing objective is used to encourage the agent to learn different trajectories for different <em>z</em>.</p>
  </li>
  <li>
    <p>Specifically, the mutual information between <em>p(Z)</em> and marginal trajectory distribution for the latent variable policy is maximized, subject to the constraint that each policy achieves close to optimal returns in the train MDP.</p>
  </li>
  <li>
    <p>The mutual information between <em>p(Z)</em> and marginal trajectory distribution for the latent variable policy is lower bounded by the sum of mutual information terms over individual states (appearing in the trajectory).</p>
  </li>
  <li>
    <p>An unsupervised reward function is defined using the mutual information between states and latent variables.</p>
  </li>
  <li>
    <p>\(r(s, a) = log(q_{\phi})(z\|s) - log(p(z))\) where \(q_{\phi}\) is a learned discriminator.</p>
  </li>
  <li>
    <p>This unsupervised reward is optimized for only when the policy achieves close to an optimal return, i.e., the environment return is close to the optimal return. Otherwise, the agent optimizes only for the environment return.</p>
  </li>
</ul>

<h3 id="implementation">Implementation</h3>

<ul>
  <li>
    <p>SMERL is implemented using SAC with a latent variable maximum entropy policy.</p>
  </li>
  <li>
    <p>The set of latent variables is a fixed discrete set \(Z\) and \(p(z)\) is set to be a uniform distribution over this set.</p>
  </li>
  <li>
    <p>At the start of an episode, a \(z\) is sampled and used throughout the episode.</p>
  </li>
  <li>
    <p>Discriminator \(q_{\phi}(z\|s)\) is trained to infer \(z\) from the visited states.</p>
  </li>
  <li>
    <p>A baseline SAC agent is trained beforehand to evaluate if the current training policy achieves close to optimal environment return.</p>
  </li>
  <li>
    <p>During the evaluation, the policy corresponding to each latent variable is executed in the test MDP, and the policy with the maximum return is returned.</p>
  </li>
</ul>

<h2 id="theoretical-analysis">Theoretical Analysis</h2>

<ul>
  <li>
    <p>Given an MDP \(M\) and \(\epsilon&gt;0\), the MDP robustness set is defined as the set of all MDPs \(M'\) where the optimal policy of \(M'\) produces the same trajectory distribution in \(M'\) as \(M\). Moreover, on the training MDP \(M\), the optimal policies (corresponding to \(M\) and \(M'\)) obtain similar returns.</p>
  </li>
  <li>
    <p>The paper shows that SMERL generalizes to MDPs belong to the robustness set.</p>
  </li>
  <li>
    <p>It also provides a simplified view of the optimization objective and shows how it naturally leads to a trajectory-centric mutual information objective.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Environments</p>

    <ul>
      <li>
        <p>2D navigation environments with point mass.</p>
      </li>
      <li>
        <p>Mujoco Environments: HalfCheetah-Goal, Walker2d-Velocity, Hopper-Velocity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>On the 2D navigation environment, the paper shows that SMERL learns to use different trajectories to reach the goal.</p>
  </li>
  <li>
    <p>On the Mujoco setup, the evaluation shows that SMERL generally outperforms the best-performing baseline or is close to the best-performing baseline on different tasks.</p>
  </li>
  <li>
    <p>Generally, higher train performance does not correlate with higher test performance, and there is no single policy that performs the best across all the tasks. Thus, it should be beneficial to learn multiple diverse policies that can be selected from during testing.</p>
  </li>
</ul>
