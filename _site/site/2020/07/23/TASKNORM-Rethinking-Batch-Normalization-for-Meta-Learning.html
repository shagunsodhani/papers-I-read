<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Meta-learning techniques are shown to benefit from the use of deep neural networks.</p>
  </li>
  <li>
    <p>BatchNorm is a commonly used component when training deep networks, especially for vision tasks.</p>
  </li>
  <li>
    <p>However, BatchNorm and meta-learning make contradictory assumptions, and their combination may not work well in practice.</p>
  </li>
  <li>
    <p>The paper proposes TaskNorm, a normalization method that is designed explicitly for meta-learning.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.03284">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>Standard meta-learning setup with $k$ tasks, each task with its own context and target set.</p>
  </li>
  <li>
    <p>Two sets of parameters are considered during meta-learning - (i) global parameters, and (ii) task-specific parameters.</p>
  </li>
  <li>
    <p>Meta-learning setup can be viewed as an inference task, where the task-specific parameters are inferred using a context set and some additional (trainable) parameters.</p>
  </li>
  <li>
    <p>Normalization layers are commonly used to accelerate the training of neural networks. The general approach is to use normalization moments (statistics) along with some learned parameters.</p>
  </li>
  <li>
    <p>BatchNorm is a well-known and widely used normalization approach. It relies on the implicit assumption that the dataset comprises of iid samples from some underlying distribution.</p>
  </li>
  <li>
    <p>However, in meta-learning, data points are assumed to be iid only within a specific task.</p>
  </li>
  <li>
    <p>This leaves open the question of what moments to use during meta-train and meta-test time.</p>
  </li>
</ul>

<h2 id="variants-of-batchnorm">Variants of BatchNorm</h2>

<h3 id="conventional-batchnorm-cbn">Conventional BatchNorm (CBN)</h3>

<ul>
  <li>
    <p>Compute moments at meta train time and use during meta test time.</p>
  </li>
  <li>
    <p>This is equivalent to lumping the moments with the global parameters. I.e., the running moments are shared globally, while the data is iid only locally.</p>
  </li>
  <li>
    <p>Using CBN with MAML leads to poor results.</p>
  </li>
  <li>
    <p>Moreover, meta-learning setup can some times require the use of a very small batch size. (e.g., 1-shot learning) In those cases, the computed statistics are likely to be inaccurate.</p>
  </li>
</ul>

<h3 id="transductive-batchnorm-tbn">Transductive BatchNorm (TBN)</h3>

<ul>
  <li>
    <p>Use context/target set statistics at both meta-train and meta-test time.</p>
  </li>
  <li>
    <p>This is the default BatchNorm mode used in MAML.</p>
  </li>
</ul>

<h3 id="instance-based-normalization">Instance-based normalization</h3>

<ul>
  <li>
    <p>Moments are computed separately for each instance.</p>
  </li>
  <li>
    <p>This mode corresponds to treating the statistics as local at the observation level.</p>
  </li>
  <li>
    <p>These methods provide only limited improvement in performance, and can sometimes have a large overhead.</p>
  </li>
</ul>

<h2 id="task-normalization-proposed">Task Normalization (Proposed)</h2>

<ul>
  <li>
    <p>The normalization statistics are local at the task level, and statistics for a given data point should only depend on the context set’s data point. It should not depend on the other elements of the target set.</p>
  </li>
  <li>
    <p>Meta-Batch Normalisation (METABN) is a precursor to TaskNorm where the context set alone is used to compute the normalization statistics for both the context and the target set (during both meta-test and meta-train time).</p>
  </li>
  <li>
    <p>METABN does not perform well when used with small context sets.</p>
  </li>
  <li>
    <p>TaskNorm overcomes this limitation by using a set of non-transductive, secondary moments (computed from the input being normalized).</p>
  </li>
  <li>
    <p>When the context is small, using additional moments will help to improve the moment estimates.</p>
  </li>
  <li>
    <p>In the general case, a trainable blending factor, $\alpha$, is used to combine the two sets of moments.</p>
  </li>
  <li>
    <p>While the computational cost of TaskNorm is slightly more than CBN, it converges faster than CBN in practice.</p>
  </li>
  <li>
    <p>Normalization mechanism in Reptile can be interpreted as a particular case of TaskNorm.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Small scale few-shot classification experiments</p>

    <ul>
      <li>
        <p>Omniglot and imin ImageNet dataset</p>
      </li>
      <li>
        <p>First order MAML, with different kinds of normalization schemes.</p>
      </li>
      <li>
        <p>Transductive BatchNorm performs the best.</p>
      </li>
      <li>
        <p>Among non-transductive approaches, TaskNorm using Instance Normalisation augmentation performs the best.</p>
      </li>
      <li>
        <p>Similar trend holds for the speed of convergence as well.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Large scale few-shot classification experiments</p>

    <ul>
      <li>
        <p>MetaDataset dataset</p>
      </li>
      <li>
        <p>CNAPs model</p>
      </li>
      <li>
        <p>The context set’s size varies across tasks in this setup and can be as small as 5.</p>
      </li>
      <li>
        <p>TaskNorm with Instance Normalisation ranks first in 10 (out of 13) datasets and is also the fastest to train.</p>
      </li>
      <li>
        <p>While Instance-based methods (Instance Normalisation and Layer Normalisation) are the slowest to converge, they still outperform the running average based methods (conventional BatchNorm).</p>
      </li>
      <li>
        <p>The results demonstrate that designing meta-learning specific normalization methods can significantly improve performance and that Transductive BatchNorm may not always be the optimal choice.</p>
      </li>
    </ul>
  </li>
</ul>
