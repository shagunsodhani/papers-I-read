<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes Stochastic Weight Averaging (SWA) procedure for improving the generalization performance of models trained with SGD (with cyclic or constant learning rate).</p>
  </li>
  <li>
    <p>Specifically, the model is checkpointed at several points along the training trajectory, and these checkpoints are averaged (in the parameter space) to obtain a single model.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1803.05407">Link to the paper</a></p>
  </li>
</ul>

<h2 id="idea">Idea</h2>

<ul>
  <li>
    <p>“Stochastic” in the name refers to the idea that with cyclical or constant learning rate, SGD proposals are approximately sampled from a neural network’s loss surface and are hence stochastic.</p>
  </li>
  <li>
    <p>SWA uses a learning rate schedule that allows exploration in the weight space.</p>
  </li>
  <li>
    <p>SGD with cyclical and constant learning rates explore points (model instances) at the periphery of high-performing networks.</p>
  </li>
  <li>
    <p>With different initializations, SGD will find different points (of low training loss) on this boundary, but will not move inside it.</p>
  </li>
  <li>
    <p>Averaging the points provide a mechanism to move inside this periphery.</p>
  </li>
  <li>
    <p>The train and the test error surfaces, while being similar, are not perfectly aligned. Hence, averaging several models (along the optimization trajectory) could lead to a more robust model.</p>
  </li>
</ul>

<h2 id="algorithm">Algorithm</h2>

<ul>
  <li>
    <p>Given a model $w$ and some training budget $B$, train the model in the conventional way for approx 75% of the budget.</p>
  </li>
  <li>
    <p>Starting from that point, continue training with the remaining budget, with a constant or cyclical learning rate.</p>
  </li>
  <li>
    <p>For fixed learning rate, checkpoint models at each epoch. For cyclical learning rate, checkpoint the model at the lowest learning rate in the cycle.</p>
  </li>
  <li>
    <p>Average all the models to get the SWA model.</p>
  </li>
  <li>
    <p>If the model has Batch Normalization layers, run an additional pass to compute the SWA model’s running mean and standard deviation.</p>
  </li>
  <li>
    <p>The computational and space complexity of computing the SWA model is relatively low.</p>
  </li>
  <li>
    <p>The paper highlights the ensembling like the effect of SWA by showing that if the model checkpoints ($w_i$) are generated by training with Fast Geometric Ensembling (FGE), the difference between averaging the weights and averaging the predictions is of the order $O(\Delta)$ where $\Delta = max ||w_i - w_{SA}||$.</p>
  </li>
  <li>
    <p>Note that SWA does not have the overhead of an extra-forward pass during inference.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Datasets: CIFAR10, CIFAR100, ImageNet</p>
  </li>
  <li>
    <p>Models: VGG16, WideResNet, 164-layer preactivation ResNet, ShakeShake, Pyramid Net.</p>
  </li>
  <li>
    <p>Baselines: Conventional SGD, Exponentially decaying average with SGD and FGE.</p>
  </li>
  <li>
    <p>In all the CIFAR experiments, SWA consistently outperforms SGD in one budget and consistently improves with training.</p>
  </li>
  <li>
    <p>SWA also achieves performance comparable to FGE, despite FGE being an ensemble method.</p>
  </li>
  <li>
    <p>On ImageNet, SWA is run on a pre-trained model, and it improves performance in all the cases.</p>
  </li>
  <li>
    <p>An ablation experiment (on CIFAR-100) shows that it is possible to train a network (with SWA) using a fixed learning rate. In that setup, using SWA improves performance by 16%.</p>
  </li>
</ul>
