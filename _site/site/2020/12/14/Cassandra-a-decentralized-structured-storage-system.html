<h2 id="introduction">Introduction</h2>

<ul>
  <li>Cassandra is a distributed storage system that runs over cheap commodity servers and handles high write throughput while maintaining low latency for read operations.</li>
  <li>At the time of writing, it was used to support the search for Facebook Inbox.</li>
  <li><a href="https://dl.acm.org/doi/10.1145/1773912.1773922">Link to the paper</a></li>
  <li><a href="https://cassandra.apache.org/">Link to the implementation</a></li>
</ul>

<h2 id="data-model">Data Model</h2>

<ul>
  <li>A table is a distributed multidimensional map.</li>
  <li>The key is a string (generally 16-36 bytes long), while the value is a structured object.</li>
  <li>Every operation under a single row key is atomic per replica.</li>
  <li>Columns are grouped together into sets called column families.</li>
  <li>There are two types of columns families:
    <ul>
      <li>Simple families.</li>
      <li>Super column families: visualized as a column family within a column family.</li>
    </ul>
  </li>
  <li>Columns can be sorted by name or time (used to display results in time sorted order).</li>
  <li>The API supports insert, get and delete operations.</li>
</ul>

<h2 id="system-architecture">System Architecture</h2>

<h3 id="handling-requests">Handling Requests</h3>

<ul>
  <li>Any read/write request gets routed to any node in the cluster. The node determines the replicas for a given key and routes the request.</li>
  <li>For write query, the system waits for a quorum of replicas to acknowledge the writes’ completion.</li>
  <li>For read query, the system either routes the requests to the closest replica (might fetch stale results) or routes the requests to all replicas and waits for a quorum of responses.</li>
</ul>

<h3 id="partitioning">Partitioning</h3>

<ul>
  <li>Cassandra partitions data across the cluster using consistent hashing with an order-preserving hash function.</li>
  <li>The hash function’s output range is treated as a fixed circular ring, and each node is assigned a random position on the ring.</li>
  <li>An incoming request specifies a key used to route requests.</li>
  <li>One benefit of this approach is that the addition/removal of a node only affects its immediate neighbors.</li>
  <li>However, randomly assigning nodes leads to non-uniform data and load distribution.</li>
  <li>Cassandra uses the load information and moves lightly loaded nodes to reduce the load on other nodes.</li>
</ul>

<h3 id="replication">Replication</h3>

<ul>
  <li>Each data item is replicated at N hosts, where N is the per-instance replication factor.</li>
  <li>Cassandra supports the following replication policies: Rack Unaware, Rack Aware (within a datacenter), and Datacenter Aware.</li>
  <li>For “Rack Aware” and “Datacenter Aware” strategies, Zookeeper elects a leader among the nodes and holds metadata about which range a node is responsible for.</li>
  <li>In case of node failure and network partitions, the quorum requirements are relaxed.</li>
</ul>

<h3 id="membership">Membership</h3>

<ul>
  <li>Cluster membership is based on Scuttlebutt, a very efficient anti-entropy Gossip based mechanism.</li>
  <li>Cassandra uses a modified version of $\phi$ Accrual Failure Detector for detecting failures, which provides the suspicion level (of failure) for each node.</li>
</ul>

<h3 id="bootstrapping">Bootstrapping</h3>

<ul>
  <li>A node, starting for the first time, chooses a random position in the ring.</li>
  <li>This information is persisted on the local disk, on Zookeeper, and gossiped around the cluster (so any node can route any query in the cluster).</li>
  <li>During bootstrapping, the newly joined node reads a list of contact points (within the cluster) using a configuration file.</li>
</ul>

<h3 id="local-persistence">Local Persistence</h3>

<ul>
  <li>Generally, a write operation involves a write into a commit log (for durability and recoverability), followed by a write into the in-memory data structures.</li>
  <li>A read operation starts with querying the in-memory data and then looks into the filesystem.</li>
  <li>Read queries on the filesystem use bloom filters.</li>
  <li>Column indices are maintained to make it faster to look up relevant columns.</li>
</ul>

<h2 id="implementation-details">Implementation Details</h2>

<ul>
  <li>Components implemented in Java.</li>
  <li>System control messages use UDP while messages for replication and request routing uses TCP.</li>
  <li>A new commit log is rolled out after the older one exceeds 128MB of size.</li>
  <li>All the data is indexed using a primary key.</li>
  <li>Data on the disk is chunked into sequences of blocks. Each block contains at most 128 keys and is demarcated by a block index.</li>
  <li>When the data is written to the disk, a block index is generated and maintained in the memory for faster access.</li>
  <li>A compaction process is performed to merge multiple files (on disk) into one file.</li>
</ul>

<h2 id="practical-experience">Practical Experience</h2>

<ul>
  <li>Data from MySQL servers is added to Cassandra using MapReduce processes.</li>
  <li>Although Cassandra is a completely decentralized system, adding some coordination (via Zookeeper) is helpful.</li>
  <li>For Inbox Search, a per-user index is maintained for all the messages.</li>
  <li>For “term search”, the key is the userid, and the words in the message become the super column.</li>
  <li>For searching all the messages ever sent/received by a user, the key is the userid, and the recipient ids are the super columns.</li>
</ul>
