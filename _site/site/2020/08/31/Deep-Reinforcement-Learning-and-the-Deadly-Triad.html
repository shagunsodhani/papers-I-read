<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper investigates the practical impact of the deadly triad (function approximation, bootstrapping, and off-policy learning) in deep Q-networks (trained with experience replay).</p>
  </li>
  <li>
    <p>The deadly triad is called so because when all the three components are combined, TD learning can diverge, and value estimates can become unbounded.</p>
  </li>
  <li>
    <p>However, in practice, the component of the deadly triad has been combined successfully. An example is training DQN agents to play Atari.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1812.02648">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>The effect of each component of the triad can be regulated with some design choices:</p>

    <ul>
      <li>
        <p>Bootstrapping - by controlling the number of steps before bootstrapping.</p>
      </li>
      <li>
        <p>Function approximation - by controlling the size of the neural network.</p>
      </li>
      <li>
        <p>Off-policy learning - by controlling how data points are sampled from the replay buffer (i.e., using different prioritization approaches)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The problem is studied in two contexts: toy example and Atari 2600 games.</p>
  </li>
  <li>
    <p>The paper makes several hypotheses about how different components may interact in the triad and evaluate these hypotheses by training DQN with different hyperparameters:</p>

    <ul>
      <li>
        <p>Number of steps before bootstrapping - 1, 3, 10</p>
      </li>
      <li>
        <p>Four levels of prioritization (for sampling data from the replay buffer)</p>
      </li>
      <li>
        <p>Bootstrap target - Q-learning, target Q-learning, inverse double Q-learning, and double Q-learning</p>
      </li>
      <li>
        <p>Network sizes-small, medium, large and extra-large.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Each experiment was run with three different seeds.</p>
  </li>
  <li>
    <p>The paper formulates a series of hypotheses and designs experiments to support/reject the hypotheses.</p>
  </li>
</ul>

<h2 id="hypothesis-1-combining-q-learning-with-conventional-deep-rl-function-spaces-does-not-commonly-lead-to-divergence">Hypothesis 1: Combining Q learning with conventional deep RL function spaces does not commonly lead to divergence</h2>

<ul>
  <li>
    <p>Rewards are clipped between -1 and 1, and the discount factor is set to 0.99. Hence, the maximum absolute action value is bound to smaller than 100. This upper bound is used soft-divergence in the value estimates.</p>
  </li>
  <li>
    <p>The paper reports that while soft-divergence does occur, the values do not become unbounded, thus supporting the hypothesis.</p>
  </li>
</ul>

<h2 id="hypothesis-2-there-is-less-divergence-when-correcting-for-overestimation-bias-or-when-bootstrapping-on-separate-networks">Hypothesis 2: There is less divergence when correcting for overestimation bias or when bootstrapping on separate networks.</h2>

<ul>
  <li>
    <p>One manifestation of bootstrapping on separate networks is target-Q learning. While using separate networks helps on Atari, it does not entirely solve the problem on the toy setup.</p>
  </li>
  <li>
    <p>One manifestation of correcting for the overestimation bias is using double Q-learning.</p>
  </li>
  <li>
    <p>In the standard form, double Q-learning benefits by bootstrapping on a separate network. To isolate the gains by using each component independently, an inverse double Q-learning update is used that does not use a separate target-network for bootstrapping.</p>
  </li>
  <li>
    <p>Experimentally, Q-learning is the most unstable while target Q-learning and double Q-learning are the most stable. This observation supports the hypothesis.</p>
  </li>
</ul>

<h2 id="hypothesis-3-longer-multi-step-returns-will-diverge-easily">Hypothesis 3: Longer multi-step returns will diverge easily</h2>

<ul>
  <li>
    <p>This hypothesis is intuitive as the dependence on bootstrapping is reduced with multi-step returns.</p>
  </li>
  <li>
    <p>Experimental results support this hypothesis.</p>
  </li>
</ul>

<h2 id="hypothesis-4-larger-more-capacity-networks-will-diverge-less-easily">Hypothesis 4: Larger, more capacity networks will diverge less easily.</h2>

<ul>
  <li>
    <p>This hypothesis is based on the assumption that more flexible value function approximations may behave more like the tabular case.</p>
  </li>
  <li>
    <p>In practice, smaller networks show fewer instances of instability than the larger networks.</p>
  </li>
  <li>
    <p>The hypothesis is not supported by the experiments.</p>
  </li>
</ul>

<h2 id="hypothesis-5-stronger-prioritization-of-updates-will-diverge-more-easily">Hypothesis 5: Stronger prioritization of updates will diverge more easily.</h2>

<ul>
  <li>This hypothesis is supported by the experiments for all the four updates.</li>
</ul>

<h2 id="effect-of-the-deadly-triad-on-the-agents-performance">Effect of the deadly triad on the agentâ€™s performance</h2>

<ul>
  <li>
    <p>Generally, soft-divergence correlates with poor control performance.</p>
  </li>
  <li>
    <p>For example, longer multi-step returns lead to fewer instances of instabilities and better performance.</p>
  </li>
  <li>
    <p>The trend is more interesting in terms of network capacity. Large networks tend to diverge more but also perform the best.</p>
  </li>
  <li>
    <p>While action-value estimates can grow to large values, they can recover to plausible values as training progresses.</p>
  </li>
</ul>
