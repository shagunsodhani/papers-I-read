<ul>
  <li>
    <p>The paper hypothesizes that main optimization challenges in multi-task learning arise because of negative interference between different tasks’ gradients.</p>
  </li>
  <li>
    <p>It hypothesizes that negative interference happens when:</p>

    <ul>
      <li>
        <p>The gradients are conflicting (i.e., have a negative cosine similarity).</p>
      </li>
      <li>
        <p>The gradients coincide with high positive curvature.</p>
      </li>
      <li>
        <p>The difference in gradient magnitude is quite large.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The paper proses to work around this problem by performing “gradient surgery.”</p>
  </li>
  <li>
    <p>If two gradients are conflicting, modify the gradients by projecting each onto the other’s normal plane.</p>
  </li>
  <li>
    <p>This modification is equivalent to removing the conflicting component of the gradient.</p>
  </li>
  <li>
    <p>This approach is referred to as <em>projecting conflicting gradients</em> (PCGrad).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2001.06782">Link to the paper</a></p>
  </li>
  <li>
    <p>Theoretical Analysis</p>

    <ul>
      <li>
        <p>The paper proves the local conditions under which PCGrad improves multi-task gradient descent in the two-task setup.</p>
      </li>
      <li>
        <p>The conditions are:</p>

        <ul>
          <li>
            <p>Angle between the task gradients is not too small.</p>
          </li>
          <li>
            <p>Difference in the magnitude of the gradients is sufficiently large.</p>
          </li>
          <li>
            <p>Curvature of the multi-task gradient is large.</p>
          </li>
          <li>
            <p>Large enough learning rate.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Experimental Setup</p>

    <ul>
      <li>
        <p>Multi-task supervised learning</p>

        <ul>
          <li>
            <p>MutliMNIST, Multi-task CIFAR100, NYUv2.</p>
          </li>
          <li>
            <p>For Multi-task CIFAR-100, PCGrad is used with the shared parameters of the routing networks.</p>
          </li>
          <li>
            <p>For NYUv2, PCGrad is combined with MTAN.</p>
          </li>
          <li>
            <p>In all the cases, using PCGrad improves the performance.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Multi-task Reinforcement Learning</p>

        <ul>
          <li>
            <p>Meta-World Benchmark</p>
          </li>
          <li>
            <p>PCGrad + SAC outperforms all other baselines.</p>
          </li>
          <li>
            <p>In the context of SAC, the paper suggests learning temperature $\alpha$ on a per-task basis.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Goal-conditioned Reinforcement Learning</p>

        <ul>
          <li>
            <p>Goal-conditioned robotic pushing task with a Sawyer robot.</p>
          </li>
          <li>
            <p>PCGrad + SAC outperforms vanilla SAC.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
