<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper introduces Multi-Object Network (MONet) architecture that learns a modular representation of images by spatially decomposing scenes into <em>objects</em> and learning a representation for these <em>objects</em>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.11390">Link to the paper</a></p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p>Two components:</p>

    <ul>
      <li>
        <p>Attention Module: generates spatial masks corresponding to the <em>objects</em> in the scene.</p>
      </li>
      <li>
        <p>VAE: learn representation for each <em>object</em>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>VAE components:</p>

    <ul>
      <li>
        <p>Encoder: It takes as input the image and the attention mask generated by the attention module and produce the parameters for distribution over latent variable <em>z</em>.</p>
      </li>
      <li>
        <p>Decoder: It takes as input the latent variable <em>z</em> and attempts to reproduce the image.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The decoder loss term is weighted by mask, i.e., the decoder tries to reproduce only those parts of the image that the attention mask focuses on.</p>
  </li>
  <li>
    <p>The attention mechanism is auto-regressive with an ongoing state (called a scope) that tracks which parts of the image are not yet attended over.</p>
  </li>
  <li>
    <p>In the last step, no attention mask is computed, and the previous scope is used as-is. This ensures that all the masks sum to 1.</p>
  </li>
  <li>
    <p>The VAE also models the attention mask over the components, i.e., the probability that the pixels belong to a particular component.</p>
  </li>
</ul>

<h2 id="motivation">Motivation</h2>

<ul>
  <li>
    <p>A model could efficiently process compositional visual scenes if it can exploit some recurring structures in the scene.</p>
  </li>
  <li>
    <p>The paper validates this hypothesis by showing that an autoencoder performs better if it can build up the scenes compositionally, processing one mask at a time (these masks are ground-truth spatial masks) rather than processing the scene at once.</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>VAE encoder parameterizes a diagonal Gaussian latent posterior with a spatial broadcast decoder that encourages the VAE to learn disentangled features.</p>
  </li>
  <li>
    <p>MONet with seven slots is trained on <em>Objects Room</em> dataset with 1-3 objects.</p>

    <ul>
      <li>
        <p>It learns to generate different attention mask for different objects.</p>
      </li>
      <li>
        <p>Combining the reconstructed components using the corresponding attention masks produces good quality reconstruction for the entire scene.</p>
      </li>
      <li>
        <p>Since it is an autoregressive model, MONet can be evaluated for more slots. The model generalizes to novel scene configurations (not seen during training).</p>
      </li>
    </ul>
  </li>
  <li>
    <p>On the Multi-dSprites dataset (modification of the dSprites dataset), the model (post-training) distinguishes individual sprites and background.</p>
  </li>
  <li>
    <p>On the CLEVER data (2-10 objects per image), the model generates good image segmentation and reconstructions and can distinguish between overlapping shapes.</p>
  </li>
</ul>
