<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper studies <em>observational overfitting</em>: The phenomenon where an agent overfits to different observation spaces even though the underlying MDP remains fixed.</p>
  </li>
  <li>
    <p>Unlike other works, the “background information” (in the pixel space) is correlated with the progress of the agent (and is not just noise).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1912.02975">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>Base MDP $M = (S, A, R, T)$ where $S$ is the state space, $A$ is the action space, $R$ is the reward function, and $T$ is the transition dynamics.</p>
  </li>
  <li>
    <p>$M$ is parameterized using $\theta$. In practice, it means introducing an observation function $\phi_{\theta}$ ie $M_{\theta} = (M, \phi_{\theta})$.</p>
  </li>
  <li>
    <p>A distribution over $\theta$ defines a distribution over the MDPs.</p>
  </li>
  <li>
    <p>The learning agent has access to the pixel space observations and not the state space observations.</p>
  </li>
  <li>
    <p>Generalization gap is defined as $J_{\theta}(\pi) - J_{\theta^{train}}(\pi)$ where $\pi$ is the learning agent, $\theta$ is the distribution over all the observation functions, $\theta^{train}$ is the distribution over the observation functions corresponding to the training environments. $J_{\theta}(\pi)$ is the average reward that the agent obtains over environments sampled from $M_{\theta}$.</p>
  </li>
  <li>
    <p>$\phi_{\theta}$ considers two featurs - generalizable (invariant across $\theta$) and non-generalizable (depends on $\theta$) ie $\phi_{\theta}(s) = concat(f(s), g_{\theta}(s))$ where $f$ is the invariant function and $g$ is the non-generalizable function.</p>
  </li>
  <li>
    <p>The problem is set up such that “explicit regularization” can easily solve it. The focus is on understanding the effect of “implicit regularization”.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="overparameterized-lqr">Overparameterized LQR</h3>

<ul>
  <li>
    <p>LQR is used as a proxy for deep RL architectures given its advantages like enabling exact gradient descent.</p>
  </li>
  <li>
    <p>The functions are parameterized as follows:</p>

    <ul>
      <li>
        <p>$f(s) = W_c(s)$</p>
      </li>
      <li>
        <p>$g_{\theta}(s) = W_{\theta}(s)$</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Observation at time $t$ , $o_t$, is given as $[W_c W_{\theta}]^{-1} s_t$.</p>
  </li>
  <li>
    <p>Action at time $t$ is given as $a_t = K o_{t}$ where $K$ is the policy matrix.</p>
  </li>
  <li>
    <p>Dimensionality:</p>

    <ul>
      <li>state $s$: $d_{state}$ 100</li>
      <li>$f(s)$: $d_{state}$ 100</li>
      <li>$g_{\theta}(s)$: $d_{noise}$ 100</li>
      <li>observation $o$: $d_{state}$ + $d_{noise}$ 1100</li>
    </ul>
  </li>
  <li>
    <p>In case of training on just one environment, multiple solutions exist, and overfitting happens.</p>
  </li>
  <li>
    <p>Increasing $d_{noise}$ increases the generalization gap.</p>
  </li>
  <li>
    <p>Overparameterizing the network decreases the generalization gap and also reduces the norm of the policy.</p>
  </li>
</ul>

<h3 id="projected-gym-environments">Projected Gym Environments</h3>

<ul>
  <li>
    <p>The base MDP is the Gym Environment.</p>
  </li>
  <li>
    <p>$M_{\theta}$ is generated as before.</p>
  </li>
  <li>
    <p>Increasing both width and depth for basic MLPs improves generalization.</p>
  </li>
  <li>
    <p>Generalization also depends on the choice of activation function, residual layers, etc.</p>
  </li>
</ul>

<h3 id="deconvolutional-projections">Deconvolutional Projections</h3>

<ul>
  <li>
    <p>In the Gym environment, the actual state is projected to a larger vector and reshaped into an 84x84 tensor (image).</p>
  </li>
  <li>
    <p>The image from $f$ is concatenated with the image from $g$. This setup is referred to as the Gym-Deconv.</p>
  </li>
  <li>
    <p>The relative order of performance between NatureCNN, IMPALA, and IMPALA-Large (on both CoinRun and Gym-Deconv) is the same as the order of the number of parameters they contain.</p>
  </li>
  <li>
    <p>In an ablation, the policy is given access to only $g_{\theta}(s)$, which makes it impossible for the model to generalize. In this test of memorization capacity, implicit regularization seems to reduce the memorization effect.</p>
  </li>
</ul>

<h3 id="overparameterization-in-coinrun">Overparameterization in CoinRun</h3>

<ul>
  <li>
    <p>The pixel space observation in CoinRun is downsized from 64x64 to 32x32 and flattened into a vector.</p>
  </li>
  <li>
    <p>In CoinRun, the dynamics change per level, and the noisy “irrelevant” features change location across the 1D input, making this setup more challenging than the previous ones.</p>
  </li>
  <li>
    <p>Overparameterization improves generalization in this scenario as well.</p>
  </li>
</ul>
