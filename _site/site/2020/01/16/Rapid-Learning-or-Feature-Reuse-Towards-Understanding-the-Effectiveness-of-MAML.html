<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper investigated two possible reasons behind the usefulness of MAML algorithm:</p>

    <ul>
      <li>
        <p><strong>Rapid Learning</strong> - Does MAML learn features that are amenable for rapid learning?</p>
      </li>
      <li>
        <p><strong>Feature Reuse</strong> - Does the MAML initialization provide high-quality features that are useful for the unseen tasks.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>This leads to a follow-up question: how much task-specific inner loop adaptation is needed.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.09157">Link to the paper</a></p>
  </li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>In a standard few-shot learning setup, the different datasets have different classes. Hence, the top-most layer (or the head) of the learning model should be different for different tasks.</p>
  </li>
  <li>
    <p>The subsequent discussion only applies to the body of the network (ie, network minus the head).</p>
  </li>
  <li>
    <p><strong>Freezing Layer Representations</strong></p>

    <ul>
      <li>
        <p>In this setup, a subset (or all) of parameters are frozen (after MAML training) and are not adapted during the representation.</p>
      </li>
      <li>
        <p>Even when the entire network is frozen, the performance drops only marginally.</p>
      </li>
      <li>
        <p>This indicates that the representation learned by the meta-initialization is good enough to be useful on the test tasks (without requiring any adaptation step).</p>
      </li>
      <li>
        <p>Note that the head of the network is still adapted during testing.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Representational Similarity</strong></p>

    <ul>
      <li>
        <p>In this setup, the paper reports the change in the latent representation (learned by the network) during the inner loop update with a fully trained model.</p>
      </li>
      <li>
        <p>Canonical Correlation Analysis (CCA) and Central Kernel Alignment (CKA) metrics are used to measure the similarity between the representations.</p>
      </li>
      <li>
        <p>The main finding is that the representations in the body of the network are very similar before and after the inner loop updates while the representations in the head of the network are very different.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The above two observations indicate that feature reuse is the primary driving factor for the success of MAML.</p>
  </li>
  <li>
    <p><strong>When does feature reuse happen</strong></p>

    <ul>
      <li>
        <p>The paper considers the model at different stages of training and compares the similarity in the representation (before and after the inner loop update).</p>
      </li>
      <li>
        <p>Even early in training, the CCA similarity between the representations (before and after the inner loop update) is quite high. Similarly, freezing the layers (for the test time update), early in training, does not degrade the test time performance much. This hints that the feature reuse happens early in the learning process.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-anil-almost-no-inner-loop-algorithm">The ANIL (Almost No Inner Loop) Algorithm</h2>

<ul>
  <li>
    <p>The empirical evidence suggests that the success of MAML lies in the feature reuse.</p>
  </li>
  <li>
    <p>The authors build on this observation and propose a simplification of the MAML algorithm: ANIL or Almost No Inner Loop Algorithm</p>
  </li>
  <li>
    <p>In this algorithm, the inner loop updates are applied only to the head of the network.</p>
  </li>
  <li>
    <p>Despite being much more straightforward, the performance of ANIL is close to the performance of MAML for both few-shot image classification and RL tasks.</p>
  </li>
  <li>
    <p>Removing most of the inner loop parameters speed up the computation by a factor of 1.7 (during training) and 4.1 (during inference).</p>
  </li>
</ul>

<h2 id="removing-the-inner-loop-update">Removing the Inner Loop Update</h2>

<ul>
  <li>
    <p>Given that it is possible to remove most of the parameters from the inner loop update (without affecting the performance), the next step is to check if the inner loop update can be removed entirely.</p>
  </li>
  <li>
    <p>This leads to the NIL (No Inner Loop) algorithm, which does not involve any inner loop adaptation steps.</p>
  </li>
</ul>

<h3 id="algorithm">Algorithm</h3>

<ul>
  <li>
    <p>A few-shot learning model is trained - either with MAML or ANIL.</p>
  </li>
  <li>
    <p>During testing, the head is removed.</p>
  </li>
  <li>
    <p>For each task, the K training examples are fed to the body to obtain class representations.</p>
  </li>
  <li>
    <p>For a given test data point, the representation of the data point is compared with the different class representations to obtain the target class.</p>
  </li>
  <li>
    <p>The NIL algorithm performs similar to the MAML and the ANIL algorithms for the few-shot image classification task.</p>
  </li>
  <li>
    <p>Note that it is still important to use MAML/ANIL during training, even though the learned head is not used during evaluation.</p>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<ul>
  <li>The paper discusses the different classes of meta-learning approaches. It concludes with the observation that feature reuse (and not rapid adaptation) seems to be the common model of operation for both optimization-based meta-learning (e.g., MAML) and model-based meta-learning.</li>
</ul>
