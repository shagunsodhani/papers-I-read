<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes to build a universal neural machine translation system that can translate between any pair of languages.</p>
  </li>
  <li>
    <p>As a concrete instance, the paper prototypes a system that handles 103 languages (25 Billion translation pairs).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1907.05019">Link to the paper</a></p>
  </li>
</ul>

<h2 id="why-universal-machine-translation">Why universal Machine Translation</h2>

<ul>
  <li>
    <p>Hypothesis: <em>The learning signal from one language should benefit the quality of other languages</em><a href="https://link.springer.com/article/10.1023/A:1007379606734">1</a></p>
  </li>
  <li>
    <p>This positive transfer is evident for low resource languages but tends to hurt the performance for high resource languages.</p>
  </li>
  <li>
    <p>In practice, adding new languages reduces the effective per-task capacity of the model.</p>
  </li>
</ul>

<h2 id="desiderata-for-multilingual-translation-model">Desiderata for Multilingual Translation Model</h2>

<ul>
  <li>
    <p>Maximize the number of languages within one model.</p>
  </li>
  <li>
    <p>Maximize the positive transfer to low resource languages.</p>
  </li>
  <li>
    <p>Minimize the negative interference to high resource languages.</p>
  </li>
  <li>
    <p>Perform well ion the realistic, multi-domain settings.</p>
  </li>
</ul>

<h2 id="datasets">Datasets</h2>

<ul>
  <li>
    <p>In-house corpus generated by crawling and extracting parallel sentences from the web.</p>
  </li>
  <li>
    <p>102 languages, with 25 billion sentence pairs.</p>
  </li>
  <li>
    <p>Compared with the existing datasets, this dataset is much larger, spans more domains, has a good variation in the amount of data available for different language pairs, and is noisier. These factors bring additional challenges to the universal NMT setup.</p>
  </li>
</ul>

<h2 id="baselines">Baselines</h2>

<ul>
  <li>
    <p>Dedicated Bilingual models (variants of Transformers).</p>
  </li>
  <li>
    <p>Most bilingual experiments used Transformer big and a shared source-target sentence-piece model (SPE).</p>
  </li>
  <li>
    <p>For medium and low resource languages, the Transformer Base was also considered.</p>
  </li>
  <li>
    <p>Batch size of 1 M tokes per-batch. Increasing the batch size improves model quality and speeds up convergence.</p>
  </li>
</ul>

<h2 id="effect-of-transfer-and-interference">Effect of Transfer and Interference</h2>

<ul>
  <li>
    <p>The paper compares the following two setups with the baseline:</p>

    <ul>
      <li>
        <p>Combine all the datasets and train over them as if it is a single dataset.</p>
      </li>
      <li>
        <p>Combine all the datasets but upsample low resource languages so all that all the languages are equally likely to appear in the combined dataset.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>A target “index” is prepended with every input sentence to indicate which language it should be translated into.</p>
  </li>
  <li>
    <p>Shared encoder and decoder are used across all the language pairs.</p>
  </li>
  <li>
    <p>The two setups use a batch size of 4M tokens.</p>
  </li>
</ul>

<h3 id="results">Results</h3>

<ul>
  <li>
    <p>When all the languages are equally sampled, the performance on the low resource languages increases, at the cost of performance on high resource languages.</p>
  </li>
  <li>
    <p>Training over all the data at once reverse this trend.</p>
  </li>
</ul>

<h3 id="countering-interference">Countering Interference</h3>

<ul>
  <li>
    <p>Temperature based sampling strategy is used to control the ratio of samples from different language pairs.</p>
  </li>
  <li>
    <p>A balanced sampling strategy improves the performance for the high resource languages (though not as good as the multilingual baselines) while retaining the high transfer performance on the low resource languages.</p>
  </li>
  <li>
    <p>Another reason behind the lagging performance (as compared to bilingual baselines) is the capacity of the multilingual models.</p>
  </li>
  <li>
    <p>Some open problems to consider:</p>

    <ul>
      <li>
        <p>Task Scheduling - How to decide the order in which different language pairs should be trained.</p>
      </li>
      <li>
        <p>Optimization for multitask learning - How to design optimizer, loss functions, etc. that can exploit task similarity.</p>
      </li>
      <li>
        <p>Understanding Transfer:</p>

        <ul>
          <li>
            <p>For the low resource languages, translating multiple languages to English leads to improved performance than translating English to multiple languages.</p>
          </li>
          <li>
            <p>This can be explained as follows: In the first case (many-to-one), the setup is that of a multi-domain model (each source language is a domain). In the second case (one-to-many), the setup is that of multitasking.</p>
          </li>
          <li>
            <p>NMT models seem to be more amenable to transfer across multiple domains than transfer across tasks (since the decoder distribution does not change much).</p>
          </li>
          <li>
            <p>In terms of zero-shot performance, the performance for most language pairs increases as the number of languages change from 10 to 102.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="effect-of-preprocessing-and-vocabulary">Effect of preprocessing and vocabulary</h2>

<ul>
  <li>
    <p>Sentence Piece Model (SPM) is used.</p>
  </li>
  <li>
    <p>Temperature sampling is used to sample vocabulary from different languages.</p>
  </li>
  <li>
    <p>Using smaller vocabulary (and hence smaller sub-word tokens) perform better for low resource languages, probably due to improved generalization.</p>
  </li>
  <li>
    <p>Low and medium resource languages tend to perform better with higher temperatures.</p>
  </li>
</ul>

<h2 id="effect-of-capacity">Effect of Capacity</h2>

<ul>
  <li>Using deeper models improves performance (as compared to the wider models with the same number of parameters) on most language pairs.</li>
</ul>
