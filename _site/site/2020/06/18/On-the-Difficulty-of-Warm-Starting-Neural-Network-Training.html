<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper considers learning scenarios where the training data is available incrementally (and not at once).</p>
  </li>
  <li>
    <p>For example, in some applications, new data is available periodically (e.g., latest news articles come out every day).</p>
  </li>
  <li>
    <p>The paper highlights that, in such scenarios, the conventional wisdom of “warm start” does not apply.</p>
  </li>
  <li>
    <p>When new data is available, it is better to train a new model from scratch than to update the model trained on previously available data.</p>
  </li>
  <li>
    <p>While the two setups lead to similar training performance, the randomly initialized model has a much better generalization performance.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.08475">Link to the paper</a></p>
  </li>
</ul>

<h2 id="basic-batch-updating">Basic Batch Updating</h2>

<ul>
  <li>
    <p>Create two random, equally-sized partitions of the training data.</p>
  </li>
  <li>
    <p>Train the model till convergence on the first half of the data. Then train the model on the entire dataset.</p>
  </li>
  <li>
    <p>Models: ResNet18, MLPs, Logisitic Regression (LR)</p>
  </li>
  <li>
    <p>Dataset: CIFAR10, CIFAR100, SVHN</p>
  </li>
  <li>
    <p>Optimizers: Adam, SGD</p>
  </li>
  <li>
    <p>Warm starting hurts generalization in all the cases.</p>
  </li>
  <li>
    <p>The effect is more pronounced in the case of ResNets and MLPs (compared to LR) and harder CIFAR 10 dataset (as compared to SVHN dataset).</p>
  </li>
</ul>

<h2 id="online-learning">Online Learning</h2>

<h3 id="passive-online-learning">Passive Online Learning</h3>

<ul>
  <li>
    <p>The model is given access to k new learning examples at each iteration.</p>
  </li>
  <li>
    <p>A warm started model reuses the previously initialized model and trains (till convergence) on the new batch of k items.</p>
  </li>
  <li>
    <p>A “randomly initialized” model is trained on all the examples (seen so far) from scratch.</p>
  </li>
  <li>
    <p>Dataset: CIFAR10</p>
  </li>
  <li>
    <p>Model: ResNet18</p>
  </li>
  <li>
    <p>As more training data becomes available, the generalization gap between the two setups increases, and warmup starts hurting generalization.</p>
  </li>
</ul>

<h3 id="active-online-learning">Active Online Learning</h3>

<ul>
  <li>
    <p>In this setup, the learner is trained to sample k new examples to add to the training dataset (using margin-based sampling).</p>
  </li>
  <li>
    <p>Like the previous setup, warmup strategy still hurts generalization.</p>
  </li>
</ul>

<h2 id="transfer-learning">Transfer Learning</h2>

<ul>
  <li>
    <p>Train a Resnet18 model on the CIFAR10 dataset and use this model to warm start training on the SVHN dataset.</p>
  </li>
  <li>
    <p>When a small percentage of the SVHN dataset is used, the setup resembles pretraining / transfer learning and performs better than training from scratch.</p>
  </li>
  <li>
    <p>As the percentage of the SVHN dataset increases, the warmup approach starts underperforming.</p>
  </li>
</ul>

<h2 id="overcoming-warm-start-problem">Overcoming warm start problem</h2>

<ul>
  <li>
    <p>ResNet18 model on CIFAR10 dataset</p>
  </li>
  <li>
    <p>When performing a hyper-parameter sweep over the learning rate and batch size, it is possible to train warm start models to reach the same generalization performance as training from scratch.</p>
  </li>
  <li>
    <p>Though, in that case, there are no computational savings as the warm-started models take about the same time (to converge) as the randomly initialized model.</p>
  </li>
  <li>
    <p>The increased training time indicates that the warm started model probably needs to forget the knowledge from previous training rounds.</p>
  </li>
  <li>
    <p>Warm start Resnet models, that generalize well, have a low correlation to their initialization stage (measured via Pearson correlation coefficient between the model weights).</p>
  </li>
  <li>
    <p>Generalization is damaged even when using a model trained on incomplete data for only a few epochs.</p>
  </li>
  <li>
    <p>For warm start models, the gradient (corresponding to the “new” data) is higher than that for randomly initialized models. This hints that regularisation may help to close the generalization gap. But in practice, regularization helps both the warmup and randomly initialized model.</p>
  </li>
  <li>
    <p>Warm starting only a few layers also does not close the gap.</p>
  </li>
  <li>
    <p>Adding some noise to the warm started model (with the motivation of having a partially random initialization) does help somewhat but also increases the training time.</p>
  </li>
  <li>
    <p>Motivating the problem as an instance of catastrophic forgetting, the authors use the EWC algorithm but report that using EWC hurts model performance.</p>
  </li>
  <li>
    <p>The paper does not propose a solution to the problem but provides a thorough analysis of the problem setup, which is quite useful for understanding the phenomenon itself.</p>
  </li>
</ul>
