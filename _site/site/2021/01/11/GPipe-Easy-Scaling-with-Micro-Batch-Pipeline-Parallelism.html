<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper introduces GPipe, a pipeline parallelism library for scaling networks that can be expressed as a sequence of layers.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1811.06965">Link to the paper</a></p>
  </li>
</ul>

<h2 id="design">Design</h2>

<ul>
  <li>
    <p>Consider training a deep neural network with <em>L</em> layers using <em>K</em> accelerators (say GPUs).</p>
  </li>
  <li>
    <p>Each of the <em>i<sup>th</sup></em> layer has its <em>forward</em> function <em>f<sub>i</sub></em>, <em>backward</em> function <em>b<sub>i</sub></em>, weights <em>w<sub>i</sub></em> and a cost <em>c<sub>i</sub></em> (say the memory footprint or computational time).</p>
  </li>
  <li>
    <p>GPipe partitions this network into <em>K</em> cells and places the <em>i<sup>th</sup></em> cell on the <em>i<sup>th</sup></em> accelerator. Output from the <em>i<sup>th</sup></em> accelerator is passed to the <em>i+1<sup>th</sup></em> accelerator as input.</p>
  </li>
  <li>
    <p>During the forward pass, the input batch (of size <em>N</em>) is divided into <em>M</em> equal micro-batches. These micro-batches are pipelined through the <em>K</em> accelerators one after another.</p>
  </li>
  <li>
    <p>During the backward pass, gradients are computed for each micro-batch. The gradients are accumulated and applied at the end of each minibatch.</p>
  </li>
  <li>
    <p>In batch normalization, the statistics are computed over each micro-batch (used during training) and mini-batch (used during evaluation).</p>
  </li>
  <li>
    <p>Micro-batching improves over the naive mode parallelism approach by reducing the underutilization of resources (due to the networkâ€™s sequential dependencies).</p>
  </li>
</ul>

<h2 id="performance-optimization">Performance Optimization</h2>

<ul>
  <li>
    <p>GPipe supports re-materialization (or checkpointing), i.e., during the forward pass, only the output activations (at partition boundaries) are stored.</p>
  </li>
  <li>
    <p>During backward pass, the forward function is recomputed at each accelerator. This trades off the memory requirement with increased time.</p>
  </li>
  <li>
    <p>One potential downside is that partitioning can introduce some idle time per accelerator (referred to as the bubble overhead). However, with a sufficiently large number of micro-batches ( more than 4 times the number of partitions), the bubble overhead is negligible.</p>
  </li>
</ul>

<h2 id="performance-analysis">Performance Analysis</h2>

<ul>
  <li>
    <p>Two different types of model architectures are compared: AmoebaNet convolutional model and Transformer sequence-to-sequence model.</p>
  </li>
  <li>
    <p>For AmoebaNet, the size of the largest trainable model (on a single 8GB Cloud TPU v2) increases from 82M to 318M. Further, a 1.8 billion parameter model can be trained on 8 accelerators (25x improvement in size using GPipe).</p>
  </li>
  <li>
    <p>For transformers, GPipe scales the model size to 83.9 B parameters with 128 partitions (298x improvement in size compared to a single accelerator).</p>
  </li>
  <li>
    <p>Since the computation is evenly distributed across transformer layers, the training throughput scales almost linearly with the number of devices.</p>
  </li>
  <li>
    <p>Quantitative experiments on ImageNet and multilingual machine translation show that models can be effectively trained using GPipe.</p>
  </li>
</ul>
