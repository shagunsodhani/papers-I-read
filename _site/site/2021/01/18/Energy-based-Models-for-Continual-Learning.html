<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes to use Energy-based Models (EBMs) for Continual Learning.</p>
  </li>
  <li>
    <p>In classification tasks, the standard approach uses a cross-entropy objective function along with a normalized probability distribution.</p>
  </li>
  <li>
    <p>However, cross-entropy reduces all negative classesâ€™ likelihood when updating the model for a given sample, potentially leading to catastrophic forgetting.</p>
  </li>
  <li>
    <p>Classification can be seen as learning an EBM across separate classes.</p>
  </li>
  <li>
    <p>During an update, the energy for a pair of samples and its ground truth class decreases while the energy corresponding to the pairs of sample and negative classes increases.</p>
  </li>
  <li>
    <p>Unlike the cross-entropy loss, EBMs allow choosing the negative classes to update.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2011.12216">Link to the paper</a></p>
  </li>
</ul>

<h2 id="applications-of-ebms-for-continual-learning">Applications of EBMs for Continual Learning</h2>

<ul>
  <li>
    <p>EBMs can be used for class-incremental learning without requiring a replay-buffer or generative model for replay.</p>
  </li>
  <li>
    <p>EBMs can be used for continual learning in setups without task boundaries, i.e., setups where the data distribution can change without a clear separation between tasks.</p>
  </li>
</ul>

<h2 id="ebms">EBMs</h2>

<ul>
  <li>
    <p>Boltzman distribution is used to define the conditional likelihood of label $y$, given an input $x$. ie, $p(y|x) = \frac{exp(E(x, y))}{Z(x)}$ where $Z(x) = \sum_{y \in Y}(-E(x, y))$. Here $E$ is the learnt energy function that maps an input-label pair to a scalar energy value.</p>
  </li>
  <li>
    <p>During training, the contrastive divergence loss is used.</p>
  </li>
  <li>
    <p>During inference, the class, for which the input-class pair has the least energy, is selected as the predicted class.</p>
  </li>
</ul>

<h2 id="ebms-for-continual-learning">EBMs for Continual Learning</h2>

<h3 id="selection-of-negative-samples">Selection of Negative Samples</h3>

<ul>
  <li>
    <p>The paper considers several strategies for the selection of negative samples:</p>

    <ul>
      <li>
        <p>one negative class per sample. The negative class is sampled from the current batch of data. This selection approach performs best.</p>
      </li>
      <li>
        <p>all the negative classes in a batch are used for creating the negative samples.</p>
      </li>
      <li>
        <p>all the classes seen so far in training are used as the negative samples. This approach works the worst in practice.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Given the flexibility of sampling the negative classes, EBMs can be used in the boundary-agnostic setups (where the data distribution can change smoothly without an explicit task boundary).</p>
  </li>
</ul>

<h3 id="energy-network">Energy Network</h3>

<ul>
  <li>
    <p>EBMs take both the sample and the class as the input. The class can be treated as an attention filter to select the most relevant information between the sample and the class.</p>
  </li>
  <li>
    <p>In theory, EBMs can train for any number of classes without knowing the number of classes beforehand. This is an advantage over the softmax-based approaches, where adding new classes requires changing the size of the softmax output layer.</p>
  </li>
</ul>

<h3 id="inference">Inference</h3>

<ul>
  <li>During inference, all the classes seen so far are evaluated via the energy function. The class, which corresponds to the least energy sample-class pair, is returned as the selected class.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>
    <p>Split MNIST</p>
  </li>
  <li>
    <p>Permuted MNIST</p>
  </li>
  <li>
    <p>CIFAR-10</p>
  </li>
  <li>
    <p>CIFAR-100</p>
  </li>
</ul>

<h3 id="results-in-boundary-aware-setting">Results in Boundary-Aware Setting</h3>

<ul>
  <li>
    <p>The paper outperforms the standard continual learning approaches that neither uses a replay-buffer nor a generative model.</p>
  </li>
  <li>
    <p>Additionally, the paper shows that for the same number of parameters, the effective capacity of EMB models is higher than the effective capacity of standard classification models.</p>
  </li>
  <li>
    <p>The paper also shows that standard classification models tend to assign a high probability to new classes for both old and new data. EBMs assign the probability more uniformly (and correctly) across the classes.</p>
  </li>
  <li>
    <p>In an ablation study, the paper shows that both label conditioning and contrastive divergence loss help in improving the performance of EBMs.</p>
  </li>
</ul>

<h3 id="results-in-boundary-agnostic-setting">Results in Boundary-Agnostic Setting</h3>

<ul>
  <li>The performance gains in the boundary-agnostic setting are even more significant than the improvements in the boundary-aware setting.</li>
</ul>
