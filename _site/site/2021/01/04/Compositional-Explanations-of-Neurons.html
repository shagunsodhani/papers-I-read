<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper describes a method to explain/interpret the representations learned by individual neurons in deep neural networks.</p>
  </li>
  <li>
    <p>The explanations are generated by searching for logical forms defined by a set of composition operators (like OR, AND, NOT) over primitive concepts (like water).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.14032">Link to the paper</a></p>
  </li>
</ul>

<h2 id="generating-compositional-explanations">Generating compositional explanations</h2>

<ul>
  <li>
    <p>Given a neural network <em>f</em>, the goal is to explain a neuron’s behavior (of this network) in human-understandable terms.</p>
  </li>
  <li>
    <p><a href="http://netdissect.csail.mit.edu/">Previous work</a> builds on the idea that a good explanation is a description that identifies the inputs for which the neuron activates.</p>
  </li>
  <li>
    <p>Given a set of pre-defined atomic concepts $c \in C$ and a similarity measure $\delta(n, c)$ where $n$ represents the activation of the $n^{th}$ neuron, the explanation, for the $n^{th}$ neuron, is the concept most similar to $n$.</p>
  </li>
  <li>
    <p>For images, a concept could be represented as an image segmentation map. For example, the water concept can be represented by the segments of the images that show water.</p>
  </li>
  <li>
    <p>The similarity can be measured by first thresholding the neuron activations (to get a neuron mask) and then computing the IoU score (or Jaccard Similarity) between the neuron mask and the concept.</p>
  </li>
  <li>
    <p>One limitation of this approach is that the explanations are restricted to pre-defined concepts.</p>
  </li>
  <li>
    <p>The paper expands the set of candidate concepts by considering the logical forms of the atomics concepts.</p>
  </li>
  <li>
    <p>In theory, the search space would explode exponentially. In practice, it is restricted to explanations with at most $N$ atomics concepts, and beam search is performed (instead of exhaustive search).</p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p><strong>Image Classification Setup</strong></p>

    <ul>
      <li>
        <p>Neurons from the final 512-unit convolutional layer of a ResNet-18 trained on the <a href="https://ieeexplore.ieee.org/abstract/document/7968387">Places365 dataset</a>.</p>
      </li>
      <li>
        <p>Probing for concepts from <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Scene_Parsing_Through_CVPR_2017_paper.html">ADE20k scenes dataset</a> with atomic concepts defined by annotations in the <a href="http://netdissect.csail.mit.edu/">Broden dataset</a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>NLI Setup</strong></p>

    <ul>
      <li>
        <p>BiLSTM baseline followed by MLP layers trained on <a href="https://nlp.stanford.edu/projects/snli/">Stanford Natural Language Inference (SNLI) corpus</a>.</p>
      </li>
      <li>
        <p>Probing the penultimate hidden layer (of the MLP component) for sentence-level explanations.</p>
      </li>
      <li>
        <p>Concepts are created using the 2000 most common words in the validation split of the SNLI dataset.</p>
      </li>
      <li>
        <p>Additional concepts are created based on the lexical overlap between premise and hypothesis.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="do-neurons-learn-compositional-concepts">Do neurons learn compositional concepts</h2>

<ul>
  <li>
    <p><strong>Image Classification Setup</strong></p>

    <ul>
      <li>
        <p>As $N$ increases, the mean IoU increases (i.e., the explanation quality increases) though the returns become diminishing beyond $N=10$.</p>
      </li>
      <li>
        <p>Manual inspection of 128 neurons and their length 10 explanations show that 69% neurons learned some meaningful combination of concepts, while 31% learned some unrelated concepts.</p>
      </li>
      <li>
        <p>The meaningful combination of concepts include:</p>

        <ul>
          <li>
            <p>perceptual abstraction that is also lexically coherent (e.g., “skyscraper OR lighthouse OR water tower”).</p>
          </li>
          <li>
            <p>perceptual abstraction that is not lexically coherent (e.g., “cradle OR autobus OR fire escape”).</p>
          </li>
          <li>
            <p>specialized abstraction of the form L1 AND NOT L2 (e.g. (water OR river) AND NOT blue).</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>NLI Setup</strong></p>

    <ul>
      <li>
        <p>As $N$ increases, the mean IoU increases (as in the image classification setup) though the IoU keeps increasing past $N=30$.</p>
      </li>
      <li>
        <p>Many neurons correspond to lexical features. For example, some neurons are gender-sensitive or activate for verbs like sitting, eating or sleeping. Some neurons are activated when the lexical overlap between premise and hypothesis is high.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="do-interpretable-neurons-contribute-to-model-accuracy">Do interpretable neurons contribute to model accuracy?</h2>

<ul>
  <li>
    <p>In image classification setup, the more interpretable the neuron is, the more accurate is the model (when the neuron is active).</p>
  </li>
  <li>
    <p>However, the opposite trend is seen in NLI models. i.e., the more interpretable neurons are less accurate.</p>
  </li>
  <li>
    <p>Key takeaway - interpretability (as measured by the paper) is not correlated with performance. Given a concept space, the identified behaviors may be correlated or anti-correlated with the model’s performance.</p>
  </li>
</ul>

<h2 id="targeting-explanations-to-change-model-behavior">Targeting explanations to change model behavior</h2>

<ul>
  <li>
    <p>The idea is to construct examples that activate (or inhibit) certain neurons, causing a change in the model’s predictions.</p>
  </li>
  <li>
    <p>These adversarial examples are referred to as “copy-paste” adversarial examples.</p>
  </li>
  <li>
    <p>For example, the neuron corresponding to “(water OR river) AND (NOT blue)” is a major contributor for detecting “swimming hole” classes. An adversarial example is created by making the water blue. This prompts the model to predict “grotto” instead of “swimming hole.”</p>
  </li>
  <li>
    <p>Similarly, in the NLI model, a neuron detects the word “nobody” in the hypothesis as highly indicative of contradiction. An adversarial example can be created by adding the word “nobody” to the hypothesis, prompting the model to predict contradiction while the true label should be neutral.</p>
  </li>
  <li>
    <p>These observations support the hypothesis that one can use explanations to create adversarial examples.</p>
  </li>
</ul>
