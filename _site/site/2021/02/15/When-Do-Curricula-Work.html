<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper systematically investigates when does curriculum learning help.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2012.03107">Link to the paper</a></p>
  </li>
</ul>

<h2 id="implicit-curricula">Implicit Curricula</h2>

<ul>
  <li>
    <p>Implicit curricula refers to the order in which a network learns data points when trained using stochastic gradient descent, with iid sampling of data.</p>
  </li>
  <li>
    <p>When training, let us say that the model makes a correct prediction for a given datapoint in the $i^{th}$ epoch (and correct prediction in all the subsequent epochs). The $i^{th}$ epoch is referred to as the <em>learned iteration</em> of the datapoint  (iteration in which the datapoint was learned).</p>
  </li>
  <li>
    <p>The paper studied multiple models (VGG, ResNet, WideResNet, DenseNet, and EfficientNet) with different optimizers (Adam and SGD with momentum).</p>
  </li>
  <li>
    <p>The resulting implicit curricula are broadly consistent within the model families, making the following discussion less dependent on the model architecture.</p>
  </li>
</ul>

<h2 id="explicit-curricula">Explicit Curricula</h2>

<ul>
  <li>When defining an explicit curriculum, three important components stand out.</li>
</ul>

<h3 id="scoring-function">Scoring Function</h3>

<ul>
  <li>
    <p>Maps a data point to a numerical score of <em>difficulty</em>.</p>
  </li>
  <li>
    <p>Choices:</p>

    <ul>
      <li>
        <p>Loss function for a model</p>
      </li>
      <li>
        <p><em>learned iteration</em></p>
      </li>
      <li>
        <p>Estimated c-score - It captures a given model’s consistency to correctly predict a given datapoint’s label when trained on an iid dataset (not containing the datapoint).</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The three scoring functions are computed for two models on the CIFAR dataset.</p>
  </li>
  <li>
    <p>The resulting six scores have a high Spearman Rank correlation. Hence for the rest of the discussion, only the c-score is used.</p>
  </li>
</ul>

<h3 id="pacing-function">Pacing Function</h3>

<ul>
  <li>
    <p>This function, denoted by $g(t)$, controls the size of the training dataset at step $t$.</p>
  </li>
  <li>
    <p>At step $t$, the model would be trained on the first $g(t)$ examples (as per the ordering).</p>
  </li>
  <li>
    <p>Choices: logarithmic, exponential, step, linear, quadratic, and root.</p>
  </li>
</ul>

<h3 id="order">Order</h3>

<ul>
  <li>
    <p>Order in which the data points are picked:</p>

    <ul>
      <li>
        <p><em>Curriculum</em> - Ordering points from lowest score to highest and training on the easiest data points first.</p>
      </li>
      <li>
        <p><em>Anti Curriculum</em> - Ordering points from highest score to lowest and training on the hardest data points first.</p>
      </li>
      <li>
        <p><em>Random</em> - Randomly selecting the data points to train on.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>
    <p>The paper performed a hyperparameter sweep over 180 pacing functions and three orderings for three random seeds over the CIFAR10 and CIFAR100 datasets. For both the datasets, the best performance is obtained with random ordering, indicating that curricula did not give any benefits.</p>
  </li>
  <li>
    <p>However, the curriculum is useful when the number of training iterations is small.</p>
  </li>
  <li>
    <p>It also helps with noisy data training (which is simulated by randomly permuting the labels).</p>
  </li>
  <li>
    <p>The observations for the smaller CIFAR10/100 dataset generalize to slightly larger datasets like FOOD101 and FOOD101N.</p>
  </li>
</ul>

