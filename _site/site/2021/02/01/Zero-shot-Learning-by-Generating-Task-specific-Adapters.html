<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper introduces HYPTER - a framework for zero-shot learning (ZSL) in text-to-text transformer models by training a <a href="https://shagunsodhani.com/papers-I-read/HyperNetworks"><strong>Hyp</strong>erNetwork</a> to generate task-specific <a href="https://arxiv.org/abs/1902.00751">adap<strong>ter</strong>s</a> from task descriptions.</p>
  </li>
  <li>
    <p>The focus is on <em>in-task</em> zero-shot learning (e.g., learning to predict an unseen class or relation) and not on <em>cross-task</em> learning (e.g., training on sentiment analysis and evaluating on question-answering task).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2101.00420">Link to the paper</a></p>
  </li>
</ul>

<h2 id="terminology">Terminology</h2>

<ul>
  <li>
    <p><em>Task</em> - a NLP task, like classification or question answering.</p>
  </li>
  <li>
    <p><em>Sub-task</em></p>

    <ul>
      <li>
        <p>A class/relation/question within a task.</p>
      </li>
      <li>
        <p>Denotes by a tuple $(d, D)$ where $d$ is the language description while $D$ represents the subtaskâ€™s dataset.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>Develop ZSL approach for transfer to new subtasks within a task, using the task description available for each subtask.</li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>HYPTER has two main parts:</p>

    <ul>
      <li>
        <p>Main network</p>

        <ul>
          <li>
            <p>A pretrained text-to-text network</p>
          </li>
          <li>
            <p>Instantiated as a BERT-Base/Large</p>
          </li>
        </ul>
      </li>
      <li>
        <p>HyperNetwork</p>

        <ul>
          <li>Generates the weights for adapter networks that will be plugged into the main network.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>HyperNetwork has two parts:</p>

    <ul>
      <li>
        <p>Encoder</p>

        <ul>
          <li>
            <p>Encodes the task description</p>
          </li>
          <li>
            <p>Instantiated as a RoBERTa-Base model</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Decoder</p>

        <ul>
          <li>
            <p>Decodes the encoding into weights for multiple adapters (in parallel)</p>
          </li>
          <li>
            <p>Instantiated as a Feedforward Network</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>The model trains in two phases:</p>

    <ul>
      <li>
        <p>Main network is trained on all the data by concatenating the task description with the input.</p>
      </li>
      <li>
        <p>Adapters are trained by sampling a task from the train set while keeping the main network frozen.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>While the idea is very promising and interesting, the evaluation felt quite limited. It uses just two datasets <a href="https://leaderboard.allenai.org/zest/submissions/public">Zero-shot learning from Task Descriptions</a> and <a href="https://eval.ai/web/challenges/challenge-page/689/overview">Zero-shot Relation Extraction</a> and shows some improvements over the baseline of directly finetuning with task descriptions as the prompt.</li>
</ul>
