<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper describes YouTube’s deep learning-based recommendation system.</p>
  </li>
  <li>
    <p><a href="https://research.google/pubs/pub45530/">Link to the paper</a></p>
  </li>
</ul>

<h2 id="challenges">Challenges</h2>

<ul>
  <li>
    <p>Scale - Very large number of users and videos.</p>
  </li>
  <li>
    <p>Freshness - Very large number of videos uploaded every hour. The recommendation system should take these new videos into account as well.</p>
  </li>
  <li>
    <p>Noise - User satisfaction needs to be modeled from noisy implicit feedback signal as the explicit signal is very sparse.</p>
  </li>
</ul>

<h2 id="system-overview">System Overview</h2>

<ul>
  <li>
    <p>Two neural networks: one for candidate generation and another one for ranking.</p>
  </li>
  <li>
    <p>Metrics</p>

    <ul>
      <li>
        <p>Offline metrics like precision, recall, ranking loss</p>
      </li>
      <li>
        <p>A/B testing via live experiments</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="candidate-generation">Candidate Generation</h3>

<ul>
  <li>
    <p>Input: events from a user’s YouTube activity history.</p>
  </li>
  <li>
    <p>Output: small subset (hundreds) of videos.</p>
  </li>
  <li>
    <p>Approach:</p>

    <ul>
      <li>
        <p>Recommendation is modeled as extreme multiclass classification.</p>
      </li>
      <li>
        <p>Predict the video (from a corpus) that a user will watch at a given time.</p>
      </li>
      <li>
        <p>The neural network’s task is to learn useful user embeddings, given the user’s context and history.</p>
      </li>
      <li>
        <p>For each positive class (relevant video), negative classes (non-relevant videos) are sampled from the video corpus.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Model Architecture</p>

    <ul>
      <li>
        <p>A feedforward network with input as user embeddings and context embeddings (watch history).</p>
      </li>
      <li>
        <p>Watch history is a variable-length sequence of video ids, where each video id is mapped to an embedding.</p>
      </li>
      <li>
        <p>The sequence of video ids is mapped to a sequence of embeddings, and this sequence is averaged to obtain fixed-sized embedding.</p>
      </li>
      <li>
        <p>Additional signals like demographic features and search query embeddings can be added along with the context embeddings.</p>
      </li>
      <li>
        <p>The age of a video is also used as a feature during training to account for the freshness of the content. This feature is set to zero (or slightly negative) during inference.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Other Insights</p>

    <ul>
      <li>
        <p>Training examples are generated from all YouTube watches, including the watches from the videos embedded on other sites, to surface new content.</p>
      </li>
      <li>
        <p>Generating the same number of training examples per user is important to avoid a small set of active users from dominating the model training.</p>
      </li>
      <li>
        <p>Predicting a user’s next watch leads to better results than predicting a randomly held-out watch. This can be attributed to the general consumption pattern of videos (e.g., episodes are usually watched in order).</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ranking">Ranking</h3>

<ul>
  <li>
    <p>Input: list of candidate videos to rank from.</p>
  </li>
  <li>
    <p>Output: score for each video.</p>
  </li>
  <li>
    <p>Approach</p>

    <ul>
      <li>A feedforward network (similar to candidate generation model) trained using logistic regression loss.</li>
    </ul>
  </li>
  <li>
    <p>Feature representation</p>

    <ul>
      <li>
        <p>Different types of features: categorical vs. continuous, univalent vs. multivalent, describes video vs. describes user or context.</p>
      </li>
      <li>
        <p>Important signals include user’s interaction with the video (or similar videos), which source/channel added the video to the candidate set.</p>
      </li>
      <li>
        <p>Embeddings are shared across features. For example, the representation for a video id remains the same, irrespective of whether it is being used for representing the “video to recommend” or the “last seen video.”</p>
      </li>
      <li>
        <p>Feature normalization and transformations like exponents (square or square root) for continuous variables improve the performance.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>To model the expected watch time, the logistic regression loss is weighted by the observed watch time. For example, if a video was watched, its weight is given by the observed watch time, and if the video was not watched, its weight is set to 1.</p>
  </li>
  <li>
    <p>In practice, this means that the logistic regression model learns odds that approximate the expected watch time of the video.</p>
  </li>
</ul>
