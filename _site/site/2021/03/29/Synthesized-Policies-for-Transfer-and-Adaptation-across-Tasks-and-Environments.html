<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper studies transfer learning in RL, focusing on simultaneous transfer across both tasks and environments.</p>
  </li>
  <li>
    <p>The key idea is to learn task and environment embeddings and compose them using a meta-rule, and the proposed approach is called SYNPO (Synthesized Policies).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.03276">Link to the paper</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>Three settings considered:</p>

    <ul>
      <li>
        <p><em>S1</em>: Transfer to a new (environment, task) pair when the agent has been trained on the environment and the task before (but not simultaneously).</p>
      </li>
      <li>
        <p><em>S2</em>: Transfer to a new (environment, task) pair where either the environment or the task is not seen previously.</p>
      </li>
      <li>
        <p><em>S3</em>: Transfer to a new (environment, task) pair where neither the environment nor the task is seen previously.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In the second and third settings, the agent is allowed to collect some data in the new environment or task.</p>
  </li>
  <li>
    <p>The (environment, task) combinations that the agent has seen during training are referred to as <em>seen</em> combinations, while the remaining combinations are referred to as the <em>unseen</em> combinations.</p>
  </li>
  <li>
    <p>The key idea is to:</p>

    <ul>
      <li>
        <p>learn embeddings of environments and tasks</p>
      </li>
      <li>
        <p>use these embeddings to compose a policy (parameterized as the linear combination of the policy basis).</p>
      </li>
    </ul>
  </li>
  <li>
    <p>A disentanglement objective is used to decouple the task and environment embedding.</p>
  </li>
</ul>

<h3 id="policy-composition">Policy Composition</h3>

<ul>
  <li>
    <p>Given an (environment, task) pair $z = (\epsilon, \tau)$, the policy is given as $\pi_z(a|s) \propto exp(\psi_s^TU(e_{\epsilon}, e_{\tau})\phi_{a} + b_{\pi} )) $.</p>
  </li>
  <li>
    <p>Here $b_{\pi}$ is a scalar bias, $\psi_{s}$ and $\phi_{a}$ are state and action representations, $U$ is parameterized as the linear comination of $K$ basis matrices $\Theta_k$</p>
  </li>
  <li>
    <p>$U(e_{\epsilon}, e_{\tau}) = \sum_{k=1}^{K}\alpha_k(e_{\epsilon}, e_{\tau})\Theta_k$.</p>
  </li>
  <li>
    <p>The basis matrices (denoted by $\Theta_k$) are shared across tasks while the coefficients ($\alpha_k$) are specific to the (environment, task) pair.</p>
  </li>
  <li>
    <p>During training, the agent also predicts rewards using the same set of basis but different coefficients.</p>
  </li>
</ul>

<h3 id="disentangling-environment-and-task-embeddings">Disentangling environment and task embeddings</h3>

<ul>
  <li>
    <p>Given an (environment, task) pair, the agent is trained to decode the environment (and task) given the agent’s trajectory.</p>
  </li>
  <li>
    <p>The sequence of state-action pairs (in the trajectory) is mapped to a sequence of state-action representations, given by $\psi_s^T\Theta_k\phi_{a}$</p>
  </li>
</ul>

<h2 id="experiment-setup">Experiment Setup</h2>

<ul>
  <li>The agent is trained (and evaluated) on imitation learning (mostly) and reinforcement learning setup.</li>
</ul>

<h3 id="environments">Environments</h3>

<ul>
  <li>
    <p>GRIDWORLD</p>

    <ul>
      <li>
        <p>Twenty $16 \times 16$ gird-aligned mazes that are similar in appearance but differ in topology.</p>
      </li>
      <li>
        <p>The task is to collect colored blocks in a given order. In each task, the starting position of the agent and the position of the blocks is randomized.</p>
      </li>
      <li>
        <p>Each environment has 20 tasks, leading to a total of 400 (environment, task) combinations.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1712.05474">THOR</a></p>

    <ul>
      <li>
        <p>This is a 3D simulator where the agent is placed in indoor photo-realistic scenes.</p>
      </li>
      <li>
        <p>The task is the search for objects and perform actions like “put cabbage on the fridge.”</p>
      </li>
      <li>
        <p>The setup uses 19 scenes (environments), with each environment comprising of 21 tasks.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="baselines">Baselines</h3>

<ul>
  <li>
    <p>MLPs that concatenate state, environment embeddings, and task embedding.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1606.05312">Successor feature model</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1609.07088">Module Network</a></p>
  </li>
  <li>
    <p>Multi-task Learning where the distinction between the environments is ignored.</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>GRIDWORLD</p>

    <ul>
      <li>
        <p>In the first setting (<em>S1</em>)</p>

        <ul>
          <li>
            <p>SYNPO outperforms all the baselines.</p>
          </li>
          <li>
            <p>As the agent is trained on more (environment, task) combinations, its performance on the unseen combinations improves. This trend saturates when the <em>seem/total</em> ratio reaches about 0.4 (i.e., training on 40% of all the combinations).</p>
          </li>
          <li>
            <p>Task disentanglement is more important than environment disentanglement.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>In the second and third setting (<em>S2</em> and <em>S3</em>)</p>

        <ul>
          <li>
            <p>The agent uses one demonstration from each test pair to finetune the embeddings.</p>
          </li>
          <li>
            <p><em>S2</em> is an easier setting than <em>S3</em>.</p>
          </li>
          <li>
            <p>Transfer learning across tasks is easier than transfer learning across environments.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>THOR</p>

    <ul>
      <li>SYNPO outperforms all the baselines on both seen and unseen combinations.</li>
    </ul>
  </li>
</ul>

