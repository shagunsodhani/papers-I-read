<h2 id="introduction">Introduction</h2>

<ul>
  <li>For the task of <a href="https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering">Visual Question Answering</a>, decompose a question into its linguistic substructures and train a neural network module for each substructure.</li>
  <li>Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.</li>
  <li>Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.</li>
  <li>The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.</li>
  <li><a href="https://arxiv.org/abs/1511.02799">Link to the paper</a></li>
</ul>

<h2 id="inspiration">Inspiration</h2>

<ul>
  <li>Questions tend to be compositional.</li>
  <li>Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.</li>
  <li>Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.</li>
</ul>

<h2 id="neural-module-network-for-vqa">Neural Module Network for VQA</h2>

<ul>
  <li>Training samples of form <em>(w, x, y)</em>
    <ul>
      <li><em>w</em> - Natural Language Question</li>
      <li><em>x</em> - Images</li>
      <li><em>y</em> - Answer</li>
    </ul>
  </li>
  <li>Model specified by collection of modules <em>{m}</em> and a network layout predictor <em>P</em>.</li>
  <li>Model instantiates a network based on <em>P(w)</em> and uses that to encode a distribution <em>P(y|w, x, model_params)</em></li>
</ul>

<h2 id="modules">Modules</h2>

<ul>
  <li>Find: Finds objects of interest.</li>
  <li>Transform: Shift regions of attention.</li>
  <li>Combine: Merge two attention maps into a single one.</li>
  <li>Describe: Map a pair of attention and input image to a distribution over the labels.</li>
  <li>Measure: Map attention to a distribution over the labels.</li>
</ul>

<h2 id="natural-language-question-to-networks">Natural Language Question to Networks</h2>

<ul>
  <li>Map question to the layout which specifies the set of modules and connections between them.</li>
  <li>Assemble the final network using the layout.</li>
  <li>Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.</li>
  <li>eg “what is the colour of the truck?” becomes “colour(truck)”</li>
  <li>The symbolic representation is mapped to a layout:
    <ul>
      <li>All leaves become <em>find</em> module.</li>
      <li>All internal nodes become <em>transform/combine</em> module.</li>
      <li>All root nodes become <em>describe/measure</em> module.</li>
    </ul>
  </li>
</ul>

<h2 id="answering-natural-language-question">Answering Natural Language Question</h2>

<ul>
  <li>Final model combines output from a simple LSTM question encoder with the output of the neural module network.</li>
  <li>This helps in modelling the syntactic and semantic regularities of the question.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>Since some modules are updated more frequently than others, adaptive per weight learning rates are better.</li>
  <li>The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).</li>
  <li>Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.</li>
  <li>Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.</li>
</ul>

