<h2 id="problem-statement">Problem Statement</h2>

<ul>
  <li>A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.</li>
  <li>But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.</li>
  <li>What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.</li>
  <li>The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.</li>
  <li>It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.</li>
  <li><a href="https://vision.cornell.edu/se3/conditional-similarity-networks/">Link to the paper</a></li>
</ul>

<h2 id="conditional-similarity-networks">Conditional Similarity Networks</h2>

<ul>
  <li>Given an image, <em>x</em>, learn a non-linear feature embedding <em>f(x)</em> such that for any 2 images <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em>, the euclidean distance between <em>f(x<sub>1</sub>)</em> and <em>f(x<sub>2</sub>)</em> reflects their similarity.</li>
</ul>

<h3 id="conditional-similarity-triplets">Conditional Similarity Triplets</h3>

<ul>
  <li>Given a triplet of images <em>(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>)</em> and a condition <em>c</em> (the notion of similarity), an oracle (say crowd) is used to determmine if <em>x<sub>1</sub></em> is more similar to <em>x<sub>2</sub></em> or <em>x<sub>3</sub></em> as per the given criteria <em>c</em>.</li>
  <li>In general, for images <em>i, j, l</em>, the triplet <em>t</em> is ordered {i, j, l | c} if <em>i</em> is more similar to <em>j</em> than <em>l</em>.</li>
</ul>

<h3 id="learning-from-triplets">Learning From Triplets</h3>

<ul>
  <li>Define a loss function <em>L<sub>T</sub>()</em> to model the similarity structure over the triplets.</li>
  <li><em>L<sub>T</sub>(i, j, l) = max{0, D(i, j) - D(i, l) + h}</em> where <em>D</em> is the euclidean distance function and <em>h</em> is the similarity scalar margin to prevent trivial solutions.</li>
  <li>To model conditional similarities, masks <em>m</em> are defined as <em>m = σ(β)</em> where σ is the RELU unit and β is a set of parameters to be learnt.</li>
  <li><em>m<sub>c</sub></em> denotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.</li>
  <li>The euclidean function <em>D</em> now computes the masked distance (<em>f(i, c)m<sub>c</sub></em>) between the two given images.</li>
  <li>Two regularising terms are also added - L2 norm for <em>D</em> and L1 norm for <em>m</em>.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>Fonts dataset by Bernhardsson
    <ul>
      <li>3.1 million 64 by 64-pixel grey scale images.</li>
    </ul>
  </li>
  <li>Zappos50k shoe dataset
    <ul>
      <li>Contains 50,000 images of individual richly annotated shoes.</li>
      <li>Characteristics of interest:
        <ul>
          <li>Type of the shoes (i.e., shoes, boots, sandals or slippers)</li>
          <li>Suggested gender of the shoes (i.e., for women, men, girls or boys)</li>
          <li>Height of the shoes’ heels (0 to 5 inches)</li>
          <li>Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="models">Models</h3>

<ul>
  <li>Initial model for the experiments is a ConvNet pre-trained on ImageNet</li>
  <li><strong>Standard Triplet Network</strong>
    <ul>
      <li>Learn from all available triplets jointly as if they have the same notion of similarity.</li>
    </ul>
  </li>
  <li><strong>Set of Task Specific Triplet Networks</strong>
    <ul>
      <li>Train n separate triplet networks such that each is trained on a single notion of similarity.</li>
      <li>Needs far more parameters and compute.</li>
    </ul>
  </li>
  <li><strong>Conditional Similarity Networks - fixed disjoint masks</strong>
    <ul>
      <li>In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.</li>
      <li>Aims to learn a fully disjoint embedding.</li>
    </ul>
  </li>
  <li><strong>Conditional Similarity Networks - learned masks</strong>
    <ul>
      <li>Learns all the components - conv filters, embedding and the masks.</li>
    </ul>
  </li>
  <li>Refer paper for details on hyperparameters.</li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.</li>
  <li>The learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.</li>
  <li>Order of performance:
    <ul>
      <li>CSNs with learned masks &gt; CSNs with fixed masks &gt; Task-specific networks &gt; standard triplet network.</li>
      <li>Though CSNs with learned masks require more training data.</li>
    </ul>
  </li>
  <li>CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.</li>
  <li>This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability</li>
</ul>
