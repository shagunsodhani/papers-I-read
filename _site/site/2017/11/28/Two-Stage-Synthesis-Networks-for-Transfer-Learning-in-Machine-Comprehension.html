<h2 id="introduction">Introduction</h2>

<ul>
  <li>The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.</li>
  <li>
    <p>The problem is the following:</p>

    <ul>
      <li>
        <p>We have a domain D<sub>S</sub> for which we have labelled dataset of question-answer pairs and another domain D<sub>T</sub> for which we do not have any labelled dataset.</p>
      </li>
      <li>
        <p>We use the data for domain D<sub>S</sub> to train SynNet and use that to generate synthetic question-answer pairs for domain D<sub>T</sub>.</p>
      </li>
      <li>
        <p>Now we can train a machine comprehension model M on D<sub>S</sub> and finetune using the synthetic data for D<sub>T</sub>.</p>
      </li>
    </ul>
  </li>
  <li><a href="https://www.microsoft.com/en-us/research/publication/two-stage-synthesis-networks-transfer-learning-machine-comprehension/">Link to the paper</a></li>
</ul>

<h2 id="synnet">SynNet</h2>

<ul>
  <li>
    <p>Works in two stages:</p>

    <ul>
      <li>Answer Synthesis - Given a text paragraph, generate an answer.</li>
      <li>Question Synthesis - Given a text paragraph and an answer, generate a question.</li>
    </ul>
  </li>
</ul>

<h3 id="answer-synthesis-network">Answer Synthesis Network</h3>

<ul>
  <li>Given the labelled dataset for D<sub>S</sub>, generate a labelled dataset of &lt;word, tag&gt; pair such that each word in the given paragraph is assigned one of the 4 tags:
    <ul>
      <li>IOB<sub>start</sub> - if it is the starting word of an answer</li>
      <li>IOB<sub>mid</sub> - if it is the intermediate word of an answer</li>
      <li>IOB<sub>end</sub> - if it is the ending word of an answer</li>
      <li>IOB<sub>none</sub> - if it is not part of any answer</li>
    </ul>
  </li>
  <li>
    <p>For training, map the words to their GloVe embeddings and pass through a Bi-LSTM. Next, pass them through two-FC layers followed by a softmax layer.</p>
  </li>
  <li>For the target domain D<sub>T</sub>, all the consecutive word spans where no label is IOB<sub>none</sub> are returned as candidate answers.</li>
</ul>

<h3 id="question-synthesis-network">Question Synthesis Network</h3>

<ul>
  <li>
    <p>Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.</p>
  </li>
  <li>
    <p>Map each word in the paragraph to their GloVe embedding. After the word vector, append a ‘1’ if the word was part of the candidate answer else append a ‘0’.</p>
  </li>
  <li>
    <p>Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far. Decoding is stopped when “END” token is produced.</p>
  </li>
  <li>
    <p>The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary. To account for such words, a copying mechanism is also incorporated.</p>
  </li>
  <li>
    <p>At each time step, a Pointer Network (C<sub>P</sub>) and a Vocabulary Predictor (V<sub>P</sub>) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.</p>
  </li>
  <li>
    <p>At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.</p>
  </li>
</ul>

<h3 id="machine-comprehension-model">Machine Comprehension Model</h3>

<ul>
  <li>Given any MC model, first train it over domain D<sub>S</sub> and then fine-tune using the artificial questions generated using D<sub>T</sub>.</li>
</ul>

<h3 id="implementation-details">Implementation Details</h3>

<ul>
  <li>
    <p><strong>Data Regularization</strong> - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.</p>
  </li>
  <li>
    <p>At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.</p>
  </li>
  <li>
    <p><strong>Checkpoint Averaging</strong> - Use the different checkpointed models to average the answer likelihood before running DP.</p>
  </li>
  <li>
    <p>Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -&gt; NewsQA). Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)</p>
  </li>
</ul>

