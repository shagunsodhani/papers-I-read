<h2 id="introduction">Introduction</h2>

<ul>
  <li>Existing word embedding models like <a href="https://gist.github.com/shagunsodhani/176a283e2c158a75a0a6">Skip-Gram</a>, <a href="https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8">GloVe</a> etc map words to fixed sized vectors in a low dimensional vector space.</li>
  <li>This fixed point setting cannot capture uncertainty about representation.</li>
  <li>Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.</li>
  <li>The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.</li>
  <li>This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.</li>
  <li><a href="https://arxiv.org/abs/1412.6623">Link to the paper</a></li>
  <li><a href="https://github.com/seomoz/word2gauss">Implementation</a></li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>KL divergence is used as the asymmetric distance function for comparing the distributions.</li>
  <li>Unlike the Word2Vec model, the proposed model uses ranking-based loss.</li>
</ul>

<h3 id="similarity-measures-used">Similarity Measures used</h3>

<ul>
  <li>
    <p><strong>Symmetric Similarity</strong></p>
  </li>
  <li>For two gaussian distributions, <em>P<sub>i</sub></em> and <em>P<sub>j</sub></em>, compute the inner product <em>E(P<sub>i</sub>, P<sub>j</sub>)</em> as <em>N(0; mean<sub>i</sub> - mean<sub>j</sub>, sigma<sub>i</sub> + sigma<sub>j</sub>)</em>.</li>
  <li>Compute the gradient of <em>mean</em> and <em>sigma</em> with respect to <em>log(E)</em>.</li>
  <li>
    <p>The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.</p>
  </li>
  <li>
    <p><strong>Asymmetric Similarity</strong></p>
  </li>
  <li>Use KL divergence to encode the context distribution.</li>
  <li>The benefit over the symmetric setting is that now entailment type relations can also be modeled.</li>
  <li>For example, a low KL divergence from x to y indicates that y can be encoded as x or that y “entails” x.</li>
</ul>

<h2 id="learning">Learning</h2>

<ul>
  <li>One of the two notions of similarity is chosen and max-margin is used as the loss function.</li>
  <li>Mean is regularized by adding a simple constraint on the L2-norm.</li>
  <li>For covariance matrix, the eigenvalues are constrained to lie within a hypercube. This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.</li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>Polysemous words have higher variance in their word embeddings as compared to specific words.</li>
  <li>KL divergence (with diagonal covariance) outperforms other models.</li>
  <li>Simple tree hierarchies can also be modeled by embedding into the Gaussian space. A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.</li>
  <li>For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.</li>
</ul>

<h2 id="future-work">Future Work</h2>

<ul>
  <li>Use combinations of low rank and diagonal matrices for covariances.</li>
  <li>Improved optimisation strategies.</li>
  <li>Trying other distributions like Student’s-t distribution.</li>
</ul>
