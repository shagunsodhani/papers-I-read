<h2 id="introduction">Introduction</h2>
<ul>
  <li>The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.</li>
  <li>Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.</li>
  <li><a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf">Link to the paper</a></li>
</ul>

<h2 id="description-of-the-system">Description of the system</h2>

<ul>
  <li>The system described in the paper uses <a href="http://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-056-R1-07-027"><strong>arc-standard</strong> system</a> (a greedy, transition-based dependency parsing system).</li>
  <li>Words, POS tags and arc labels are represented as d dimensional vectors.</li>
  <li>S<sup>w</sup>, S<sup>t</sup>, S<sup>l</sup> denote the set of words, POS and labels respectively.</li>
  <li>Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.</li>
  <li>Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such.</li>
  <li>Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).</li>
  <li>Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.</li>
  <li>Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.</li>
  <li>This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.</li>
  <li>L2-regularization term is also added to the loss.</li>
  <li>During inference, a greedy decoding strategy is used and transition with the highest score is chosen.</li>
  <li>The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>Dataset
    <ul>
      <li>English Penn Treebank (PTB)</li>
      <li>Chinese Penn Treebank (CTB)</li>
    </ul>
  </li>
  <li>Two dependency representations used:
    <ul>
      <li>CoNLL Syntactic Dependencies (CD)</li>
      <li>Stanford Basic Dependencies (SD)</li>
    </ul>
  </li>
  <li>Metrics:
    <ul>
      <li>Unlabeled Attached Scores (UAS)</li>
      <li>Labeled Attached Scores (LAS)</li>
    </ul>
  </li>
  <li>Benchmarked against:
    <ul>
      <li>Greedy arc-eager parser</li>
      <li>Greedy arc-standard parser</li>
      <li>Malt-Parser</li>
      <li>MSTParser</li>
    </ul>
  </li>
  <li>Results
    <ul>
      <li>The system proposed in the paper outperforms all other parsers in both speed and accuracy.</li>
    </ul>
  </li>
</ul>

<h2 id="analysis">Analysis</h2>

<ul>
  <li>Cube function gives a 0.8-1.2% improvement over tanh.</li>
  <li>Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.</li>
  <li>Using POS and labels gives an improvement of 1.7% and 0.4% respectively.</li>
</ul>
