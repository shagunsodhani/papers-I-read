<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper presents a new activation function called Swish with formulation <em>f(x) = x.sigmod(x)</em> and its parameterised version called Swish-β where <em>f(x, β) = 2x.sigmoid(β.x)</em> and β is a training parameter.</p>
  </li>
  <li>
    <p>The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1710.05941">Link to the paper</a></p>
  </li>
</ul>

<h2 id="properties-of-swish">Properties of Swish</h2>

<ul>
  <li>
    <p><img src="https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/Swish/plot.png" alt="Plot Of Swish" /></p>
  </li>
  <li>
    <p>Smooth, non-monotonic function.</p>
  </li>
  <li>
    <p>Swish-β can be thought of as a smooth function that interpolates between a linear function and RELU.</p>
  </li>
  <li>
    <p>Uses self-gating mechanism (that is, it uses its own value to gate itself). Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.</p>
  </li>
  <li>
    <p>Being unbounded on the x&gt;0 side, it avoids saturation when training is slow due to near 0 gradients.</p>
  </li>
  <li>
    <p>Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten.</p>
  </li>
  <li>
    <p>Since the Swish function is smooth, the output landscape and the loss landscape are also smooth. A smooth landscape should be more traversable and less sensitive to initialization and learning rates.</p>
  </li>
</ul>

<h2 id="criticism">Criticism</h2>

<ul>
  <li>Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU.</li>
</ul>
