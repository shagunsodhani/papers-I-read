<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words.</p>
  </li>
  <li>
    <p>Learning representations for OOV words directly on the end task often results in poor representation.</p>
  </li>
  <li>
    <p>The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship).</p>
  </li>
  <li>
    <p>The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc.</p>
  </li>
  <li>
    <p>The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task.</p>
  </li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>Given a rare word <em>w</em>, let <em>d(w) = &lt;x<sub>1</sub>, x<sub>2</sub>…&gt;</em> denote its defination where <em>x<sub>i</sub></em> are words.</p>
  </li>
  <li>
    <p><em>d(w)</em> is fed to a <em>defination reader</em> network <em>f</em> (LSTM) and its last state is used as the <em>defination embedding e<sub>d</sub>(w)</em></p>
  </li>
  <li>
    <p>In case <em>w</em> has multiple definitions, the embeddings are combined using mean pooling.</p>
  </li>
  <li>
    <p>The approach can be extended to in-vocabulary words as well by using the <em>definition embedding</em> of such words to update their original embeddings.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>Auxiliary data sources
    <ul>
      <li>Word definitions from WordNet</li>
      <li>Spelling of words</li>
    </ul>
  </li>
  <li>
    <p>The proposed approach was tested on following tasks:</p>

    <ul>
      <li>Extractive Question Answering over SQuAD
        <ul>
          <li>Base model from <a href="https://arxiv.org/abs/1611.01604">Xiong et al. 2016</a></li>
        </ul>
      </li>
      <li>Entailment Prediction over SNLI corpus
        <ul>
          <li>Base models from <a href="https://nlp.stanford.edu/pubs/snli_paper.pdf">Bowman et al. 2015</a> and <a href="https://arxiv.org/abs/1609.06038">Chen et al. 2016</a></li>
        </ul>
      </li>
      <li>One Billion Words Language Modelling</li>
    </ul>
  </li>
  <li>
    <p>For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one.</p>
  </li>
  <li>While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly.</li>
</ul>

<h2 id="future-work">Future Work</h2>

<ul>
  <li>
    <p>Multi-token words like “San Francisco” are not accounted for now.</p>
  </li>
  <li>
    <p>The model does not handle the rare words which appear in the definition and just replaces them by the <UNK> token. Making the model recursive would be a useful addition.</UNK></p>
  </li>
</ul>
