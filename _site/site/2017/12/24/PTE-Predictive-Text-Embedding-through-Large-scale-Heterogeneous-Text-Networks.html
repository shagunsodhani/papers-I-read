<h1 id="introduction">Introduction</h1>

<ul>
  <li>
    <p>Unsupervised text embeddings can be generalized for different tasks but they have weaker predictive powers (as compared to end-to-end trained deep learning methods) for any particular task. But the deep learning techniques are expensive and need a large amount of supervised data and a large number of parameters to tune.</p>
  </li>
  <li>
    <p>The paper introduces Predictive Text Embedding (PTE) - a semi-supervised approach which learns an effective low dimensional representation using a large amount of unsupervised data and a small amount of supervised data.</p>
  </li>
  <li>
    <p>The work can be extended to general information networks as well as classic techniques like MDS, Iso-map, Laplacian EigenMaps etc do not scale well for large graphs.</p>
  </li>
  <li>
    <p>Further, this model can be applied to heterogeneous networks as well unlike the previous works <a href="https://arxiv.org/abs/1503.03578">LINE</a> and <a href="https://arxiv.org/abs/1403.6652">DeepWalk</a> which work on homogeneous networks only.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1508.00200">Link to the paper</a></p>
  </li>
</ul>

<h1 id="approach">Approach</h1>

<ul>
  <li>
    <p>The paper proposes 3 different kinds of networks:</p>

    <ul>
      <li><strong>Word-Word Network</strong> which captures the word co-occurrence information (local level).</li>
      <li><strong>Word-Document Network</strong> which captures the word-document co-occurrence information (local + document level).</li>
      <li><strong>Word-Label Network</strong> which captures the word-label co-occurrence information (bipartite graph).</li>
    </ul>
  </li>
  <li>
    <p>All 3 graphs are integrated into one heterogeneous text network.</p>
  </li>
  <li>
    <p>First, the authors extend their previous work, LINE, for heterogenous bipartite text networks as explained:</p>

    <ul>
      <li>
        <p>Given a bipartite graph <em>G = (V<sub>A</sub> \bigcup V<sub>B</sub>, E)</em> , where <em>V<sub>A</sub> and V<sub>B</sub></em> are disjoint set of vertices, the conditional probability of <em>v<sub>a</sub></em> (in set <em>V<sub>A</sub></em>) being generated by <em>v<sub>b</sub></em> (in set <em>V<sub>B</sub></em>) is given as the softmax score between embeddings of <em>v<sub>a</sub></em> and <em>v<sub>b</sub></em> and normalised by the sum of exponentials of dot products between <em>v<sub>b</sub></em>  and all nodes in <em>V<sub>A</sub></em>.</p>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>The second order proximity can be determined by the conditional distributions *p(.</td>
              <td>v<sub>j</sub>)*p(.</td>
              <td>v<sub>j</sub>)*.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>The objective to be minimised the KL divergence between the conditional distribution <em>p(.\v<sub>j</sub>)</em> and the emperical distribution <em>p<sup>^</sup>(.\v<sub>j</sub>)</em> (given as w<sub>i, j</sub>/deg<sub>j</sub>).</p>
      </li>
      <li>The objective can be further simplified and optimised using SGD with edge sampling and negative sampling.</li>
    </ul>
  </li>
  <li>
    <p>Now, the 3 individual networks can all be interpreted as bipartite networks. So node representation of all the 3 individual networks is obtained as described above.</p>
  </li>
  <li>
    <p>For the word-label network, since the training data is sparse, one could either train the unlabelled networks first and then the labelled network or they all could be trained jointly.</p>
  </li>
  <li>
    <p>For the case of joint training, the edges are sampled from the 3 networks alternatively.</p>
  </li>
  <li>
    <p>For the fine-tuning case, the edges are first sampled from the unlabelled network and then from the labelled network.</p>
  </li>
  <li>
    <p>Once the word embeddings are obtained, the text embeddings may be obtained by simply averaging the word embeddings.</p>
  </li>
</ul>

<h1 id="evaluation">Evaluation</h1>

<ul>
  <li>
    <p><strong>Baseline Models</strong></p>

    <ul>
      <li>Local word co-occurence based methods - SkipGram, LINE(Gww)</li>
      <li>Document word co-occurence based methods - LINE(Gwd), PV-DBOW</li>
      <li>Combined method - LINE (Gww + Gwd)</li>
      <li>CNN</li>
      <li>PTE</li>
    </ul>
  </li>
  <li>
    <p>For long documents, PTE (joint) outperforms CNN and other PTE variants and is around 10 times faster than CNN model.</p>
  </li>
  <li>
    <p>For short documents, PTE (joint) does not always outperform CNN model probably because the word sense ambiguity is more relevant in the short documents.</p>
  </li>
</ul>
