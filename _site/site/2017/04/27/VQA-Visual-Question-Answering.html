<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>
    <p>Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1505.00468v6">Link to the paper</a></p>
  </li>
</ul>

<h3 id="vqa-challenge-and-workshop"><a href="http://www.visualqa.org/">VQA Challenge and Workshop</a></h3>

<ul>
  <li>The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.</li>
  <li>Interestingly, the second version is starting on 27th April 2017 (today).</li>
</ul>

<h3 id="benefits-over-tasks-like-image-captioning">Benefits over tasks like image captioning:</h3>

<ul>
  <li>Simple, <em>n-gram</em> statistics based methods are not sufficient.</li>
  <li>Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.</li>
  <li>Since only short answers are expected, evaluation is easier.</li>
</ul>

<h3 id="dataset">Dataset</h3>

<ul>
  <li>Created a new dataset of 50000 realistic, abstract images.</li>
  <li>Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (&gt;200K images) and abstract images.</li>
  <li>Three questions per image and ten answers per question (along with their confidence) were collected.</li>
  <li>The entire dataset contains over 760K questions and 10M answers.</li>
  <li>The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.</li>
</ul>

<h3 id="highlights-of-data-collection-methodology">Highlights of data collection methodology</h3>

<ul>
  <li>Emphasis on questions that require an image, and not just common sense, to be answered correctly.</li>
  <li>Workers were shown previous questions when writing new questions to increase diversity.</li>
  <li>Answers collected from multiple users to account for discrepancies in answers by humans.</li>
  <li>Two modalities supported:
    <ul>
      <li><strong>Open-ended</strong> - produce the answer</li>
      <li><strong>multiple-choice</strong> - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)</li>
    </ul>
  </li>
</ul>

<h3 id="highlights-from-data-analysis">Highlights from data analysis</h3>

<ul>
  <li>Most questions range from four to ten words while answers range from one to three words.</li>
  <li>Around 40% questions are “yes/no” questions.</li>
  <li>Significant (&gt;80%) inter-human agreement for answers.</li>
  <li>The authors performed a study where human evaluators were asked to answer the questions without looking at the images.</li>
  <li>Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.</li>
  <li>The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.</li>
</ul>

<h3 id="baseline-models">Baseline Models</h3>

<ul>
  <li><strong>random</strong> selection</li>
  <li><strong>prior (“yes”)</strong> - always answer as yes.</li>
  <li><strong>per Q-type prior</strong> - pick the most popular answer per question type.</li>
  <li><strong>nearest neighbor</strong> - find the k nearest neighbors for the given (image, question) pair.</li>
</ul>

<h3 id="methods">Methods</h3>

<ul>
  <li>
    <p>2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.</p>
  </li>
  <li><strong>Image Channel</strong>
    <ul>
      <li><strong>I</strong> - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.</li>
      <li><strong>norm I</strong> - : l2 normalized version of <strong>I</strong>.</li>
    </ul>
  </li>
  <li><strong>Question Channel</strong>
    <ul>
      <li><strong>BoW Q</strong> - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.</li>
      <li><strong>LSTM Q</strong> - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding.</li>
      <li><strong>Deeper LSTM Q</strong> - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.</li>
    </ul>
  </li>
  <li><strong>Multi-Layer Perceptron (MLP)</strong> - Combine image and question embeddings to obtain a single embedding.
    <ul>
      <li><strong>BoW Q + I</strong> method - concatenate BoW Q and I embeddings.</li>
      <li><strong>LSTM Q + I, deeper LSTM Q + norm I</strong> methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.</li>
    </ul>
  </li>
  <li>Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.</li>
  <li>Cross-entropy loss with VGGNet parameters frozen.</li>
</ul>

<h3 id="results">Results</h3>

<ul>
  <li>Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (&gt;80% and &gt;90% respectively).</li>
  <li>The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.</li>
  <li>Vision only model performs even worse than the model which always produces “yes” as the answer.</li>
</ul>
