<h3 id="problem-statement">Problem Statement</h3>

<ul>
  <li>VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.</li>
  <li>The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.</li>
  <li><a href="http://arxiv.org/pdf/1512.02167.pdf">Link to the paper</a></li>
</ul>

<h3 id="model">Model</h3>

<ul>
  <li>VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.</li>
  <li><strong>Text Features</strong> - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.</li>
  <li><strong>Image Features</strong> - Last layer activations from GoogLeNet.</li>
  <li>Text features are concatenated with image features and fed into a softmax.</li>
  <li>Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.</li>
</ul>

<h3 id="results">Results</h3>

<ul>
  <li>iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.</li>
</ul>

<h3 id="interpretation-of-the-model">Interpretation of the model</h3>

<ul>
  <li>Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive.</li>
  <li>The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.</li>
  <li>Question words generally can influence the answer given the bias in images occurring in COCO dataset.</li>
  <li>Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.</li>
  <li>The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.</li>
  <li>While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.</li>
</ul>
