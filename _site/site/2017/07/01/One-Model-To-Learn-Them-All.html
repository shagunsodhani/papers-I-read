<ul>
  <li>
    <p>The current trend in deep learning is to design, train and fine tune a separate model for each problem.</p>
  </li>
  <li>
    <p>Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.</p>
  </li>
  <li>
    <p>The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.05137">Link to the paper</a></p>
  </li>
</ul>

<h2 id="design-philosophy">Design Philosophy</h2>

<ul>
  <li>
    <p>Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.</p>

    <ul>
      <li>
        <p>The joint representation is to be of variable size.</p>
      </li>
      <li>
        <p>Different tasks from the same domain share the modality net.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.</p>

    <ul>
      <li>Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.</li>
    </ul>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p>MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.</p>
  </li>
  <li>
    <p>Encoder and decoder use the following computational blocks:</p>

    <ul>
      <li>
        <p><strong>Convolutional Block</strong></p>

        <ul>
          <li>ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.</li>
        </ul>
      </li>
      <li>
        <p><strong>Attention Block</strong></p>

        <ul>
          <li>Multihead, dot product based attention mechanism.</li>
        </ul>
      </li>
      <li>
        <p><strong>Mixture-of-Experts (MoE) Block</strong></p>

        <ul>
          <li>Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.</li>
        </ul>
      </li>
      <li>
        <p>For further details, refer the <a href="https://arxiv.org/abs/1706.05137">original paper</a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Encoder</strong> consists of 6 conv blocks with a MoE block in the middle.</p>
  </li>
  <li>
    <p><strong>I/O mixer</strong> consists of an attention block and 2 conv blocks.</p>
  </li>
  <li>
    <p><strong>Decoder</strong> consists of 4 blocks of convolution and attention with a MoE block in the middle.</p>
  </li>
  <li>
    <p><strong>Modality Nets</strong></p>

    <ul>
      <li>
        <p><strong>Language Data</strong></p>

        <ul>
          <li>
            <p>Input is the sequence of tokens ending in a termination token.</p>
          </li>
          <li>
            <p>This sequence is mapped to correct dimensionality using a learned embedding.</p>
          </li>
          <li>
            <p>For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Image</strong> and <strong>Categorical Data</strong></p>

        <ul>
          <li>
            <p>Uses residual convolution blocks.</p>
          </li>
          <li>
            <p>Similar to the exit flow for <a href="https://arxiv.org/abs/1610.02357">Xception Network</a></p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Audio Data</strong></p>

        <ul>
          <li>1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="tasks">Tasks</h2>

<ul>
  <li>
    <p>WSJ speech corpus</p>
  </li>
  <li>
    <p>ImageNet dataset</p>
  </li>
  <li>
    <p>COCO image captioning dataset</p>
  </li>
  <li>
    <p>WSJ parsing dataset</p>
  </li>
  <li>
    <p>WMT English-German translation corpus</p>
  </li>
  <li>
    <p>German-English translation</p>
  </li>
  <li>
    <p>WMT English-French translation corpus</p>
  </li>
  <li>
    <p>German-French translation</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>The experimental section is not very rigorous with many details skipped (would probably be added later).</p>
  </li>
  <li>
    <p>While MultiModel does not beat the state of the art models, it does outperform some recent models.</p>
  </li>
  <li>
    <p>Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).</p>
  </li>
  <li>
    <p>Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.</p>
  </li>
  <li>
    <p>Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.</p>
  </li>
  <li>
    <p>But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.</p>
  </li>
</ul>
