<h2 id="introduction">Introduction</h2>

<ul>
  <li>The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).</li>
  <li>This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.</li>
  <li><a href="https://arxiv.org/abs/1709.03980">Link to the paper</a></li>
</ul>

<h2 id="limitations-of-existing-nmt-models">Limitations of existing NMT models</h2>

<ul>
  <li>The RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information.</li>
  <li>In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.</li>
  <li>While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.</li>
</ul>

<h2 id="contributions-of-the-paper">Contributions of the paper</h2>

<ul>
  <li>Learn the relationship between the source words using the context (neighboring words).</li>
  <li>Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.</li>
</ul>

<h2 id="relation-network">Relation Network</h2>

<ul>
  <li>Neural network which is desgined for relational reasoning.</li>
  <li>Given a set of inputs * O = o<sub>1</sub>, …, o<sub>n</sub> *, RN is formed as a composition of inputs:
   RN(O) = f(sum(g(o<sub>i</sub>, o<sub>j</sub>))), f and g are functions used to learn the relations (feed forward networks)</li>
  <li><em>g</em> learns how the objects are related hence the name “relation”.</li>
  <li><strong>Components</strong>:
    <ul>
      <li>CNN Layer
        <ul>
          <li>Extract information from the words surrounding the given word (context).</li>
          <li>The final output of this layer is the sequence of vectors for different kernel width.</li>
        </ul>
      </li>
      <li>Graph Propagation (GP) Layer
        <ul>
          <li>Connect all the words with each other in the form of a graph.</li>
          <li>Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.</li>
          <li>The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.</li>
        </ul>
      </li>
      <li>Multi-Layer Perceptron (MLP) Layer
        <ul>
          <li>The representation from the GP Layer is fed to the MLP layer.</li>
          <li>The layer uses residual connections from previous layers in form of concatenation.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="datasets">Datasets</h2>

<ul>
  <li>IWSLT Data - 44K sentences from tourism and travel domain.</li>
  <li>NIST Data - 1M Chinese-English parallel sentence pairs.</li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li>MOSES - Open source translation system - http://www.statmt.org/moses/</li>
  <li>NMT - Attention based NMT</li>
  <li>NMT+ - NMT with improved decoder</li>
  <li>TRANSFORMER - Google’s new NMT</li>
  <li>RNMT+ - Relation Network integrated with NMT+</li>
</ul>

<h2 id="evaluation-metric">Evaluation Metric</h2>

<ul>
  <li>case-insensitive 4-gram BLEU score</li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>As sentences become larger (more than 50 words), RNMT clearly outperforms other baselines.</li>
  <li>Qualitative evaluation shows that RNMT+ model captures the word alignment better than the NMT+ models.</li>
  <li>Similarly, NMT+ system tends to miss some information from the source sentence (more so for longer sentences). While both CNNs and RNNs are weak at capturing long-term dependency, using the relation layer mitigates this issue to some extent.</li>
</ul>
