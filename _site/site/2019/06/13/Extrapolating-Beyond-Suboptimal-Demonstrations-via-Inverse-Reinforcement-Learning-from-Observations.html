<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.</p>
  </li>
  <li>
    <p>Standard IRL approaches aim to learn a reward function that “justifies” the demonstration policy and hence those approaches cannot outperform the demonstration policy.</p>
  </li>
  <li>
    <p>In contrast, T-REX aims to learn a reward function that “explains” the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.06387">Link to the paper</a></p>
  </li>
</ul>

<h2 id="approach">Approach</h2>

<ul>
  <li>
    <p>The input is a sequence of trajectories <em>T<sub>1</sub>, … T<sub>m</sub></em> which are ranked in the order of preference. That is, given any pair of trajectories, we know which of the two trajectories is better.</p>
  </li>
  <li>
    <p>The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.</p>
  </li>
  <li>
    <p>Reward Inference</p>

    <ul>
      <li>
        <p>A parameterized reward function <em>r<sub>θ</sub></em> is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.</p>
      </li>
      <li>
        <p>Given a trajectory, the reward function predicts the reward for each state. The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.</p>
      </li>
      <li>
        <p>T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Policy Optimization</p>

    <ul>
      <li>Once a reward function has been learned, standard RL approaches can be used to train a new policy.</li>
    </ul>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari</p>
  </li>
  <li>
    <p>Demonstrations generated using PPO (checkpointed at different stages of training).</p>
  </li>
  <li>
    <p>Ensemble of networks used to learn the reward functions.</p>
  </li>
  <li>
    <p>The proposed approach outperforms the baselines <a href="https://arxiv.org/abs/1805.01954">Behaviour Cloning from Observations</a> and <a href="https://arxiv.org/abs/1606.03476">Generative Adversarial Imitation Learning</a>.</p>
  </li>
  <li>
    <p>In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.</p>
  </li>
  <li>
    <p>Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent.</p>
  </li>
</ul>
