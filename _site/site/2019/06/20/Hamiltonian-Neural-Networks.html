<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes a very cool idea at the intersection of deep learning and physics.</p>
  </li>
  <li>
    <p>The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.01563">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/greydanus/hamiltonian-nn">Link to the code</a></p>
  </li>
  <li>
    <p><a href="https://greydanus.github.io/2019/05/15/hamiltonian-nns/">Link to author’s blog</a></p>
  </li>
</ul>

<h2 id="hamiltonian-mechanics">Hamiltonian Mechanics</h2>

<ul>
  <li>
    <p>It is a branch of physics that can describe systems which follow some conservation laws and invariants.</p>
  </li>
  <li>
    <p>Consider a set of <em>N</em> pair of coordinates [(q<sub>1</sub>, p<sub>1</sub>), …, (q<sub>N</sub>, p<sub>N</sub>)] where <strong>q</strong> = [q<sub>1</sub>, …, q<sub>N</sub>] dnotes the position of the set of objects while <strong>p</strong> = [p<sub>1</sub>, …, p<sub>N</sub>] denotes the momentum of the set of variables.</p>
  </li>
  <li>
    <p>Together these <em>N</em> pairs completely describe the system.</p>
  </li>
  <li>
    <p>A scalar function <em>H(<strong>q</strong>, <strong>p</strong>)</em>, called as the Hamiltonian is defined such that the partial derivative of <em>H</em> with respect to <strong>p</strong> is equal to derivative of <strong>q</strong> with respect to time <em>t</em> and the negative of partial derivative of <em>H</em> with respect to <strong>q</strong> is equal to derivative of <strong>p</strong> with respect to time <em>t</em>.</p>
  </li>
  <li>
    <p>This can be expressed in the form of the equation as follows:</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation1.png" alt="equation1" width="100" height="100" /></p>

<ul>
  <li>The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.</li>
</ul>

<h2 id="hamiltonian-neural-network-hnn">Hamiltonian Neural Network (HNN)</h2>

<ul>
  <li>
    <p>The Hamiltonian <em>H</em> can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.</p>
  </li>
  <li>
    <p>The loss function looks as follows:</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation2.png" alt="equation2" width="400" height="50" /></p>

<ul>
  <li>The partial derivatives can be obtained by computing the <em>in-graph</em> gradient of the output variables with respect to the input variables.</li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>
    <p>For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.</p>
  </li>
  <li>
    <p>For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.</p>
  </li>
  <li>
    <p>In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.</p>
  </li>
  <li>
    <p>In the final experiment, the model is trained on pixel observations and not state observations. In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations. Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons.</p>
  </li>
</ul>
