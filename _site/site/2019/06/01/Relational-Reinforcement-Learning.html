<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.</p>
  </li>
  <li>
    <p>The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1806.01830">Link to the paper</a>.</p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p>The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.</p>
  </li>
  <li>
    <p>Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.</p>
  </li>
  <li>
    <p>Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).</p>
  </li>
  <li>
    <p>Iterative computations can be used to capture higher-order interactions between entities.</p>
  </li>
  <li>
    <p>Entity extraction is based on the assumption that entities are things located at a particular point in space.</p>
  </li>
  <li>
    <p>A CNN is used to parse the pixel space observation into <em>k</em> feature maps of size <em>nxn</em>. The <em>(x, y)</em> coordinates are concatenated to each <em>k-</em>dimensional pixel feature-vector to indicate the pixel’s position in the map.</p>
  </li>
  <li>
    <p>The resulting <em>n<sup>2</sup> x k</em> matrix acts as the entity matrix.</p>
  </li>
  <li>
    <p>Actor-critic architecture (using distributed agent IMPALA) is used.</p>
  </li>
</ul>

<h2 id="environment">Environment</h2>

<h3 id="box-world">Box-World</h3>

<ul>
  <li>
    <p>12 x 12-pixel room with keys and boxes placed randomly.</p>
  </li>
  <li>
    <p>Agent can move in 4 directions.</p>
  </li>
  <li>
    <p>The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).</p>
  </li>
  <li>
    <p>Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.</p>
  </li>
  <li>
    <p>Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal. (ii) The number of distractor branches, (iii)  Length of distractor branches.</p>
  </li>
</ul>

<h3 id="starcraft-ii-minigames">StarCraft II minigames</h3>

<ul>
  <li>9 mini games designed as specific scenarios in the Starcraft game are used.</li>
</ul>

<h2 id="results">Results</h2>

<h3 id="box-world-1">Box-World</h3>

<ul>
  <li>
    <p>RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.</p>
  </li>
  <li>
    <p>Visualising the attention scores indicate that:</p>

    <ul>
      <li>
        <p>keys attend to locks they can unlock.</p>
      </li>
      <li>
        <p>all objects attend to agent’s location.</p>
      </li>
      <li>
        <p>agent and gem attend to each other (and themselves).</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Generalization capacity is tested in two ways:</p>

    <ul>
      <li>
        <p>Performance on levels that require opening a larger sequence of boxes than it is trained on.</p>
      </li>
      <li>
        <p>Performance on levels that require key-lock combinations not seen during training.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In both the scenarios, the RRL agent significantly outperforms the RL agent.</p>
  </li>
</ul>

<h2 id="starcraft">StarCraft</h2>

<ul>
  <li>
    <p>RLL agent achieves better or equal results that the RL agent in all but one game.</p>
  </li>
  <li>
    <p>For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines. These results are not conclusive given the high variability.</p>
  </li>
</ul>
