<ul>
  <li>
    <p>The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.</p>
  </li>
  <li>
    <p>Several RL algorithms are evaluated and the key takeaway is that the “vanilla” RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.12282">Link to the paper</a></p>
  </li>
  <li>
    <p>The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).</p>
  </li>
  <li>
    <p>Two generalization regimes are considered:</p>

    <ul>
      <li>
        <p>Interpolation - parameters of the test environment are similar to the parameters of the training environment.</p>
      </li>
      <li>
        <p>Extrapolation - parameters of the test environment are different from the parameters of the training environment.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Following algorithms are considered as part of the benchmark:</p>

    <ul>
      <li>
        <p>“Vanilla” RL algorithms - A2C, PPO</p>
      </li>
      <li>
        <p>RL algorithms that are designed to generalize:</p>

        <ul>
          <li>
            <p>EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).</p>
          </li>
          <li>
            <p>RL<sup>2</sup> - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.</p>
      </li>
      <li>
        <p>The models are either composed of feedforward networks completely or feedforward + recurrent networks.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Environments</p>

    <ul>
      <li>
        <p>CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.</p>
      </li>
      <li>
        <p>HalfCheetah and Hopper from OpenAI Roboschool.</p>
      </li>
      <li>
        <p>Three versions of each environment are considered:</p>

        <ul>
          <li>
            <p>Deterministic: Environment parameters are fixed. This case corresponds to the standard environment setup in classical RL.</p>
          </li>
          <li>
            <p>Random: Environment parameters are sampled randomly. This case corresponds to sampling from a distribution of environments.</p>
          </li>
          <li>
            <p>Extreme: Environment parameters are sampled from their extreme values. This case corresponds to the edge-case environments which would not be encountered during training generally.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Performance Metrics</p>

    <ul>
      <li>
        <p>Average total reward per episode.</p>
      </li>
      <li>
        <p>Success percentage: Percentage of episodes where a certain goal (or reward) is obtained.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Evaluation Metrics/Setups</p>

    <ul>
      <li>
        <p>Default: success percentage when training and evaluating the deterministic version of the environment.</p>
      </li>
      <li>
        <p>Interpolation: success percentage when training and evaluating on the random version of the environment.</p>
      </li>
      <li>
        <p>Extrapolation: the geometric mean of the success percentage of following three versions:</p>

        <ul>
          <li>
            <p>Train on deterministic and evaluate on the random version.</p>
          </li>
          <li>
            <p>Train on deterministic and evaluate on extreme version.</p>
          </li>
          <li>
            <p>Train on random and evaluate on the extreme version.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Observations</p>

    <ul>
      <li>
        <p>Extrapolation is harder than interpolation.</p>
      </li>
      <li>
        <p>Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.</p>
      </li>
      <li>
        <p>EPOpt improves generalization only for continuous control environments and only with PPO.</p>
      </li>
      <li>
        <p>RL<sup>2</sup> is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.</p>
      </li>
      <li>
        <p>EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not</p>
      </li>
    </ul>
  </li>
</ul>

