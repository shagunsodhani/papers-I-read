<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes the PHYRE (PHYsical REasoning) benchmark - consisting of classic mechanical puzzles in 2D physical environments - as a means to evaluate the physical reasoning ability of machine learning models.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1908.05656">Link to the paper</a></p>
  </li>
</ul>

<h2 id="environment">Environment</h2>

<ul>
  <li>
    <p>2D world that obeys Newtonian mechanics.</p>
  </li>
  <li>
    <p>Gravitational force + Friction.</p>
  </li>
  <li>
    <p>Non-deformable objects that can be static (ie fixed) or dynamic (ie can move and are affected by collisions etc).</p>
  </li>
</ul>

<h2 id="task">Task</h2>

<ul>
  <li>
    <p>The learning agent starts in some initial world state (ie configuration of objects).</p>
  </li>
  <li>
    <p>Goal is described in the form of (<code class="language-plaintext highlighter-rouge">subject</code>, <code class="language-plaintext highlighter-rouge">relation</code>, <code class="language-plaintext highlighter-rouge">object</code>) where the agent’s task is to satisfy the <code class="language-plaintext highlighter-rouge">relation</code> between the <code class="language-plaintext highlighter-rouge">subject</code> and the <code class="language-plaintext highlighter-rouge">object</code>.</p>
  </li>
  <li>
    <p>Currently, only the “touch” <code class="language-plaintext highlighter-rouge">relation</code> is supported.</p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>
    <p>The learning agent has to take a single action - placing one or more new dynamic objects in the world.</p>
  </li>
  <li>
    <p>A simulator is run on the new configuration (for a fixed amount of time) to check if the goal condition is satisfied.</p>
  </li>
  <li>
    <p>At the end of the simulation, a binary reward and intermediate observations (collected as the simulator executes) are provided to the learning agent.</p>
  </li>
  <li>
    <p>These observations are 256*256 grids where each grid cell can take 1 of the 7 values (denoting different types of objects).</p>
  </li>
  <li>
    <p>Since only one relation supported currently, the color is sufficient to encode the goal.</p>
  </li>
</ul>

<h2 id="benchmark-tiers">Benchmark Tiers</h2>

<ul>
  <li>
    <p>Two benchmark tiers are provided where each tier comprises of a combination of:</p>

    <ul>
      <li>
        <p>a predefined set of all the actions that the agent is allowed to perform.</p>
      </li>
      <li>
        <p>set of tasks that can be solved by at least one action from the allowed action set.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>PHYRE-B</strong> - The agent is allowed to place a single (ball of any radii) at any valid location.</p>
  </li>
  <li>
    <p><strong>PHYRE-2B</strong> - The agent is allowed to place 2 balls at any valid pair of locations.</p>
  </li>
  <li>
    <p>Each of the two tiers has 25 task templates where each template comprises of variants of a single task (same goal but different initial conditions).</p>
  </li>
</ul>

<h2 id="evaluation">Evaluation</h2>

<ul>
  <li>
    <p>Two evaluation setups are considered:</p>

    <ul>
      <li>
        <p><strong>within-template</strong> where the agent is trained on some tasks in a template and evaluated on a set of held-out tasks from the same template.</p>
      </li>
      <li>
        <p><strong>cross-template</strong> where the agent is evaluated on tasks from a different template.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In the training phase, the model has access to the simulator (but not to the correct solution). So the model could learn an action-prediction model or forward dynamics model or both.</p>
  </li>
  <li>
    <p>In the testing phase, the model can query the simulator only a few times. Each query provides it with the binary reward and the intermediate observations.</p>
  </li>
</ul>

<h2 id="performance-measure">Performance Measure</h2>

<ul>
  <li>
    <p>The emphasis is on solving more tasks (in few queries) during the test phase.</p>
  </li>
  <li>
    <p>This requirement is captured using a metric called AUCCESS.</p>
  </li>
  <li>
    <p>In general, the tasks in PHYRE-2B are harder than tasks in PHYRE-B.</p>
  </li>
</ul>

<h2 id="baseline-agents">Baseline Agents</h2>

<ul>
  <li>
    <p>Random Agent - Randomly samples actions</p>
  </li>
  <li>
    <p>Non-parametric agent (MEM) - generates R actions at random and uses the simulator to check how many tasks can be solved using these R random actions. During testing, try the R actions in the decreasing order of the number of tasks they solve.</p>
  </li>
  <li>
    <p>Non-parametric agent with online learning (MEM-O) - Variant of MEM where an online adaptation step is performed during test time (to update the rank of the actions).</p>
  </li>
  <li>
    <p>Deep Q Networks with an action encoder, observation encoder and fusion model (combine action and observation representation).</p>
  </li>
  <li>
    <p>DQN with online learning (DQN-0): Variant of DQN with online updates (during the test phase).</p>
  </li>
  <li>
    <p>Contextual bandits.</p>
  </li>
  <li>
    <p>Policy learning approaches like PPO and A2C.</p>
  </li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>
    <p>Both Contextual bandits and policy-based approaches show poor training stability.</p>
  </li>
  <li>
    <p>The best agent, DQN-O, reaches AUCCESS of 56.2\% on PHYRE-B and 39.26\% on PHYRE-2B. In general, agents with online adaptation perform better.</p>
  </li>
  <li>
    <p>The tasks are designed such that 100000 attempts are sufficient to solve 100\% of tasks in PHYRE-B and 95\% of tasks in PHYRE-2B.</p>
  </li>
  <li>
    <p>Even though only two tiers are provided right now, the benchmark is readily extensible and new tasks can be added in the future.</p>
  </li>
</ul>
