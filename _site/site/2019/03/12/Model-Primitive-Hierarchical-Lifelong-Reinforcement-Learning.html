<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.</p>
  </li>
  <li>
    <p>Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.</p>
  </li>
  <li>
    <p>The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1903.01567">Link to the paper</a></p>
  </li>
</ul>

<h2 id="idea">Idea</h2>

<ul>
  <li>
    <p>Instead of learning a single transition model of the environment (aka <em>world model</em>) that can model the transitions very well, it is sufficient to learn several (say <em>k</em>) suboptimal models (aka <em>model primitives</em>).</p>
  </li>
  <li>
    <p>Each <em>model primitive</em> will be good in only a small part of the state space (aka <em>region of specialization</em>).</p>
  </li>
  <li>
    <p>These <em>model primitives</em> can then be used to train a gating mechanism for selecting sub-policies to solve a given task.</p>
  </li>
  <li>
    <p>Since these <em>model primitives</em> are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.</p>
  </li>
</ul>

<h2 id="single-task-learning">Single Task Learning</h2>

<ul>
  <li>
    <p>A gating controller is trained to choose the sub-policy whose <em>model primitive</em> makes the best prediction.</p>
  </li>
  <li>
    <p>This requires modeling <em>p(M<sub>k</sub> | s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>)</em> where <em>p(M<sub>k</sub>)</em> denotes the probability of selecting the <em>k<sup>th</sup> model primitive</em>. This is hard to compute as the system does not have access to <em>s<sub>t+1</sub></em>  and <em>a<sub>t</sub></em> at time <em>t</em> before it has choosen the sub-policy.</p>
  </li>
  <li>
    <p>Properly marginalizing <em>s<sub>t+1</sub></em> and <em>a<sub>t</sub></em> would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce <em>p(M<sub>k</sub> | s<sub>t</sub>)</em>. This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.</p>
  </li>
  <li>
    <p>The paper notes that technique is biased but reports that it still works for the downstream tasks.</p>
  </li>
  <li>
    <p>The gating controller composes the sub-policies as a mixture of Gaussians.</p>
  </li>
  <li>
    <p>For learning, PPO algorithm is used with each <em>model primitives</em> gradient weighted by the probability from the gating controller.</p>
  </li>
</ul>

<h2 id="lifelong-learning">Lifelong Learning</h2>

<ul>
  <li>Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).</li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>Domains:</p>

    <ul>
      <li>
        <p>Mujoco ant navigating different mazes.</p>
      </li>
      <li>
        <p>Stacker arm picking up and placing different boxes.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Implementation Details:</p>

    <ul>
      <li>
        <p>Gaussian subpolicies</p>
      </li>
      <li>
        <p>PPO as the baseline</p>
      </li>
      <li>
        <p>Model primitives are hand-crafted using the true next state provided by the environment simulator.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Single Task</p>

    <ul>
      <li>
        <p>Only maze task is considered with the start position (of the ant) and the goal position is fixed.</p>
      </li>
      <li>
        <p>Observation includes distance from the goal.</p>
      </li>
      <li>
        <p>Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Lifelong Learning</p>

    <ul>
      <li>
        <p>Maze</p>

        <ul>
          <li>
            <p>10 random Mujoco ant mazes used as the task distribution.</p>
          </li>
          <li>
            <p>MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Pick and Place</p>

        <ul>
          <li>
            <p>8 Pick and Place tasks are created with max 3 goal locations.</p>
          </li>
          <li>
            <p>Observation includes the position of the goal.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Ablations</p>

    <ul>
      <li>
        <p>Overlapping <em>model primitives</em> can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.</p>
      </li>
      <li>
        <p>Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.</p>
      </li>
      <li>
        <p>The order of tasks (in the 10-Mazz task) does not degrage the performance.</p>
      </li>
      <li>
        <p>Transfering the gating controller leads to negative transfer.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Notes</p>

    <ul>
      <li>I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.</li>
    </ul>
  </li>
</ul>
