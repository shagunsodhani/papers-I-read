<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.</p>
  </li>
  <li>
    <p>Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1802.06070">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/ben-eysenbach/sac">Link to the code</a></p>
  </li>
</ul>

<h2 id="setup">Setup</h2>

<ul>
  <li>Unsupervised “exploration” stage followed by supervised stage.</li>
</ul>

<h2 id="desirable-qualities-of-skills">Desirable Qualities of Skills</h2>

<ul>
  <li>
    <p>Skills should dictate the states that the agent visits. Different skills should visit different states to be distinguishable.</p>
  </li>
  <li>
    <p>States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).</p>
  </li>
  <li>
    <p>Skills are encouraged to be diverse and “exploratory” by learning skills that act randomly (have high entropy).</p>
  </li>
</ul>

<h2 id="loss-formulation">Loss Formulation</h2>

<ul>
  <li>
    <p>(S, A) - state and action</p>
  </li>
  <li>
    <p>z ~ p(z) - latent variable to condition the policy.</p>
  </li>
  <li>
    <p>Skill - policy conditioned on a fixed z.</p>
  </li>
  <li>
    <p>Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.</p>
  </li>
  <li>
    <p>Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.</p>
  </li>
  <li>
    <p>Maximize the entropy of the mixture of policies (p(z) and all the skills).</p>
  </li>
</ul>

<h2 id="implementation">Implementation</h2>

<ul>
  <li>
    <p>Policy π(a | s, z)</p>
  </li>
  <li>
    <p>Task reward replaced by the pseduoreward logq<sub>φ</sub>(z | s) - log(p(z)).</p>
  </li>
  <li>
    <p>During unsupervised training, z is sampled at the start of the episode and then not changed during the episode.</p>
  </li>
  <li>
    <p>Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.</p>
  </li>
</ul>

<h2 id="observations">Observations</h2>

<h3 id="analysis-of-learned-skills">Analysis of Learned Skills</h3>

<ul>
  <li>
    <p>The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.</p>
  </li>
  <li>
    <p>for inverted pendulum and mountain car, the skills become increasingly diverse throughout training.</p>
  </li>
  <li>
    <p>Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.</p>
  </li>
  <li>
    <p>The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.</p>
  </li>
  <li>
    <p>The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).</p>
  </li>
  <li>
    <p>Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies. This provides a mechanism for incorporating “supervision” in the learning setup.</p>
  </li>
  <li>
    <p>The “discovered” primitives can also be used for imitation learning.</p>
  </li>
</ul>
