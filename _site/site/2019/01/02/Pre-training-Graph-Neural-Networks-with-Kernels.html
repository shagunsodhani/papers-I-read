<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes a pretraining technique that can be used with the <a href="https://shagunsodhani.in/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry">GNN</a> architecture for learning graph representation as induced by powerful graph kernels.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1811.06930">Paper</a></p>
  </li>
</ul>

<h2 id="idea">Idea</h2>

<ul>
  <li>
    <p>Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations.</p>
  </li>
  <li>
    <p>GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs.</p>
  </li>
  <li>
    <p>Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks.</p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p>Given a dataset of graphs <em>g<sub>1</sub>, g<sub>2</sub>, …, g<sub>n</sub></em>, use a relevant kernel function to compute <em>k(g<sub>i</sub>, g<sub>j</sub>)</em> for all pairs of graphs.</p>
  </li>
  <li>
    <p>A siamese network is used to encode the pair of graphs into representations <em>f(g<sub>i</sub>)</em> and <em>f(g<sub>j</sub>)</em> such that <em>dot(f(g<sub>i</sub>), f(g<sub>j</sub>))</em> equals <em>k(g<sub>i</sub>, g<sub>j</sub>)</em>.</p>
  </li>
  <li>
    <p>The function <em>f</em> is trained to learn the compressed representation of kernel’s feature space.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1</li>
</ul>

<h3 id="baselines">Baselines</h3>

<ul>
  <li><a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf">DGCNN</a></li>
  <li>Graphlet Kernel (GK)</li>
  <li>Random Walk Kernel</li>
  <li>Propogation Kernel</li>
  <li>Weisfeiler-Lehman subtree kernel (WL)</li>
</ul>

<h3 id="results">Results</h3>

<ul>
  <li>
    <p>Pretraining uses the WL kernel</p>
  </li>
  <li>
    <p>Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset.</p>
  </li>
</ul>

<h2 id="notes">Notes</h2>

<ul>
  <li>The idea is straightforward and intuitive. In general, this kind of pretraining should help the downstream model. It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained.</li>
</ul>
