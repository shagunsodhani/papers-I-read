<h2 id="contributions">Contributions</h2>

<ul>
  <li>
    <p>A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.</p>
  </li>
  <li>
    <p>A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.</p>
  </li>
  <li>
    <p>Modification of <a href="https://arxiv.org/abs/1706.08840">Gradient Episodic Memory ie GEM</a> which reduces the computational overhead of GEM without compromising on the results.</p>
  </li>
  <li>
    <p>Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1812.00420">Link to the paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/facebookresearch/agem/">Link to the code</a></p>
  </li>
</ul>

<h2 id="learning-protocol">Learning Protocol</h2>

<ul>
  <li>
    <p>Two group of datasets - one for training and evaluation (D<sup>EV</sup>) and other for cross validation (D<sup>CV</sup>).</p>
  </li>
  <li>
    <p>Data can be sampled multiple times for cross-validation dataset but only once from the training dataset.</p>
  </li>
  <li>
    <p>Each group of dataset (say D<sup>EV</sup> or D<sup>CV</sup>) is a list of task-specific datasets D<sub>k</sub> (k is the task index).</p>
  </li>
  <li>
    <p>Each sample in D<sub>k</sub> is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.</p>
  </li>
  <li>
    <p>D<sub>k</sub> contains B<sup>k</sup> minibatches of data.</p>
  </li>
</ul>

<h2 id="metrics">Metrics</h2>

<h3 id="accuracy">Accuracy</h3>

<ul>
  <li>
    <p>a<sub>k,i,j</sub> = accuracy on test task j after training on ith minibatch of training task k.</p>
  </li>
  <li>
    <p>A<sub>k</sub> = mean over all j = 1 to k (a<sub>k, B<sub>k</sub>, j</sub>) ie train the model on data for task k and then test it on all the tasks.</p>
  </li>
</ul>

<h3 id="forgetting-measure">Forgetting Measure</h3>

<ul>
  <li>
    <p>f<sub>j</sub><sup>k</sup> = forgetting on task j after training on all minibatches upto task k.</p>
  </li>
  <li>
    <p>f<sub>j</sub><sup>k</sup> = max over all l = 1 to k-1 (a<sub>l, B<sub>l</sub>j</sub> - a<sub>k, B<sub>k</sub>j</sub>)</p>
  </li>
  <li>
    <p>Forgetting = F<sub>k</sub> = mean over all j = 1 to k-1 (f<sub>j</sub><sup>k</sup>)</p>
  </li>
</ul>

<h3 id="lca---learning-curve-area">LCA - Learning Curve Area</h3>

<ul>
  <li>
    <p>Z<sub>b</sub> = average b shot performance where b is the minibatch number.</p>
  </li>
  <li>
    <p>Z<sub>b</sub> = mean over all k = 0 to T (a<sub>k, b, k</sub>)</p>
  </li>
  <li>
    <p>LCA<sub>β</sub> = mean over all b = 0 to β (Z<sub>b</sub>)</p>
  </li>
  <li>
    <p>One special case is LCA<sub>0</sub> which is the forward transfer performance or performance on the unseen task.</p>
  </li>
  <li>
    <p>In experiments, β is kept small as we want the model to learn from few examples.</p>
  </li>
</ul>

<h2 id="model">Model</h2>

<ul>
  <li>
    <p>GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.</p>
  </li>
  <li>
    <p>Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.</p>
  </li>
  <li>
    <p>While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.</p>
  </li>
</ul>

<h2 id="joint-embedding-model-using-compositional-task-descriptors">Joint Embedding Model Using Compositional Task Descriptors</h2>

<ul>
  <li>
    <p>Compositional Task Descriptors are used to speed training on the subsequent tasks.</p>
  </li>
  <li>
    <p>A matrix specifying the attribute value of objects (to be recognized in the task) are used.</p>
  </li>
  <li>
    <p>A joint-embedding space between image features and attribute embeddings is learned.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1612.00796">Permuted MNIST</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1703.04200">Split CIFAR</a></p>
  </li>
  <li>
    <p><a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">Split CUB</a></p>
  </li>
  <li>
    <p><a href="http://cvml.ist.ac.at/papers/lampert-cvpr2009.pdf">Split AWA</a></p>
  </li>
</ul>

<h3 id="setup">Setup</h3>

<ul>
  <li>
    <p>Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA</p>
  </li>
  <li>
    <p>Baselines include <a href="https://arxiv.org/abs/1706.08840">GEM</a>, <a href="https://arxiv.org/abs/1611.07725">iCaRL</a>, <a href="https://arxiv.org/pdf/1612.00796.pdf">Elastic Weight Consolidation</a>, <a href="https://arxiv.org/abs/1606.04671">Progressive Neural Networks</a> etc.</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead. One reason could be that MNIST has a large number of training examples per task. But Progressive Neural Networks lead to bad utilization of capacity.</p>
  </li>
  <li>
    <p>While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.</p>
  </li>
  <li>
    <p>Use of task descriptors improves the accuracy for all the models.</p>
  </li>
  <li>
    <p>It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs.</p>
  </li>
</ul>
