<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper proposes MAML++ - a modification of MAML algorithm that stabilizes its training improves generalization performance and reduces the computational overhead.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.09502">Link to the paper</a></p>
  </li>
</ul>

<h2 id="notes">Notes</h2>

<h3 id="unstable-training">Unstable Training</h3>

<ul>
  <li>
    <p>Training the outer loop requires unfolding the inner loop multiple times.</p>
  </li>
  <li>
    <p>In absence of skip connections, the gradient is multiplied by the same parameter multiple times.</p>
  </li>
  <li>
    <p>Large depth and absent skip connections could lead to exploding and vanishing gradients respectively.</p>
  </li>
  <li>
    <p>The paper proposes to stabilize the gradient propagation by minimizing the target set loss computed by the base-network after every step towards a support set task.</p>
  </li>
  <li>
    <p>It is important to anneal the contribution of earlier steps and increase the contribution of later steps over time.</p>
  </li>
</ul>

<h3 id="second-order-derivatives-are-expensive-to-compute">Second Order derivatives are expensive to compute</h3>

<ul>
  <li>
    <p>While the first-order MAML is faster, the resulting model may not have as good a generalization error as the second-order MAML.</p>
  </li>
  <li>
    <p>The paper proposes to use derivative order annealing where the first order gradients are used for the first 50 epochs and the network uses second-order gradients from thereon.</p>
  </li>
  <li>
    <p>This derivative order annealing appears to be more stable than models that use second-order derivatives only.</p>
  </li>
</ul>

<h3 id="batch-normalization">Batch Normalization</h3>

<ul>
  <li>
    <p>In MAML, the statistics of the current batch are used for normalization instead of accumulating the running statistics.</p>
  </li>
  <li>
    <p>The paper proposes to collect the statistics per step which can increase the convergence speed, stability, and generalization performance.</p>
  </li>
  <li>
    <p>In MAML, the batch normalization biases are not updated in the inner-loop which can adversely impact the performance.</p>
  </li>
  <li>
    <p>The paper proposes to learn a set of biases (per step) within the inner loop update.</p>
  </li>
</ul>

<h3 id="fixed-learning-rate">Fixed Learning Rate</h3>

<ul>
  <li>
    <p>MAML uses a single learning rate across all the steps and all the parameters. This means there is one single learning rate that needs to be hyperparameter to work well for all the layers and steps.</p>
  </li>
  <li>
    <p>An alternate solution would be to learn a separate learning rate per parameter but this can be impractical as it doubles the number of parameters to be learned.</p>
  </li>
  <li>
    <p>The paper proposes to learn a learning rate and direction for each layer in the network, for each step it takes in the inner loop.</p>
  </li>
  <li>
    <p>The paper also proposed to anneal the learning rate of the outer loop (using cosine annealing) as it helps to achieve better generalization.</p>
  </li>
</ul>

<h2 id="results">Results</h2>

<ul>
  <li>
    <p>Using these modifications helps to outperform the MAML model on both Omniglot and MiniImagenet datasets.</p>
  </li>
  <li>
    <p>The biggest benefit comes by learning the per-layer, per-step learning rates and by using the per-step batch normalization.</p>
  </li>
</ul>
