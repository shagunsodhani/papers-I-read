<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      One Model To Learn Them All &middot; Papers I Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/poole.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/syntax.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/lanyon.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/style.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/font-awesome-4.7.0/css/font-awesome.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://shagunsodhani.in/papers-I-read/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://shagunsodhani.in/papers-I-read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>I am trying a new initiative - <i>A Paper A Week</i>. This blog will hold all the notes and summaries.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/archieve">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/tags">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read/archive/v1.0.0.zip">Download</a> -->
    <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read">GitHub project</a>
    <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/atom.xml">Feed</a>
    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2017. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://shagunsodhani.in/papers-I-read/" title="Home">Papers I Read</a>
            <small>Notes and Summaries</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">One Model To Learn Them All</h1>
  <p class="entry-tags"><a href="https://shagunsodhani.in/papers-I-read/tags.html#2017" title="Pages tagged 2017" rel="tag">2017</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#AI" title="Pages tagged AI" rel="tag">AI</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Attention" title="Pages tagged Attention" rel="tag">Attention</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#CV" title="Pages tagged CV" rel="tag">CV</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Multi+Modal" title="Pages tagged Multi Modal" rel="tag">Multi Modal</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Multi+Model" title="Pages tagged Multi Model" rel="tag">Multi Model</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#NLP" title="Pages tagged NLP" rel="tag">NLP</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Speech" title="Pages tagged Speech" rel="tag">Speech</a></p>
  <span class="post-date">01 Jul 2017</span>
  <ul>
  <li>
    <p>The current trend in deep learning is to design, train and fine tune a separate model for each problem.</p>
  </li>
  <li>
    <p>Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.</p>
  </li>
  <li>
    <p>The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.05137">Link to the paper</a></p>
  </li>
</ul>

<h2 id="design-philosophy">Design Philosophy</h2>

<ul>
  <li>
    <p>Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.</p>

    <ul>
      <li>
        <p>The joint representation is to be of variable size.</p>
      </li>
      <li>
        <p>Different tasks from the same domain share the modality net.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.</p>

    <ul>
      <li>Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.</li>
    </ul>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<ul>
  <li>
    <p>MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.</p>
  </li>
  <li>
    <p>Encoder and decoder use the following computational blocks:</p>

    <ul>
      <li>
        <p><strong>Convolutional Block</strong></p>

        <ul>
          <li>ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.</li>
        </ul>
      </li>
      <li>
        <p><strong>Attention Block</strong></p>

        <ul>
          <li>Multihead, dot product based attention mechanism.</li>
        </ul>
      </li>
      <li>
        <p><strong>Mixture-of-Experts (MoE) Block</strong></p>

        <ul>
          <li>Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.</li>
        </ul>
      </li>
      <li>
        <p>For further details, refer the <a href="https://arxiv.org/abs/1706.05137">original paper</a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Encoder</strong> consists of 6 conv blocks with a MoE block in the middle.</p>
  </li>
  <li>
    <p><strong>I/O mixer</strong> consists of an attention block and 2 conv blocks.</p>
  </li>
  <li>
    <p><strong>Decoder</strong> consists of 4 blocks of convolution and attention with a MoE block in the middle.</p>
  </li>
  <li>
    <p><strong>Modality Nets</strong></p>

    <ul>
      <li>
        <p><strong>Language Data</strong></p>

        <ul>
          <li>
            <p>Input is the sequence of tokens ending in a termination token.</p>
          </li>
          <li>
            <p>This sequence is mapped to correct dimensionality using a learned embedding.</p>
          </li>
          <li>
            <p>For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Image</strong> and <strong>Categorical Data</strong></p>

        <ul>
          <li>
            <p>Uses residual convolution blocks.</p>
          </li>
          <li>
            <p>Similar to the exit flow for <a href="https://arxiv.org/abs/1610.02357">Xception Network</a></p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Audio Data</strong></p>

        <ul>
          <li>1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="tasks">Tasks</h2>

<ul>
  <li>
    <p>WSJ speech corpus</p>
  </li>
  <li>
    <p>ImageNet dataset</p>
  </li>
  <li>
    <p>COCO image captioning dataset</p>
  </li>
  <li>
    <p>WSJ parsing dataset</p>
  </li>
  <li>
    <p>WMT English-German translation corpus</p>
  </li>
  <li>
    <p>German-English translation</p>
  </li>
  <li>
    <p>WMT English-French translation corpus</p>
  </li>
  <li>
    <p>German-French translation</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>
    <p>The experimental section is not very rigorous with many details skipped (would probably be added later).</p>
  </li>
  <li>
    <p>While MultiModel does not beat the state of the art models, it does outperform some recent models.</p>
  </li>
  <li>
    <p>Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).</p>
  </li>
  <li>
    <p>Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.</p>
  </li>
  <li>
    <p>Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.</p>
  </li>
  <li>
    <p>But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.</p>
  </li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/Revisiting-Semi-Supervised-Learning-with-Graph-Embeddings">
            Revisiting Semi-Supervised Learning with Graph Embeddings
            <small>11 Dec 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension">
            Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension
            <small>28 Nov 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/Higher-order-organization-of-complex-networks">
            Higher-order organization of complex networks
            <small>19 Nov 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://shagunsodhani.in/papers-I-read/One-Model-To-Learn-Them-All"  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/One-Model-To-Learn-Them-All"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://papers-i-read.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68140113-4', 'auto');
  ga('send', 'pageview');

</script>
</html>
