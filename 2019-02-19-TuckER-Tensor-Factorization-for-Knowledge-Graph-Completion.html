<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1901.09590">Paper</a></p>
  </li>
  <li>
    <p><a href="https://github.com/ibalazevic/TuckER">Implementation</a></p>
  </li>
</ul>

<h2 id="knowledge-graph-as-a-tensor">Knowledge Graph as a Tensor</h2>

<ul>
  <li>
    <p>Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG).</p>
  </li>
  <li>
    <p>The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (e<sub>s</sub>, r, e<sub>o</sub>).</p>
  </li>
  <li>
    <p>The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element’s value corresponds to ether that element is present in the KG or not.</p>
  </li>
  <li>
    <p>The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple. The score indicates whether the triple is actually present in the KG or not.</p>
  </li>
</ul>

<h2 id="tucker-decomposition">TuckER Decomposition</h2>

<ul>
  <li>
    <p>Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor.</p>
  </li>
  <li>
    <p>In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor <strong>X</strong> (of shape <em>IxJxK</em>) can be factorized into a core tensor <strong>W</strong> (of shape <em>PxQxR</em>) and 3 factor matrics - <strong>A</strong> (of shape <em>IxP</em>), <strong>B</strong> (of shape <em>JxQ</em>) and <strong>C</strong> (of shape <em>KxR</em>) such that <strong>X</strong> is approximately <strong>W</strong> x<sub>1</sub> <strong>A</strong> x<sub>2</sub> <strong>B</strong> x<sub>3</sub> <strong>C</strong>, where X<sub>n</sub> denotes the tensor product along the nth mode.</p>
  </li>
  <li>
    <p>Generally, <em>P, Q, R</em> are smaller than <em>I, J K</em> (respectively) and <strong>W</strong> can be seen as a compressed version of <strong>X</strong>.</p>
  </li>
</ul>

<h2 id="tucker-decomposition-for-link-prediction">TuckER Decomposition for Link Prediction</h2>

<ul>
  <li>
    <p>Two embedding matrics are used for embedding the entities and the relations respectively.</p>
  </li>
  <li>
    <p>Entity embedding matrix <strong>E</strong> is shared for both subject and the object ie <strong>E</strong> = <strong>A</strong> = <strong>B</strong>.</p>
  </li>
  <li>
    <p>The scoring function is gives as <strong>W</strong> x<sub>1</sub> <strong>e<sub>s</sub></strong> x<sub>2</sub> <strong>w<sub>r</sub></strong> x<sub>3</sub> <strong>e<sub>0</sub></strong> where <strong>e<sub>s</sub></strong>, <strong>w<sub>r</sub></strong> and <strong>e<sub>o</sub></strong> are the embedding vectors corresonding to e<sub>s</sub>, e<sub>r</sub> and e<sub>o</sub> respectively. Note that both the core tensor and the factor matrices are to be learnt.</p>
  </li>
  <li>
    <p>Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)</p>
  </li>
  <li>
    <p>To speed up training and increase accuracy, 1-N scoring is used. A given (e<sub>s</sub>, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete).</p>
  </li>
  <li>
    <p>Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations.</p>
  </li>
</ul>

<h2 id="theoretical-analysis">Theoretical Analysis</h2>

<ul>
  <li>
    <p>One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE.</p>
  </li>
  <li>
    <p>It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding.</p>
  </li>
  <li>
    <p>For full expressiveness, dimensionality of entity (relation) is n<sub>E</sub> (n<sub>R</sub>) where n<sub>E</sub> (n<sub>R</sub>) are the number of entities (relations). In comparsion, the required dimensionality for ComplEx is n<sub>E</sub> * n<sub>R</sub> (for both entity and relations) and for SimplE, it is min(<sub>E</sub> * n<sub>R</sub>, number of facts + 1) (for both entity and relations).</p>
  </li>
  <li>
    <p>Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER.</p>
  </li>
</ul>

<h2 id="experiments">Experiments</h2>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>
    <p>FB15k, FB!5k-237, WN18, WN18RR</p>
  </li>
  <li>
    <p>The max number of entities is around 41K and max number of relations is around 1.3K</p>
  </li>
</ul>

<h3 id="implementation">Implementation</h3>

<ul>
  <li>BatchNorm, Dropout and Learning rate decay are used.</li>
</ul>

<h3 id="metrics">Metrics</h3>

<ul>
  <li>
    <p>Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall n<sub>e</sub> generated triples.</p>
  </li>
  <li>
    <p>hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the n<sub>e</sub> generated triples.</p>
  </li>
  <li>
    <p>Higher is better for both the metrics.</p>
  </li>
</ul>

<h3 id="results">Results</h3>

<ul>
  <li>
    <p>TuckER outperforms all the baseline models on all but one task.</p>
  </li>
  <li>
    <p>Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting).</p>
  </li>
  <li>
    <p>TuckER improves performance more significantly when the number of relations is large.</p>
  </li>
  <li>
    <p>Even with lower embedding dimensions, TuckER’s performance does not deteriorate as much as other models.</p>
  </li>
</ul>
