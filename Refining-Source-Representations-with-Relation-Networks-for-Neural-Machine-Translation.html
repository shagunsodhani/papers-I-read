<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Refining Source Representations with Relation Networks for Neural Machine Translation &middot; Papers I Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/poole.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/syntax.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/lanyon.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/css/style.css">
  <link rel="stylesheet" href="https://shagunsodhani.in/papers-I-read/public/font-awesome-4.7.0/css/font-awesome.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://shagunsodhani.in/papers-I-read/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://shagunsodhani.in/papers-I-read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>I am trying a new initiative - <i>A Paper A Week</i>. This blog will hold all the notes and summaries.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/archieve">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/tags">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read/archive/v1.0.0.zip">Download</a> -->
    <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read">GitHub project</a>
    <a class="sidebar-nav-item" href="https://shagunsodhani.in/papers-I-read/atom.xml">Feed</a>
    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://shagunsodhani.in/papers-I-read/" title="Home">Papers I Read</a>
            <small>Notes and Summaries</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Refining Source Representations with Relation Networks for Neural Machine Translation</h1>
  <p class="entry-tags"><a href="https://shagunsodhani.in/papers-I-read/tags.html#2017" title="Pages tagged 2017" rel="tag">2017</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Relational+Network" title="Pages tagged Relational Network" rel="tag">Relational Network</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#Representation+Learning" title="Pages tagged Representation Learning" rel="tag">Representation Learning</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#AI" title="Pages tagged AI" rel="tag">AI</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#NLP" title="Pages tagged NLP" rel="tag">NLP</a> &bull; <a href="https://shagunsodhani.in/papers-I-read/tags.html#NMT" title="Pages tagged NMT" rel="tag">NMT</a></p>
  <span class="post-date">22 Sep 2017</span>
  <h2 id="introduction">Introduction</h2>

<ul>
  <li>The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).</li>
  <li>This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.</li>
  <li><a href="https://arxiv.org/abs/1709.03980">Link to the paper</a></li>
</ul>

<h2 id="limitations-of-existing-nmt-models">Limitations of existing NMT models</h2>

<ul>
  <li>The RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information.</li>
  <li>In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.</li>
  <li>While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.</li>
</ul>

<h2 id="contributions-of-the-paper">Contributions of the paper</h2>

<ul>
  <li>Learn the relationship between the source words using the context (neighboring words).</li>
  <li>Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.</li>
</ul>

<h2 id="relation-network">Relation Network</h2>

<ul>
  <li>Neural network which is desgined for relational reasoning.</li>
  <li>Given a set of inputs * O = o<sub>1</sub>, …, o<sub>n</sub> *, RN is formed as a composition of inputs:
   RN(O) = f(sum(g(o<sub>i</sub>, o<sub>j</sub>))), f and g are functions used to learn the relations (feed forward networks)</li>
  <li><em>g</em> learns how the objects are related hence the name “relation”.</li>
  <li><strong>Components</strong>:
    <ul>
      <li>CNN Layer
        <ul>
          <li>Extract information from the words surrounding the given word (context).</li>
          <li>The final output of this layer is the sequence of vectors for different kernel width.</li>
        </ul>
      </li>
      <li>Graph Propagation (GP) Layer
        <ul>
          <li>Connect all the words with each other in the form of a graph.</li>
          <li>Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.</li>
          <li>The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.</li>
        </ul>
      </li>
      <li>Multi-Layer Perceptron (MLP) Layer
        <ul>
          <li>The representation from the GP Layer is fed to the MLP layer.</li>
          <li>The layer uses residual connections from previous layers in form of concatenation.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="datasets">Datasets</h2>

<ul>
  <li>IWSLT Data - 44K sentences from tourism and travel domain.</li>
  <li>NIST Data - 1M Chinese-English parallel sentence pairs.</li>
</ul>

<h2 id="models">Models</h2>

<ul>
  <li>MOSES - Open source translation system - http://www.statmt.org/moses/</li>
  <li>NMT - Attention based NMT</li>
  <li>NMT+ - NMT with improved decoder</li>
  <li>TRANSFORMER - Google’s new NMT</li>
  <li>RNMT+ - Relation Network integrated with NMT+</li>
</ul>

<h2 id="evaluation-metric">Evaluation Metric</h2>

<ul>
  <li>case-insensitive 4-gram BLEU score</li>
</ul>

<h2 id="observations">Observations</h2>

<ul>
  <li>As sentences become larger (more than 50 words), RNMT clearly outperforms other baselines.</li>
  <li>Qualitative evaluation shows that RNMT+ model captures the word alignment better than the NMT+ models.</li>
  <li>Similarly, NMT+ system tends to miss some information from the source sentence (more so for longer sentences). While both CNNs and RNNs are weak at capturing long-term dependency, using the relation layer mitigates this issue to some extent.</li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/Emotional-Chatting-Machine-Emotional-Conversation-Generation-with-Internal-and-External-Memory">
            Emotional Chatting Machine - Emotional Conversation Generation with Internal and External Memory
            <small>22 Jan 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/How-transferable-are-features-in-deep-neural-networks">
            How transferable are features in deep neural networks
            <small>06 Jan 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.in/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network">
            Distilling the Knowledge in a Neural Network
            <small>31 Dec 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://shagunsodhani.in/papers-I-read/Refining-Source-Representations-with-Relation-Networks-for-Neural-Machine-Translation"  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/Refining-Source-Representations-with-Relation-Networks-for-Neural-Machine-Translation"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://papers-i-read.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68140113-4', 'auto');
  ga('send', 'pageview');

</script>
</html>
