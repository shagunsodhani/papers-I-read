<!DOCTYPE html>
<html lang="en-us">
  
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Linguistic Knowledge as Memory for Recurrent Neural Networks &middot; Papers I Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/poole.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/syntax.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/lanyon.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/style.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/font-awesome-4.7.0/css/font-awesome.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://shagunsodhani.github.io/papers-I-read/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://shagunsodhani.github.io/papers-I-read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68140113-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68140113-4');
</script>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>I am trying a new initiative - <i>A Paper A Week</i>. This blog will hold all the notes and summaries.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/archieve">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/tags">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read/archive/v1.0.0.zip">Download</a> -->
    <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read">GitHub project</a>
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/atom.xml">Feed</a>
    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://shagunsodhani.github.io/papers-I-read/" title="Home">Papers I Read</a>
            <small>Notes and Summaries</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Linguistic Knowledge as Memory for Recurrent Neural Networks</h1>
  <p class="entry-tags"><a href="https://shagunsodhani.github.io/papers-I-read/tags.html#2017" title="Pages tagged 2017" rel="tag">2017</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Natural+Language+Processing" title="Pages tagged Natural Language Processing" rel="tag">Natural Language Processing</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#AI" title="Pages tagged AI" rel="tag">AI</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Memory" title="Pages tagged Memory" rel="tag">Memory</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#NLP" title="Pages tagged NLP" rel="tag">NLP</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#QA" title="Pages tagged QA" rel="tag">QA</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#RNN" title="Pages tagged RNN" rel="tag">RNN</a></p>
  <span class="post-date">05 Feb 2019</span>
  <ul>
  <li>
    <p><a href="https://arxiv.org/abs/1703.02620">Link to the paper</a></p>
  </li>
  <li>
    <p>Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.</p>
  </li>
  <li>
    <p>For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.</p>
  </li>
  <li>
    <p>These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.</p>
  </li>
  <li>
    <p>One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).</p>
  </li>
  <li>
    <p>The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.</p>
  </li>
  <li>
    <p>This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture. Its GRU version is referred to as MAGE-GRU.</p>
  </li>
  <li>
    <p>Given an input sequence of tokens [x<sub>1</sub>, x<sub>2</sub>, â€¦, x<sub>T</sub>] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.</p>
  </li>
  <li>
    <p>Given the graph <em>G</em>, two DFS orderings are computed - forward DFS and backward DFS.</p>
  </li>
  <li>
    <p>MAGE-RNN uses separate networks for accessing the forward and backward DFS orders.</p>
  </li>
  <li>
    <p>A separate hidden state is maintained for each entity type to separate memory content from addressing.</p>
  </li>
  <li>
    <p>For any DFS order (forward or backward), the representation at time <em>t</em> is given as the concatenation of representation of different edge types at that time.</p>
  </li>
  <li>
    <p>The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at x<sub>t</sub>.</p>
  </li>
  <li>
    <p>The representation of the DFS order is given as the sequence of all the previous representations.</p>
  </li>
  <li>
    <p>In some cases, elements across multiple sequences could be related to each other. In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.</p>
  </li>
  <li>
    <p>The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).</p>
  </li>
  <li>
    <p>MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.</p>
  </li>
  <li>
    <p>DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.</p>
  </li>
  <li>
    <p>For all the cases, the model with MAGE-GRU works the best.</p>
  </li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools">
            Toolformer - Language Models Can Teach Themselves to Use Tools
            <small>10 Feb 2023</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Synthesized-Policies-for-Transfer-and-Adaptation-across-Tasks-and-Environments">
            Synthesized Policies for Transfer and Adaptation across Tasks and Environments
            <small>29 Mar 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Deep-Neural-Networks-for-YouTube-Recommendations">
            Deep Neural Networks for YouTube Recommendations
            <small>22 Mar 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://shagunsodhani.github.io/papers-I-read/Linguistic-Knowledge-as-Memory-for-Recurrent-Neural-Networks"  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/Linguistic-Knowledge-as-Memory-for-Recurrent-Neural-Networks"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://papers-i-read.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

  </body>
</html>
