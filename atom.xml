<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Papers I Read</title>
 <link href="https://shagunsodhani.github.io/papers-I-read/atom.xml" rel="self"/>
 <link href="https://shagunsodhani.github.io/papers-I-read/"/>
 <updated>2020-02-13T22:59:45-05:00</updated>
 <id>https://shagunsodhani.github.io/papers-I-read</id>
 <author>
   <name>Shagun Sodhani</name>
   <email>sshagunsodhani@gmail.com</email>
 </author>

 
 <entry>
   <title>Observational Overfitting in Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Observational-Overfitting-in-Reinforcement-Learning"/>
   <updated>2020-01-23T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Observational Overfitting in Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies &lt;em&gt;observational overfitting&lt;/em&gt;: The phenomenon where an agent overfits to different observation spaces even though the underlying MDP remains fixed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike other works, the “background information” (in the pixel space) is correlated with the progress of the agent (and is not just noise).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02975&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Base MDP $M = (S, A, R, T)$ where $S$ is the state space, $A$ is the action space, $R$ is the reward function, and $T$ is the transition dynamics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$M$ is parameterized using $\theta$. In practice, it means introducing an observation function $\phi_{\theta}$ ie $M_{\theta} = (M, \phi_{\theta})$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A distribution over $\theta$ defines a distribution over the MDPs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has access to the pixel space observations and not the state space observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization gap is defined as $J_{\theta}(\pi) - J_{\theta^{train}}(\pi)$ where $\pi$ is the learning agent, $\theta$ is the distribution over all the observation functions, $\theta^{train}$ is the distribution over the observation functions corresponding to the training environments. $J_{\theta}(\pi)$ is the average reward that the agent obtains over environments sampled from $M_{\theta}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\phi_{\theta}$ considers two featurs - generalizable (invariant across $\theta$) and non-generalizable (depends on $\theta$) ie $\phi_{\theta}(s) = concat(f(s), g_{\theta}(s))$ where $f$ is the invariant function and $g$ is the non-generalizable function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is set up such that “explicit regularization” can easily solve it. The focus is on understanding the effect of “implicit regularization”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;overparameterized-lqr&quot;&gt;Overparameterized LQR&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LQR is used as a proxy for deep RL architectures given its advantages like enabling exact gradient descent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The functions are parameterized as follows:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$f(s) = W_c(s)$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$g_{\theta}(s) = W_{\theta}(s)$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observation at time $t$ , $o_t$, is given as $[W_c W_{\theta}]^{-1} s_t$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Action at time $t$ is given as $a_t = K o_{t}$ where $K$ is the policy matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dimensionality:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;state $s$: $d_{state}$ 100&lt;/li&gt;
      &lt;li&gt;$f(s)$: $d_{state}$ 100&lt;/li&gt;
      &lt;li&gt;$g_{\theta}(s)$: $d_{noise}$ 100&lt;/li&gt;
      &lt;li&gt;observation $o$: $d_{state}$ + $d_{noise}$ 1100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of training on just one environment, multiple solutions exist, and overfitting happens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing $d_{noise}$ increases the generalization gap.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overparameterizing the network decreases the generalization gap and also reduces the norm of the policy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projected-gym-environments&quot;&gt;Projected Gym Environments&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The base MDP is the Gym Environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$M_{\theta}$ is generated as before.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing both width and depth for basic MLPs improves generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization also depends on the choice of activation function, residual layers, etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deconvolutional-projections&quot;&gt;Deconvolutional Projections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the Gym environment, the actual state is projected to a larger vector and reshaped into an 84x84 tensor (image).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The image from $f$ is concatenated with the image from $g$. This setup is referred to as the Gym-Deconv.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The relative order of performance between NatureCNN, IMPALA, and IMPALA-Large (on both CoinRun and Gym-Deconv) is the same as the order of the number of parameters they contain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In an ablation, the policy is given access to only $g_{\theta}(s)$, which makes it impossible for the model to generalize. In this test of memorization capacity, implicit regularization seems to reduce the memorization effect.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overparameterization-in-coinrun&quot;&gt;Overparameterization in CoinRun&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The pixel space observation in CoinRun is downsized from 64x64 to 32x32 and flattened into a vector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In CoinRun, the dynamics change per level, and the noisy “irrelevant” features change location across the 1D input, making this setup more challenging than the previous ones.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overparameterization improves generalization in this scenario as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Rapid-Learning-or-Feature-Reuse-Towards-Understanding-the-Effectiveness-of-MAML"/>
   <updated>2020-01-16T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper investigated two possible reaosns behind the usefulness of MAML algorithm:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Rapid Learning&lt;/strong&gt; - Does MAML learn features that are amenable for rapid learning?&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Feature Reuse&lt;/strong&gt; - Does the MAML initialization provide high quality featues that are useful for the unseen tasks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This leads to a followup questions: how much task-specific inner loopadaptation is needed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.09157&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a standard few shot learning setup, the different datatsets have different classes. Hence, the top-most layer (or the head) of the learning model should be different for dofferent tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The subsequent discussion only applies to the body of the network (ie network minus the head).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Freezing Layer Representations&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setup, a subset (or all) of parameters are forzen after MAML training and are not adapted during representation.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even when the entire network is frozen, the performance drops only marginally.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This indicates that the representation learnt by the meta-initialization is good enough to be useful on the test tasks (without requiring any adapation setep).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Note that the head of the network is still adapated during testing.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Representational Similarity&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setup, the paper reports the change in the latent representation (learned by the network) during the inner loop update with a fully trained model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Canonical Correlation Analysis (CCA) and Central Kernel Alignment (CKA) metrics are used to measure the similarity between the representations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The main finding is that the representations in the body of the network are very sunular before and after the inner loop updates while the representaions in the head of the network are very different.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above two observations indicate that feature reuse is the primary diriving factir for the success of MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;When does feature reuse happen&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper considers the model at different stages of training and compares the similariyt in the representation (beofre and after the iunner loop update).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even early in training, the CCA similairyt between the representations (before and after the inner loop update) is quote high. This suggests that feature reuse happens early in the training.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Accurate-Large-Minibatch-SGD-Training-ImageNet-in-1-Hour"/>
   <updated>2020-01-09T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Accurate Large Minibatch SGD - Training ImageNet in 1 Hour</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training models with large minibatches (using distributed synchronous SGD) can lead to optimization issues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents techniques for training models with large batch size while matching the accuracy of small minibatch setups.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on the ImageNet dataset, but many of the proposed ideas are applicable broadly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.02677&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-scaling-rule&quot;&gt;Linear Scaling Rule&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When the minibatch size increases by a factor of &lt;em&gt;k&lt;/em&gt;, the learning rate should also be increased by a factor of &lt;em&gt;k&lt;/em&gt; (while keeping all other hyperparameters like weight decay fixed).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that this is an empirical rule and is not expected to hold under all conditions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One such condition is when the model is changing rapidly during the first few epochs. In this case, a warmup phase is introduced to stabilize the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper verifies that the scaling rule is applicable to batch sizes as large as 8K.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;warmup&quot;&gt;Warmup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The learning rate should be gradually ramped up from a small value to a large value to allow convergence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization uses batch statistics to normalize the data. Hence, the loss corresponding to each data point (in the batch) is not independent. Thus, changing the batch size could change the underlying function being optimized.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the distributed SGD setup, the per-GPU (or per-worker) batch size should be kept constant, and only one worker should compute the batch norm statistics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pitfalls-when-using-distributed-sgd&quot;&gt;Pitfalls when using distributed SGD&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When using weight decay, scaling the cross-entropy loss is not the same as scaling the learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When using momentum, changing the learning rate could require “momentum correction.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that the per-worker loss is normalized by the size of the total minibatch and not just by the size of minibatch that each worker sees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each epoch, uses a single random shuffling of the training data (before dividing between the workers).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes various techniques to speed up the training pipeline by reducing the communication overhead between nodes. (Each node can have one or more GPUs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First, a node sums the gradient from all the GPUs it has.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The gradients are shared and summed across all the nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each node broadcasts the resulting gradient to all the GPUs it has.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient Aggregation is performed in parallel with the backpropagation operator. While aggregating the gradient for one layer, the system starts computing the gradient of the next layer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using these approaches, a Resnet50 model can be trained on the ImageNet dataset in an hour (using 256 workers).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When an appropriate warmup strategy is used, the training and the validation curves (for the large batch size setup) matches the corresponding curves for the small batch size setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best performing warmup strategy is the one where training starts at a learning rate of 0.1 and linearly increases to 3.2 over five epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that the results are not specific to the Resnet50 model (experiments with Resnet101 model) or the use case (experiments with object detection and instance segmentation using Mask R-CNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Along with providing the empirical validation of the proposed ideas, the paper describes all the hyperparameters. It also includes the training and validation curves with the different configurations which enable others to replicate and build on this work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Superposition of many models into one</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Superposition-of-many-models-into-one"/>
   <updated>2020-01-02T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Superposition of many models into one</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a technique (called Parameter Superposition or PSP) for training and storing multiple models within a single set (or instance) of parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The different models exist in “superposition” and can be retrieved dynamically given task-specific context information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.05522&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameter-substitution&quot;&gt;Parameter Substitution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a task with input &lt;script type=&quot;math/tex&quot;&gt;x \in R^N&lt;/script&gt; and parameter &lt;script type=&quot;math/tex&quot;&gt;W$ \in R^{M \times N}&lt;/script&gt; where the output (target or features) are given as &lt;script type=&quot;math/tex&quot;&gt;y=Wx&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now consider &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; such tasks with parameters &lt;script type=&quot;math/tex&quot;&gt;W_1, W_2, \cdots W_K&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If each &lt;script type=&quot;math/tex&quot;&gt;W_k&lt;/script&gt; requires only a small subspace in &lt;script type=&quot;math/tex&quot;&gt;R^N&lt;/script&gt;, then a linear transformation &lt;script type=&quot;math/tex&quot;&gt;C_k^{-1}&lt;/script&gt; can be used such that each &lt;script type=&quot;math/tex&quot;&gt;W_kC_k^{-1}&lt;/script&gt; occupies a mutually orthogonal subspace in &lt;script type=&quot;math/tex&quot;&gt;R^N&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The set of parameters &lt;script type=&quot;math/tex&quot;&gt;W_1, \cdots W_K&lt;/script&gt; can be represented by a single &lt;script type=&quot;math/tex&quot;&gt;W^{M \times N}&lt;/script&gt; by adding &lt;script type=&quot;math/tex&quot;&gt;W_kC_k^{-1}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter corresponding to the &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; task can be retrived (with some noise) using the context &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;W^{~}_k = WC_k&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even though the retrieval is noisy, the effect of noise is limited for the context vectors used in the paper.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, &lt;script type=&quot;math/tex&quot;&gt;\widetilde(y) = \widetilde(W)_{k}x = (WC_{k})x = W(C_{k}x)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of learning &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; separate models, only &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; context vectors (along with 1 superimposed model) needs to be learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key assumption is that &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; (in &lt;script type=&quot;math/tex&quot;&gt;x \in R^N)&lt;/script&gt; is large enough such that each &lt;script type=&quot;math/tex&quot;&gt;W_k&lt;/script&gt; requires only a small subspace of &lt;script type=&quot;math/tex&quot;&gt;R^N&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since images and speech signals tend to occupy a low dimensional manifold, this requirement can be satisfied by over-parameterizing x.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;choice-of-context-c&quot;&gt;Choice of Context C&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rotational Superposition (pspRotation)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sample rotations uniformly from the orthogonal group &lt;script type=&quot;math/tex&quot;&gt;O(M)&lt;/script&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Downside is that if &lt;script type=&quot;math/tex&quot;&gt;M \sim N&lt;/script&gt;, it requires storing as many parameters as learning &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; individual models (since &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is of the size of ##M \times M$$).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Complex Superposition (pspComplex)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The design of rotational superposition can be improved by choosing &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt; to be a diagonal matrix ie &lt;script type=&quot;math/tex&quot;&gt;C_k = diag(c_k)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;c_k&lt;/script&gt; is a vector of size &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Choosing &lt;script type=&quot;math/tex&quot;&gt;c_k&lt;/script&gt; to be a vector of complex numbers (of the form &lt;script type=&quot;math/tex&quot;&gt;c_{k}^{j} = e^{i\phi_{j}(k)}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\phi_{j}(k)&lt;/script&gt; or the phase is sampled uniformly from &lt;script type=&quot;math/tex&quot;&gt;[-\pi, \pi]&lt;/script&gt;) leads to &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt; being a digonal orthogonal matrix.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Powers of a single context&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The memory footprint can be further reduced by choosing the context vectors to be integral powers of the first context vector.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Binary Superposition (pspBinary)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This is a special case of complex superposition where the context vectors are binary.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-network-superposition&quot;&gt;Neural Network Superposition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter superposition principle can be applied to all the linear layers of a network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the convolutional layers, it makes more sense to apply superposition to the convolutional kernel and not to the input image (as the dimensionality of convolutional parameters is smaller than that of inputs).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For all the experiments, the baseline is a standard supervised learning setup, unless mentioned otherwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The metric is the performance on the previous tasks when the model has been trained on the newer tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Input Interference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The input distribution changes over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Permuted MNIST dataset is used where each permutation of the pixels corresponds to a new task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A new task is sampled every 1000 mini-batches.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;As the network size increases, the performance of Parameter Superposition (psp) outperforms the baseline significantly.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;pspRotation &amp;gt; pspComplex &amp;gt; pspBinary in terms of both performance and the number of additional parameters required for each new task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Given that pspBinary is the easiest to implement while being comparable to more sophisticated baselines like Elastic Weight Consolidation (EWC) and Synaptic Intelligence, the paper presents most of the results with the pspBinary model.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continous Domain Shift&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Rotating-MNIST and Rotating-FashionMNIST tasks are proposed to simulate continuous domain shift.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In these tasks, the input images are rotated in-plane by a small angle such that the rotation is complete after 1000 steps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A new context is assigned after 100 steps as per step changes in the angle would be very small.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The 10 context vectors used in the first 1000 steps are reused for the subsequent steps.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly changing the context vector&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper considers an ablation where the context vector is randomly changed at every step (of the 1000 step cycle). This required the superposition model to store 1000 models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This approach is better than the supervised learning baseline but not as good as the proposed psp* models.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output Interference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;This is the setup where the model transitions from one classification task to another.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Incremental CIFAR dataset is used with Resnet18 as the base model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Baseline is a standard supervised learning model where a new classification head is used for each task (since the classes have a different meaning in each dataset). The model component before the classification layer is shared across the tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even though the labels are different across the datasets, the pspBinary model, trained with a single output layer, outperforms the multi-headed baseline.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Towards a Unified Theory of State Abstraction for MDPs</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Towards-a-Unified-Theory-of-State-Abstraction-for-MDPs"/>
   <updated>2019-12-26T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Towards a Unified Theory of State Abstraction for MDPs</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies five different techniques for stat abstraction in MDPs (Markov Decision Processes) and evaluates their usefulness for planning and learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The general idea behind abstraction is to map the actual (or observed) state to an abstract state that should be more amenable for learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It can be thought of as a mapping from one representation to another representation while preserving some useful properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/ca9a/2d326b9de48c095a6cb5912e1990d2c5ab46.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;general-definition&quot;&gt;General Definition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a MDP &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
M = &lt;S, A, P, R, \gamma&gt; %]]&gt;&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is the finite set of states, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; is finite set of actions, &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is the transition function, &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; is the bounded reward function and &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; is the discount factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The abstract version of the MDP is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\widetilde{M} = &lt;\widetilde{S}, A, \widetilde{P}, \widetilde{R}, \gamma&gt; %]]&gt;&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\widetilde{S}&lt;/script&gt; is the finite set if abstract states, &lt;script type=&quot;math/tex&quot;&gt;\widetilde{P}&lt;/script&gt; is the transition function in the abstract state space and &lt;script type=&quot;math/tex&quot;&gt;\widetilde{R}&lt;/script&gt; is the bounded reward function in the abstract reward space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abstraction function &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; is a function that maps a given state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; to its abstract counterpart &lt;script type=&quot;math/tex&quot;&gt;\widetilde{s}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The inverse image &lt;script type=&quot;math/tex&quot;&gt;\phi^{-1}(\widetilde{s})&lt;/script&gt; is the set of ground states that map to the &lt;script type=&quot;math/tex&quot;&gt;\widetilde{s}&lt;/script&gt; under the abstraction function &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A wieghing functioon &lt;script type=&quot;math/tex&quot;&gt;w(s)&lt;/script&gt; is used to measure how much does a state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; contribute to the abstract state &lt;script type=&quot;math/tex&quot;&gt;\phi(s)&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topology-of-abstraction-space&quot;&gt;Topology of Abstraction Space&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given two abstraction functions &lt;script type=&quot;math/tex&quot;&gt;\phi_{1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi_{2}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\phi_{1}&lt;/script&gt; is said to be &lt;em&gt;finer&lt;/em&gt; than &lt;script type=&quot;math/tex&quot;&gt;\phi_{2}&lt;/script&gt; iff for any states &lt;script type=&quot;math/tex&quot;&gt;s_{1}, s_{2}&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;\phi_{1}(s_{1}) = \phi_{1}(s_{2})&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;\phi_{2}(s_{1}) = \phi_{2}(s_{2})&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This &lt;em&gt;finer&lt;/em&gt; relation is reflex, antisymmetric, transitive and partially ordered.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;five-types-of-abstraction&quot;&gt;Five Types of Abstraction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While many abstractions are possible, not all abstractions are equally important.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model-irrelevance abstraction &lt;script type=&quot;math/tex&quot;&gt;\phi_{model}&lt;/script&gt;:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If two states $s_{1}$ and $s_{2}$ have the same abstracted state, then their one-step model is preserved.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Consider any action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and any abstract state &lt;script type=&quot;math/tex&quot;&gt;\widetilde{s}&lt;/script&gt;, if &lt;script type=&quot;math/tex&quot;&gt;\phi_{model}(s_{1} = \phi_{model}(s_{2})&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;R(s_1, a) = R(s_2, a)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sum_{s' \in \phi_{model}^{-1}\widetilde(s)}P_{s_1, s'}^{a} = \sum_{s' \in \phi_{model}^{-1}\widetilde(s)}P_{s_2, s'}^{a}&lt;/script&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}&lt;/script&gt;-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It preserves the state-action value finction for all the states.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi_{Q^{\pi}}(s_1) = \phi_{Q^{\pi}}(s_2)&lt;/script&gt; implies &lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_1, a) = Q^{\pi}(s_1, a)&lt;/script&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{*}&lt;/script&gt;-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal state-action value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;a^{*}&lt;/script&gt;-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal action and its value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi_{\pi^{*}}&lt;/script&gt;-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal action.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of &lt;em&gt;fineness&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;\phi_0 \geq \phi_{model} \geq \phi_{Q^{\pi}} \geq \phi_{Q^*} \geq \phi_{a^*} \geq \phi_{\pi^*}&lt;/script&gt;. Here &lt;script type=&quot;math/tex&quot;&gt;\phi_0&lt;/script&gt; is the identity mapping ie &lt;script type=&quot;math/tex&quot;&gt;\phi_0(s) = s&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If a property applies to any abstraction, it also applies to all the finer abstractions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-theorems&quot;&gt;Key Theorems&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As we go from finer to coarser abstractions, the information loss increases (ie fewer components can be recovered) while the state-space reduces (ie the efficiency of solving the problem increases). This leads to a tradeoff when selecting abstractions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, with abstractions &lt;script type=&quot;math/tex&quot;&gt;\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}, \phi_{a^*}&lt;/script&gt;, the optimal abstract policy &lt;script type=&quot;math/tex&quot;&gt;\widetilde(\pi)^*&lt;/script&gt; is optimal in the ground MDP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, if each state-action pair is visited infinitely often and the step-size decays properly, Q-learning with &lt;script type=&quot;math/tex&quot;&gt;\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}&lt;/script&gt; converges to the optimal state-action value functions in the MDP. More conditions are needed for convergence in the case of the remaining two abstractions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}, \phi_{a^*}&lt;/script&gt;, the model built with the experience converges to the true abstract model with infinite experience if the weighing function &lt;script type=&quot;math/tex&quot;&gt;w(s)&lt;/script&gt; is fixed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>ALBERT - A Lite BERT for Self-supervised Learning of Language Representations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"/>
   <updated>2019-12-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/ALBERT - A Lite BERT for Self-supervised Learning of Language Representations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes parameter-reduction techniques to lower the memory consumption (and improve training speed) of BERT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also proposes to use a self-supervised loss (based on inter-sentence coherence) and argues that this loss is better than the NSP loss used by BERT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.11942&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT architecture is similar to that of BERT with three major differences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Factorized Embedding Parameterization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In BERT and followup works, the embedding size was tied to the size of the context vector.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Since context vector is expected to encoder the entire context, it needs to have a large dimensionality.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;One consequence of this choice is that even the embedding layer (which encodes the representation for each token) has a large size. This increases the overall memory footprint of the model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper proposed to factorize the embedding parameters into two smaller matrics.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The embedding layer learns a low dimensional representation of the tokens and this representation is projected into a high dimensional space.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cross-layer parameter sharing&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;ALBERT shares all the parameters across the layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inter-sentence coherence loss&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;BERT uses two losses - Masked Language Modeling loss (MLM) and Next Sentence Prediction (NSP).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In the NSP task, the model is provided a pair of sentences and it has to predict if the two sentences appear consecutively in the same document or not. Negative samples are created by sampling sentences from different documents.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper argues that NSP is not effective as a loss function as it merges topic prediction and coherence prediction into one task (as the two sentences come from different documents). The topic prediction is an easier task as compared to coherence prediction.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Hence the paper proposes to use the Sentence Order Prediction task where the model has to predict which of the two sentences comes first in a document. The negative samples are created by simply swapping the order in the positive samples. Hence both the sentences come from the same document and topic prediction alone can not be used to solve the task.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Different variants (in terms of size) of ALBERT and BERT models are compared (eg ALBERT, ALBERT-x, BERT-x, etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, ALBERT models have many-times fewer parameters as compared to the BERT models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets - BookCorpus, English Wikipedia.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT-xxlarge significantly outperforms the BERT-large model even though it has around 70% parameters as the BERT-large model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BERT-xlarge performs worse than BERT-base hinting that it is difficult to train such large models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT models also have better data throughput as compared to BERT models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the ALBERT models, an embedding size of 128 performs the best.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the hidden dimension is increased, the model obtains better performance, but with diminishing returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Very wide ALBERT models (say with a context size of 1024) do not benefit much from depth.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using additional training data boosts the performance for most of the downstream tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper empirically shows that using dropout could hurt the performance of the ALBERT models. This observation may not hold for BERT as it does not share parameters across layers and hence may need regularization via dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT also improves the state of the art performance on GLUE, SQuAD and RACE benchmarks, for both single-model and ensemble setup.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Everything Happens for a Reason - Discovering the Purpose of Actions in Procedural Text</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Everything-Happens-for-a-Reason-Discovering-the-Purpose-of-Actions-in-Procedural-Text"/>
   <updated>2019-12-12T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Everything Happens for a Reason - Discovering the Purpose of Actions in Procedural Text</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Procedural text comprehension tasks focus on modeling the effect of actions and predicting what happens next.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But they do not consider &lt;em&gt;why&lt;/em&gt; some actions need to happen before other actions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new model called XPAD (eXPlainable Action Dependency) that considers the &lt;em&gt;purpose&lt;/em&gt; of actions while predicting their effect.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model favors &lt;em&gt;effects&lt;/em&gt; that:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;explain more of actions in the text.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;are more plausible given the context.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An existing procedural text benchmark dataset (Propara) is expanded by adding the task of explaining actions by predicting their dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.04745&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://data.allenai.org/propara/&quot;&gt;Link to the dataset&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Input&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Procedural (chronologically ordered text) sequence of &lt;em&gt;T&lt;/em&gt; sentences.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;List of &lt;em&gt;N&lt;/em&gt; participant entities, whose state changes at some step.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State change matrix $\pi(T \times N)$ with four possible states - move, create destroy, none.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This matrix tracks how property changes after each step.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dependency Explanation Graph&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Identify what steps are necessary to execute a given step (say &lt;em&gt;s&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;) and represent this dependency in the form of a dependency explanation graph &lt;em&gt;G = &amp;lt;S, E&amp;gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this graph, each node is a step and the direction of edge describes the order of dependency.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dependency-graph-dataset&quot;&gt;Dependency Graph Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06975&quot;&gt;Propara dataset&lt;/a&gt; is expanded to extract the dependency graph using both heuristic and automated methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The automated method is based on the coherence assumption that if step &lt;em&gt;s&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; changes state of entity &lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; then &lt;em&gt;s&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; is a precondition for the first subsequent step that changes the state of &lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;xpad-model&quot;&gt;XPAD Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is based on the ProStruct system and uses an encoder-decoder based architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Input: Sentence &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; and entity &lt;em&gt;e&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sentence is encoded using the GloVe vectors and a BiLSTM model and the entity is encoded as an indicator variable.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The combined representation is denoted as &lt;em&gt;c&lt;sub&gt;tj&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This representation is passed through an MLP to generate &lt;em&gt;k&lt;/em&gt; logits that encode the probability of each entity &lt;em&gt;j&lt;/em&gt; undergoing a state change at step &lt;em&gt;t&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Beam search is performed to decode the encoder representation into the state change matrix and dependency graph using a score function that ensures global consistency.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Score function has two components:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;State change score - depends on the likelihood that the selected state changes at step &lt;em&gt;t&lt;/em&gt; given the text and state change history from steps &lt;em&gt;s&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; to &lt;em&gt;s&lt;sub&gt;t-1&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Dependency graph score&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;This is based on the connectivity and likelihood of the resulting dependency explanation graph.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;This score is used to bias the graph search towards:&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;p&gt;predictions that have an identifiable purpose ie checking if a particular state change prediction leads to a connection in the dependency explanation graph.&lt;/p&gt;
                  &lt;/li&gt;
                  &lt;li&gt;
                    &lt;p&gt;graphs that are more likely according to the background knowledge to distinguish likely dependency links from the unlikely ones.&lt;/p&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, XPAD has access to the correct path (in the search space) and learns to minimize the joint loss corresponding to predicting the state change and the dependency explanation graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, XPAD performs beam search to predict the most likely state change and dependency explanation graph.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tasks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State change prediction&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Dependency explanation prediction&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.03969&quot;&gt;Recurrent Entity Networks&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.04582&quot;&gt;Query-Reduction Networks&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06975&quot;&gt;ProLocal and ProGlobal&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.10012&quot;&gt;ProStruct&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;XPAD significantly outperforms all the baseline models on the dependency explanation task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvements on the state change prediction task are less significant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing dependency graph scores from XPAD leads to a drop in the F1 score.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides an elaborate discussion on the different types of errors that the XPAD system makes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Mastering-Atari,-Go,-Chess-and-Shogi-by-Planning-with-a-Learned-Model"/>
   <updated>2019-12-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents the MuZero algorithm that performs planning with a learned model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The algorithm achieves state of the art results on Atari suite (where generally model-free approaches perform the best) and on planning-oriented games like Chess and Go (where generall planning approaches perform the best).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.08265&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-to-standard-model-based-approaches&quot;&gt;Relation to standard Model-Based Approaches&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model-based approaches generally focus on reconstructing the true environment state or the sequence of full observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero focuses on predicting only those aspects that are most relevant for planning - policy, value functions, and rewards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model consists of three components: (representation) encoder, dynamics function, and the prediction network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has two kinds of interactions - real interactions (ie the actions that are actually executed in the real environment) and hypothetical or imaginary actions (ie the actions that are executed in the learned model or the dynamics function).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At any timestep &lt;em&gt;t&lt;/em&gt;, the past observations &lt;em&gt;o&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt;, … &lt;em&gt;o&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; are encoded into the state &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; using the encoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the model takes hypothetical actions for the next &lt;em&gt;K&lt;/em&gt; timesteps by unrolling the model for &lt;em&gt;K&lt;/em&gt; steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each timestep &lt;em&gt;k = 1, …, K&lt;/em&gt;, the dynamics model predicts the immediate reward &lt;em&gt;r&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; and a new hidden state &lt;em&gt;h&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; using the previous hidden state &lt;em&gt;h&lt;sub&gt;k-1&lt;/sub&gt;&lt;/em&gt; and action &lt;em&gt;a&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the same time, the policy &lt;em&gt;p&lt;sup&gt;k&lt;/sup&gt;&lt;/em&gt; and the value function &lt;em&gt;v&lt;sup&gt;k&lt;/sup&gt;&lt;/em&gt; are computed using the prediction network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The initial hidden state &lt;em&gt;h&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt; is initialized using the state &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Any MDP Planning algorithm can be used to search for optimal policy and value function given the state transitions and the rewards induced by the dynamics function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, the MCTS (Monte Carlo Tree Search) algorithm is used and the action &lt;em&gt;a&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; (ie the action that is executed in the actual environment) is selected from the policy outputted by MCTS.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;collecting-data-for-the-replay-buffer&quot;&gt;Collecting Data for the Replay Buffer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;At each timestep &lt;em&gt;t&lt;/em&gt;, the MCTS algorithm is executed to choose the next action (which will be executed in the real environment).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting next observation &lt;em&gt;o&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; and reward &lt;em&gt;r&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; are stored and the trajectory is written to the replay buffer (at the end of the episode).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For every hypothetical step &lt;em&gt;k&lt;/em&gt;, match the predicted policy, value, and reward to the actual target values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The target policy is generated by the MCTS algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The target value function and reward are generated by actually playing the game (or the MDP).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-to-alphazero&quot;&gt;Relation to AlphaZero&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero leverages the search-based policy iteration from AlphaZero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It extends AlphaZero to setups with a single agent (where self-play is not possible) and setups with a non-zero reward at the intermediate time steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The encoder and the predictions functions are similar to ones used by AlphZero.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;K&lt;/em&gt; is set to 5.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: 57 games in Atari along with Chess, Go and Shogi&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero achieves the same level of performance as AlphaZero for Chess and Shogi. In Go, MuZero slightly outperforms AlphaZero despite doing fewer computations per node in the search tree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Atari, MuZero achieves a new state-of-the-art compared to both model-based and model-free approaches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers a variant called MuZero Reanalyze that reanalyzes old trajectories by re-running the MCTS algorithm with the updated network parameter. The motivation is to have a better sample complexity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero performs well even when using a single simulation of MCTS (during inference).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, using more simulations of MCTS helps to achieve better performance through even just 6 simulations per move is sufficient to learn a good model for Ms. Pacman.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Contrastive Learning of Structured World Models</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Contrastive-Learning-of-Structured-World-Models"/>
   <updated>2019-11-28T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Contrastive Learning of Structured World Models</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces Contrastively-trained Structured World Models (C-SWMs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These models use a contrastive approach for learning representations in environments with compositional structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.12247&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tkipf/c-swm&quot;&gt;Link to the code&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The training data is in the form of an experience buffer &lt;script type=&quot;math/tex&quot;&gt;B = \{(s_t, a_t, s_{t+1})\}_{t=1}^T&lt;/script&gt; of state transition tuples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The goal is to learn:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;an encoder &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; that maps the observed states $s_t$ (pixel state observations) to latent state $z_t$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;a transition model &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that predicts the dynamics in the hidden state.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model defines the enegry of a tuple &lt;script type=&quot;math/tex&quot;&gt;(s_t, a_t, s_{t+1})&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;H = d(z_t + T(z_t, a_t), z_{t+1})&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model has an inductive bias for modeling the effect of action as translation in the abstract state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An extra hinge-loss term is added: &lt;script type=&quot;math/tex&quot;&gt;max(0, \gamma - d(z^{~}_{t}, z_{t+1}))&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;z^{~}_{t} = E(s^{~}_{t})&lt;/script&gt; is a corrputed latent state corresponding to a randomly sampled state &lt;script type=&quot;math/tex&quot;&gt;s^{~}_{t}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-oriented-state-factorization&quot;&gt;Object-Oriented State Factorization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The goal is to learn object-oriented representations where each state embedding is structured as a set of objects.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assuming the number of object slots to be &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;, the latent space, and the action space can be factored into &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; independent latent spaces (&lt;script type=&quot;math/tex&quot;&gt;Z_1 \times ... \times Z_K&lt;/script&gt;) and action spaces (&lt;script type=&quot;math/tex&quot;&gt;A_1 \times ... \times A_k&lt;/script&gt;) respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are &lt;em&gt;K&lt;/em&gt; CNN-based object extractors and an MLP-based object encoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The actions are represented as one-hot vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A fully connected graph is induced over &lt;em&gt;K&lt;/em&gt; objects (representations) and the transition function is modeled as a Graph Neural Network (GNN) over this graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The transition function produces the change in the latent state representation of each object.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The factorization can be taken into account in the loss function by summing over the loss corresponding to each object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environments&quot;&gt;Environments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Grid World Environments - 2D shapes, 3D blocks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Atari games - Pong and Space Invaders&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3-body physics simulation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Random policy is used to collect the training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation is performed in the latent space (no reconstruction in the pixel space) using ranking metrics. The observations (to compare against) are randomly sampled from the buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines - auto-encoder based World Models and &lt;a href=&quot;https://arxiv.org/abs/1905.11169&quot;&gt;Physics as Inverse Graphics model&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the grid-world environments, C-SWM models the latent dynamics almost perfectly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing either the state factorization or the GNN transition model hurts the performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;C-SWM performs well on Atari as well but the results tend to have high variance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The optimal values of $K$ should be obtained by hyperparameter tuning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the 3-body physics tasks, both the baselines and proposed models work quite well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, the paper has a section on limitations:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The object extractor module can not disambiguate between multiple instances of the same object (in a scene).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The current formulation of C-SWM can only be used with deterministic environments.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Gossip based Actor-Learner Architectures for Deep RL</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Gossip-based-Actor-Learner-Architectures-for-Deep-RL"/>
   <updated>2019-09-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Gossip based Actor-Learner Architectures for Deep RL</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.04585&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers the task of training an RL system by sampling data from multiple simulators (over parallel devices).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is that of distributed RL setting with &lt;em&gt;n&lt;/em&gt; agents or actor-learners (composed of a single learner and several actors). These agents are trying to maximize a common value function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One (existing) approach is to perform on-policy updates with a shared policy. The policy could be updated in synchronous (does not scale well) or asynchronous manner (can be unstable due to stale gradients).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Off policy approaches allow for better computational efficiency but can be unstable during training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed Gossip based Actor-Learner Architecture (GALA) which uses asynchronous communication (gossip) between the &lt;em&gt;n&lt;/em&gt; agents to improve the training of Deep RL models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These agents are expected to converge to the same policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the different agents are not required to share the same policy and it is sufficient that the agent’s policies remain $\epsilon$-close to each other. This relaxation allows the policies to be trained asynchronously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA approach is combined with A2C agents resulting in GALA-A2C agents. They have better computational efficiency and scalability (as compared to A2C) and similar in performance to A3C and Impala.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training alternates between one local policy-gradient (and TD update) and asynchronous gossip between agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the gossip step, the agents send their parameters to some of the other agents (referred to as the peers) and update their parameters based on the parameters received from the other agents (for which the given agent is a peer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA agents are implemented using non-blocking communication so that they can operate asynchronously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper includes the proof that the policies learned by the different agents are within $\epsilon$ distance of each other (ie all the policies lie within an $\epsilon$-distance ball) thus ensuring that the policies do not diverge much from each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Six games from the Ataru 2600 games suite are used for the experiments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines: A2C, A3C, Impala&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA agents are configured in a directed ring graph topology.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With A2C, as the number of simulators increases, the number of convergent runs (runs with a threshold reward) decreases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using gossip algorithms increases or maintains the number of convergent runs. It also improves the performance, sample efficiency and compute efficiency of A2C across all the six games.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When compared to Impala and A3C, GALA-A2C generally outperforms (or performs as well as) those baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the learned policies remain within an $\epsilon$ ball, the agent’s gradients are less correlated as compared to the A2C agents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How to train your MAML</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/How-to-train-your-MAML"/>
   <updated>2019-09-05T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/How to train your MAML</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes MAML++ - a modification of MAML algorithm that stabilizes its training improves generalization performance and reduces the computational overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.09502&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&quot;unstable-training&quot;&gt;Unstable Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training the outer loop requires unfolding the inner loop multiple times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In absence of skip connections, the gradient is multiplied by the same parameter multiple times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Large depth and absent skip connections could lead to exploding and vanishing gradients respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to stabilize the gradient propagation by minimizing the target set loss computed by the base-network after every step towards a support set task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is important to anneal the contribution of earlier steps and increase the contribution of later steps over time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;second-order-derivatives-are-expensive-to-compute&quot;&gt;Second Order derivatives are expensive to compute&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While the first-order MAML is faster, the resulting model may not have as good a generalization error as the second-order MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to use derivative order annealing where the first order gradients are used for the first 50 epochs and the network uses second-order gradients from thereon.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This derivative order annealing appears to be more stable than models that use second-order derivatives only.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In MAML, the statistics of the current batch are used for normalization instead of accumulating the running statistics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to collect the statistics per step which can increase the convergence speed, stability, and generalization performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In MAML, the batch normalization biases are not updated in the inner-loop which can adversely impact the performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a set of biases (per step) within the inner loop update.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fixed-learning-rate&quot;&gt;Fixed Learning Rate&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MAML uses a single learning rate across all the steps and all the parameters. This means there is one single learning rate that needs to be hyperparameter to work well for all the layers and steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An alternate solution would be to learn a separate learning rate per parameter but this can be impractical as it doubles the number of parameters to be learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a learning rate and direction for each layer in the network, for each step it takes in the inner loop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also proposed to anneal the learning rate of the outer loop (using cosine annealing) as it helps to achieve better generalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using these modifications helps to outperform the MAML model on both Omniglot and MiniImagenet datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The biggest benefit comes by learning the per-layer, per-step learning rates and by using the per-step batch normalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>PHYRE - A New Benchmark for Physical Reasoning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/PHYRE-A-New-Benchmark-for-Physical-Reasoning"/>
   <updated>2019-08-29T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/PHYRE - A New Benchmark for Physical Reasoning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes the PHYRE (PHYsical REasoning) benchmark - consisting of classic mechanical puzzles in 2D physical environments - as a means to evaluate the physical reasoning ability of machine learning models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.05656&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D world that obeys Newtonian mechanics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gravitational force + Friction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-deformable objects that can be static (ie fixed) or dynamic (ie can move and are affected by collisions etc).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent starts in some initial world state (ie configuration of objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal is described in the form of (&lt;code class=&quot;highlighter-rouge&quot;&gt;subject&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;relation&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;object&lt;/code&gt;) where the agent’s task is to satisfy the &lt;code class=&quot;highlighter-rouge&quot;&gt;relation&lt;/code&gt; between the &lt;code class=&quot;highlighter-rouge&quot;&gt;subject&lt;/code&gt; and the &lt;code class=&quot;highlighter-rouge&quot;&gt;object&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Currently, only the “touch” &lt;code class=&quot;highlighter-rouge&quot;&gt;relation&lt;/code&gt; is supported.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has to take a single action - placing one or more new dynamic objects in the world.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A simulator is run on the new configuration (for a fixed amount of time) to check if the goal condition is satisfied.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the end of the simulation, a binary reward and intermediate observations (collected as the simulator executes) are provided to the learning agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These observations are 256*256 grids where each grid cell can take 1 of the 7 values (denoting different types of objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since only one relation supported currently, the color is sufficient to encode the goal.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmark-tiers&quot;&gt;Benchmark Tiers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two benchmark tiers are provided where each tier comprises of a combination of:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;a predefined set of all the actions that the agent is allowed to perform.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;set of tasks that can be solved by at least one action from the allowed action set.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PHYRE-B&lt;/strong&gt; - The agent is allowed to place a single (ball of any radii) at any valid location.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PHYRE-2B&lt;/strong&gt; - The agent is allowed to place 2 balls at any valid pair of locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each of the two tiers has 25 task templates where each template comprises of variants of a single task (same goal but different initial conditions).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two evaluation setups are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;within-template&lt;/strong&gt; where the agent is trained on some tasks in a template and evaluated on a set of held-out tasks from the same template.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;cross-template&lt;/strong&gt; where the agent is evaluated on tasks from a different template.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the training phase, the model has access to the simulator (but not to the correct solution). So the model could learn an action-prediction model or forward dynamics model or both.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the testing phase, the model can query the simulator only a few times. Each query provides it with the binary reward and the intermediate observations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-measure&quot;&gt;Performance Measure&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The emphasis is on solving more tasks (in few queries) during the test phase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This requirement is captured using a metric called AUCCESS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the tasks in PHYRE-2B are harder than tasks in PHYRE-B.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baseline-agents&quot;&gt;Baseline Agents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Random Agent - Randomly samples actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-parametric agent (MEM) - generates R actions at random and uses the simulator to check how many tasks can be solved using these R random actions. During testing, try the R actions in the decreasing order of the number of tasks they solve.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-parametric agent with online learning (MEM-O) - Variant of MEM where an online adaptation step is performed during test time (to update the rank of the actions).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep Q Networks with an action encoder, observation encoder and fusion model (combine action and observation representation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DQN with online learning (DQN-0): Variant of DQN with online updates (during the test phase).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Contextual bandits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy learning approaches like PPO and A2C.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Both Contextual bandits and policy-based approaches show poor training stability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best agent, DQN-O, reaches AUCCESS of 56.2\% on PHYRE-B and 39.26\% on PHYRE-2B. In general, agents with online adaptation perform better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The tasks are designed such that 100000 attempts are sufficient to solve 100\% of tasks in PHYRE-B and 95\% of tasks in PHYRE-2B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even though only two tiers are provided right now, the benchmark is readily extensible and new tasks can be added in the future.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Large Memory Layers with Product Keys</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Large-Memory-Layers-with-Product-Keys"/>
   <updated>2019-08-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Large Memory Layers with Product Keys</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes a structured key-value memory layer that:
    &lt;ul&gt;
      &lt;li&gt;Can scale to a very large size (and capacity).&lt;/li&gt;
      &lt;li&gt;Has very low computational overhead.&lt;/li&gt;
      &lt;li&gt;Supports exact search in the keyspace.&lt;/li&gt;
      &lt;li&gt;Can be easily integrated with neural networks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.05242&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The memory layer is composed of 3 components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Query Network&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Maps input to a latent space.&lt;/li&gt;
          &lt;li&gt;Can be implemented as a feed-forward network.&lt;/li&gt;
          &lt;li&gt;Adding batch-norm on top of the query network helps to spread out keys.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Key selection module&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Lets say there are a total of &lt;em&gt;K&lt;/em&gt; keys of dimensionality &lt;em&gt;d&lt;sub&gt;q&lt;/sub&gt;&lt;/em&gt; of which we want to select top &lt;em&gt;k&lt;/em&gt; keys.&lt;/li&gt;
          &lt;li&gt;Partition the set of keys into two sets of &lt;em&gt;subkeys&lt;/em&gt; (say &lt;em&gt;Q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;Q&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;) where each subset has &lt;em&gt;K&lt;/em&gt; keys of dimensionality &lt;em&gt;d_q/2&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;The query is split into two subqueries (say &lt;em&gt;q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;q&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;).&lt;/li&gt;
          &lt;li&gt;Each of these two queries are compared with every query in their corresponding set of &lt;em&gt;subkeys&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;For example, &lt;em&gt;q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is compared with every query is &lt;em&gt;Q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;Top &lt;em&gt;k&lt;/em&gt; ranked keys are selected from each set to create two new sets &lt;em&gt;C&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;C2&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;The keys from these two sets are combined uder the concatenation operator to obtain &lt;em&gt;k&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; vectors.&lt;/li&gt;
          &lt;li&gt;the final top &lt;em&gt;k&lt;/em&gt; (concatenated) keys are searched from these *k&lt;sup&gt;2* keys.&lt;/sup&gt;&lt;/li&gt;
          &lt;li&gt;The overall complexity is $O((\sqrt K + k^2) \times d_q)$ where &lt;em&gt;K&lt;/em&gt; is the total number of keys (whiuc)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Value lookup table&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;The values (corresponding to selected subkeys) are aggregated (using weighted sum operation) to obtain the output.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the parameters are trainable, though, in practice, only the selected &lt;em&gt;k&lt;/em&gt; memory slots are updated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Multihead attention mechanism helps to improve the performance further.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;1 or more feedforward layers in transformers are placed by the memory layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is evaluated on large scale language modeling tasks with 140 Gb of data from common crawl corpora (28n billion words).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Perplexity on the test set.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Fraction of accessed values.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;KL divergence between the (normalized) weights of key access and uniform distribution.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The last two metrics are used together to determine how well the keys are utilized.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the large size of the training dataset, adding more layers to the transformer model helps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Effect of using memory layer is more powerful than the effect of adding new layers to the transformer. For example, a 12 layer transformer + memory layer outperforms a 24 layer transformer while being almost twice as fast.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best position to place the memory is at an intermediate layer and placing the memory layer right after the input or just before the softmax layer does not work well in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Abductive Commonsense Reasoning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Abductive-Commonsense-Reasoning"/>
   <updated>2019-08-15T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Abductive Commonsense Reasoning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents the task of abductive NLP (pronounced as &lt;em&gt;alpha NLP&lt;/em&gt;) where the model needs to perform abductive reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.05739&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task-setup&quot;&gt;Task Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a pair of observations &lt;em&gt;O&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;O&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; and two hypothesis &lt;em&gt;h&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;h&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, the task is to select the most plausible hypothesis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, &lt;em&gt;P(h | O&lt;sub&gt;1&lt;/sub&gt;, O&lt;sub&gt;2&lt;/sub&gt;)&lt;/em&gt; is propotional to &lt;em&gt;P(h |O&lt;sub&gt;1&lt;/sub&gt;)P(O&lt;sub&gt;2&lt;/sub&gt;|h, O&lt;sub&gt;1&lt;/sub&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the “fully connected” assumption would jointly model both the observations and the hypothesis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observation pairs are created using the &lt;a href=&quot;https://aclweb.org/anthology/N16-1098&quot;&gt;ROCStories dataset&lt;/a&gt; which is a collection of short, manually crafted stories of 5 sentences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The average word length for both the content and the hypothesis is between 8 to 9.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To collect plausible hypothesis, the crowd workers were asked to fill in a plausible “in-between” sentence in natural language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adversarial filtering approach from &lt;a href=&quot;https://aclweb.org/anthology/D18-1009&quot;&gt;Zellers et al.&lt;/a&gt; is used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-observations&quot;&gt;Key Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Human performance: 91.4%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BERT: 68.9%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPT: 63.1%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Numerical and spatial knowledge-based data points are particularly hard.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Deep-Reinforcement-Learning-in-a-Handful-of-Trials-using-Probabilistic-Dynamics-Models"/>
   <updated>2019-08-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new algorithm called as Probabilistic Ensemble with Trajectory sampling (PETS) that combines uncertainty aware deep learning models (ensemble of deep learning models that encode uncertainty) with sampling-based uncertainty propagation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PETS improves over other probabilistic MBRL approaches by isolating epistemic uncertainty (due to limited training data) and aleatoric uncertainty (inherent in the system).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;uncertainty-aware-neural-network-dynamics-model&quot;&gt;Uncertainty-Aware Neural Network Dynamics Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Aleatoric uncertainty can be accounted for by learning a parameterized distribution (probabilistic neural network) trained with negative log-likelihood.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Epistemic uncertainty can be accounted for by either having an infinite amount of data or by using ensembles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses a neural network to predict the mean and standard deviation of a gaussian distribution which defines the predictive model. This setup is referred to as the “probabilistic” model and denoted by &lt;strong&gt;P&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternate setup of the deterministic model is where a neural network is used to make a point prediction (and is denoted by &lt;strong&gt;D&lt;/strong&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble of probabilistic models is denoted as &lt;strong&gt;PE&lt;/strong&gt; while that of deterministic models is denoted as &lt;strong&gt;DE&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;planning-and-control-with-learned-dynamics&quot;&gt;Planning and Control with learned Dynamics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model Predictive Control (MPC) is used for planning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a start state and an action sequence, the probabilistic dynamics model induces a distribution over the trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first action, among the sequence of optimized actions, is executed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of random shooting, &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/B9780444538598000035&quot;&gt;Cross Entropy Method (CEM)&lt;/a&gt; is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let us say there are B bootstrap models in the ensemble. Given the current state, P particles are created and each particle is propagated using one of the bootstrap models. Two variants are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;TS1 - At each timestep, each particle samples a bootstrap. In this case, particle separation can not be attributed to ti the compounding effects of the bootstraps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;TS$\infty$ - The bootstrapped model (per particle) is sampled just once and is not changed after that. This setup separates aleatoric and epistemic uncertainty. Aleatoric state variance is the average variance of the particles of the same bootstrap while epistemic state variance is the variance of the average of particles of same bootstrap indexes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach reaches the asymptotic performance of state-of-the-art model-free algorithms in much fewer samples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The general performance trend is probabilistic emsemble &amp;gt; probabilisitc model &amp;gt; deterministc ensemble &amp;gt; determinisitc model./.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Initial experiments for learning policy by propagating gradients through the ensemble of models did not work and has been left as future work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Assessing Generalization in Deep Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Assessing-Generalization-in-Deep-Reinforcement-Learning"/>
   <updated>2019-08-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Assessing Generalization in Deep Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Several RL algorithms are evaluated and the key takeaway is that the “vanilla” RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.12282&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two generalization regimes are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Interpolation - parameters of the test environment are similar to the parameters of the training environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation - parameters of the test environment are different from the parameters of the training environment.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Following algorithms are considered as part of the benchmark:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;“Vanilla” RL algorithms - A2C, PPO&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RL algorithms that are designed to generalize:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;RL&lt;sup&gt;2&lt;/sup&gt; - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The models are either composed of feedforward networks completely or feedforward + recurrent networks.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HalfCheetah and Hopper from OpenAI Roboschool.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Three versions of each environment are considered:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Deterministic: Environment parameters are fixed. This case corresponds to the standard environment setup in classical RL.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Random: Environment parameters are sampled randomly. This case corresponds to sampling from a distribution of environments.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Extreme: Environment parameters are sampled from their extreme values. This case corresponds to the edge-case environments which would not be encountered during training generally.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performance Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Average total reward per episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Success percentage: Percentage of episodes where a certain goal (or reward) is obtained.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation Metrics/Setups&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Default: success percentage when training and evaluating the deterministic version of the environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Interpolation: success percentage when training and evaluating on the random version of the environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation: the geometric mean of the success percentage of following three versions:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Train on deterministic and evaluate on the random version.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Train on deterministic and evaluate on extreme version.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Train on random and evaluate on the extreme version.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation is harder than interpolation.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EPOpt improves generalization only for continuous control environments and only with PPO.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RL&lt;sup&gt;2&lt;/sup&gt; is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Quantifying Generalization in Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Quantifying-Generalization-in-Reinforcement-Learning"/>
   <updated>2019-07-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Quantifying Generalization in Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.02341&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;coinrun-environment&quot;&gt;CoinRun Environment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CoinRun is made of multiple levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are many obstacles in between and colliding with an obstacle leads to agent’s death.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each episode extends for a maximum for 1000 steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalization&quot;&gt;Generalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;9 agents are trained to play CoinRun, on different training sets (each with a different number of levels).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-architectures&quot;&gt;Evaluating Architectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two convolutional architectures (of different sizes) are compared:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Nature-CNN: The CNN architecture used in the &lt;a href=&quot;https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf&quot;&gt;Deep Q Network&lt;/a&gt;. This is the smaller network among the two models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;IMPALA-CNN: The CNN architecture used in the &lt;a href=&quot;https://arxiv.org/abs/1802.01561&quot;&gt;Imapla architecture&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;IMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization. But increasing the network size beyond a limit gives diminishing returns.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-regularization&quot;&gt;Evaluating Regularization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color. This approach seems to improve performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch Normalization helps to improve performance as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus. Both these forms of stochasticity boost performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach. This suggests that these different approaches induce similar generalization properties.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additional-environments&quot;&gt;Additional Environments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CoinRun-Platforms:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Unlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Levels are larger as well so the agent might need to backtrack their steps.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RandomMazes:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Partially observed environment with square mazes of dimensions 3x3 to 25x25.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Timelimit of 500 steps&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overfitting is observed for both these environments as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Set Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Set-Transformer-A-Framework-for-Attention-based-Permutation-Invariant-Neural-Networks"/>
   <updated>2019-07-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Set Transformer A Framework for Attention-based Permutation-Invariant Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider problems where the input to the model is a set. In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In “set pooling” methods (&lt;a href=&quot;https://arxiv.org/abs/1606.02185&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1703.06114&quot;&gt;2&lt;/a&gt;), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the “sum” operator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This approach can be shown to be bot permutation-invariant and a universal function approximator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.00825&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An attention function &lt;em&gt;Attn(Q, K, V) = (QK&lt;sup&gt;T&lt;/sup&gt;)V&lt;/em&gt; is used to map queries &lt;em&gt;Q&lt;/em&gt; to output using key-value pairs &lt;em&gt;K, V&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of multi-head attention, the key, query, and value are projected into &lt;em&gt;h&lt;/em&gt; different vectors and attention is applied on all these vectors. The output is a linear transformation of the concatenation of all the vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;set-transformer&quot;&gt;Set Transformer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;3 modules are introduced: MAB, SAB and ISAB.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Set Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie &lt;em&gt;SAB(X) = MAB(X, X)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The time complexity of the SAB operation is &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt; where &lt;em&gt;n&lt;/em&gt; is the number of elements in the set. It can be reduced to &lt;em&gt;O(m*n)&lt;/em&gt; by using Induced Set Attention Blocks (ISAB) with &lt;em&gt;m&lt;/em&gt; induced point vectors (denoted as I).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;ISAB&lt;sub&gt;m&lt;/sub&gt; = MAB(X, MAB(I, X))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ISAB can be seen as performing a low-rank projection of inputs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These modules can be used to model the interactions between data points in any given set.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pooling-by-multihead-attention-pma&quot;&gt;Pooling by Multihead Attention (PMA)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Aggregation is performed by applying multi-head attention on a set of &lt;em&gt;k&lt;/em&gt; seed vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The interaction between the &lt;em&gt;k&lt;/em&gt; outputs (from PMA) can be modeled by applying another SAB.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thus the entire network is a stack of SABs and ISABs. Both the modules are permutation invariant and so is any network obtained by stacking them.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets include:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Predicting the maximum value from a set.&lt;/li&gt;
      &lt;li&gt;Counting unique (Omniglot) characters from an image.&lt;/li&gt;
      &lt;li&gt;Clustering with a mixture of Gaussians (synthetic points and CIFAR 100).&lt;/li&gt;
      &lt;li&gt;Set Anomaly detection (celebA).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, increasing &lt;em&gt;m&lt;/em&gt; (the number of inducing datapoints) improve performance, to some extent. This is somewhat expected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The work has two main benefits over prior work:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Reducing the &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt; complexity to &lt;em&gt;O(m*n)&lt;/em&gt; complexity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using self-attention mechanism for both encodings the inputs and for aggregating the encoded representations.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Measuring abstract reasoning in neural networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Measuring-Abstract-Reasoning-in-Neural-Networks"/>
   <updated>2019-06-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Measuring Abstract Reasoning in Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Visual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let’s say that we want to test if the model understands the abstract notion of “increasing”. We could train the model on data that captures the notion of “increasing”, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;h3 id=&quot;ravens-progressive-matrics-rpms&quot;&gt;Raven’s Progressive Matrics (RPMs):&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;procedurally-generated-matrices-pgms&quot;&gt;Procedurally Generated Matrices (PGMs)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generating RPM like matrices procedurally by building an abstract structure for matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;The abstract structure &lt;em&gt;S&lt;/em&gt; consists of 3 components: (i) Relation types (&lt;em&gt;R&lt;/em&gt;), (ii) Object types (&lt;em&gt;O&lt;/em&gt;) and (iii) Attribute types (&lt;em&gt;A&lt;/em&gt;). ie *S = {(r, o, a)&lt;/td&gt;
          &lt;td&gt;r in R, o in O and a in A}*.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be read as: “Structure &lt;em&gt;S&lt;/em&gt; is instantiated on attribute &lt;em&gt;a&lt;/em&gt; of object &lt;em&gt;o&lt;/em&gt; and exhibits the relation &lt;em&gt;r&lt;/em&gt;”. For example, &lt;em&gt;S&lt;/em&gt; is instantiated on “color” of object “shape” and exhibits the relation “increasing”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the structure could be made of more than one such tuple and more are the tuples, harder is the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the structure, sample values &lt;em&gt;v&lt;/em&gt; for each attribute &lt;em&gt;a&lt;/em&gt; while conforming with the relation &lt;em&gt;r&lt;/em&gt;. For example, if the attribute is “color” and the relation is “increasing”, the intensity of color must increase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The resulting structure is rendered as pixels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;test-for-generalization&quot;&gt;Test for Generalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper tests for the following generalization scenarios:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neutral: The structure of the training and test data can contain any tuple.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: 8 context panels (from the 3x3) matrix where the last panel needs to be filled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN-MLP - 4 layer CNN with batchnorm and ReLU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Context-blind ResNet - ResNet network without the context (or the 8 input panels).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WReN model outperforms the other models on the Neutral setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models have a harder time differentiating between size than quantity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WRen is the best performing models in all the setups and rest of the discussion only applies to that model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auxiliary-training&quot;&gt;Auxiliary Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The auxiliary training helps in all the cases. Further, the model’s accuracy on the main task is where in the cases where it solves the auxiliary tasks well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-takeaway&quot;&gt;Key Takeaway&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using auxiliary loss that encourages the model to “explain” its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hamiltonian Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hamiltonian-Neural-Networks"/>
   <updated>2019-06-20T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hamiltonian Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a very cool idea at the intersection of deep learning and physics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01563&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/greydanus/hamiltonian-nn&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://greydanus.github.io/2019/05/15/hamiltonian-nns/&quot;&gt;Link to author’s blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hamiltonian-mechanics&quot;&gt;Hamiltonian Mechanics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is a branch of physics that can describe systems which follow some conservation laws and invariants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a set of &lt;em&gt;N&lt;/em&gt; pair of coordinates [(q&lt;sub&gt;1&lt;/sub&gt;, p&lt;sub&gt;1&lt;/sub&gt;), …, (q&lt;sub&gt;N&lt;/sub&gt;, p&lt;sub&gt;N&lt;/sub&gt;)] where &lt;strong&gt;q&lt;/strong&gt; = [q&lt;sub&gt;1&lt;/sub&gt;, …, q&lt;sub&gt;N&lt;/sub&gt;] dnotes the position of the set of objects while &lt;strong&gt;p&lt;/strong&gt; = [p&lt;sub&gt;1&lt;/sub&gt;, …, p&lt;sub&gt;N&lt;/sub&gt;] denotes the momentum of the set of variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Together these &lt;em&gt;N&lt;/em&gt; pairs completely describe the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A scalar function &lt;em&gt;H(&lt;strong&gt;q&lt;/strong&gt;, &lt;strong&gt;p&lt;/strong&gt;)&lt;/em&gt;, called as the Hamiltonian is defined such that the partial derivative of &lt;em&gt;H&lt;/em&gt; with respect to &lt;strong&gt;p&lt;/strong&gt; is equal to derivative of &lt;strong&gt;q&lt;/strong&gt; with respect to time &lt;em&gt;t&lt;/em&gt; and the negative of partial derivative of &lt;em&gt;H&lt;/em&gt; with respect to &lt;strong&gt;q&lt;/strong&gt; is equal to derivative of &lt;strong&gt;p&lt;/strong&gt; with respect to time &lt;em&gt;t&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be expressed in the form of the equation as follows:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation1.png&quot; alt=&quot;equation1&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hamiltonian-neural-network-hnn&quot;&gt;Hamiltonian Neural Network (HNN)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Hamiltonian &lt;em&gt;H&lt;/em&gt; can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The loss function looks as follows:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation2.png&quot; alt=&quot;equation2&quot; width=&quot;400&quot; height=&quot;50&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The partial derivatives can be obtained by computing the &lt;em&gt;in-graph&lt;/em&gt; gradient of the output variables with respect to the input variables.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the final experiment, the model is trained on pixel observations and not state observations. In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations. Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Extrapolating-Beyond-Suboptimal-Demonstrations-via-Inverse-Reinforcement-Learning-from-Observations"/>
   <updated>2019-06-13T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard IRL approaches aim to learn a reward function that “justifies” the demonstration policy and hence those approaches cannot outperform the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In contrast, T-REX aims to learn a reward function that “explains” the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06387&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The input is a sequence of trajectories &lt;em&gt;T&lt;sub&gt;1&lt;/sub&gt;, … T&lt;sub&gt;m&lt;/sub&gt;&lt;/em&gt; which are ranked in the order of preference. That is, given any pair of trajectories, we know which of the two trajectories is better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reward Inference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A parameterized reward function &lt;em&gt;r&lt;sub&gt;θ&lt;/sub&gt;&lt;/em&gt; is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Given a trajectory, the reward function predicts the reward for each state. The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Optimization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Once a reward function has been learned, standard RL approaches can be used to train a new policy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Demonstrations generated using PPO (checkpointed at different stages of training).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble of networks used to learn the reward functions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach outperforms the baselines &lt;a href=&quot;https://arxiv.org/abs/1805.01954&quot;&gt;Behaviour Cloning from Observations&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1606.03476&quot;&gt;Generative Adversarial Imitation Learning&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Meta-Reinforcement Learning of Structured Exploration Strategies</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Meta-Reinforcement-Learning-of-Structured-Exploration-Strategies"/>
   <updated>2019-06-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Meta-Reinforcement Learning of Structured Exploration Strategies</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper looks at the problem of learning structured exploration policies for training RL agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Link to the &lt;a href=&quot;https://arxiv.org/abs/1802.07245&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;structured-exploration&quot;&gt;Structured Exploration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a stochastic, parameterized policy π&lt;sub&gt;θ&lt;/sub&gt;(a|s) where θ represents the &lt;em&gt;policy-parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a distibution over the tasks p(T). At the start of any episode of the i&lt;sup&gt;th&lt;/sup&gt; task, a latent variable z&lt;sub&gt;i&lt;/sub&gt; is sampled from the distribution &lt;em&gt;N(μ&lt;sub&gt;i&lt;/sub&gt;, σ&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; where μ&lt;sub&gt;i&lt;/sub&gt; and σ&lt;sub&gt;i&lt;/sub&gt; are the learned parameters of the distribution and are referred to as the &lt;em&gt;variation parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once sampled, the same &lt;em&gt;z&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution π&lt;sub&gt;θ&lt;/sub&gt;(a|s, z&lt;sub&gt;i&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The intuition is that the latent variable z&lt;sub&gt;i&lt;/sub&gt; would encode the notion of a task or goal that does not change arbitrarily during the episode.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-agnostic-exploration-with-structured-noise&quot;&gt;Model Agnostic Exploration with Structured Noise&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the &lt;em&gt;policy-parameters&lt;/em&gt; and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General meta-RL approaches have two limitations when it comes to “learning to explore”:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively.&lt;/li&gt;
      &lt;li&gt;Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Idea behind MAESN is to meta-train &lt;em&gt;policy-parameters&lt;/em&gt; so that they learn to use the task-specific &lt;em&gt;latent variables&lt;/em&gt; for exploration and can quickly adapt to a new task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For every iteration of meta-training, an “inner” gradient update is performed on the variational parameters and the &lt;em&gt;post-inner-update&lt;/em&gt; parameters are used to perform the meta-update.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors report that performing the “inner” gradient update on the &lt;em&gt;policy-parameters&lt;/em&gt; does not help the overall learning objective and that the step size for each parameter had to be meta-learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After training, the &lt;em&gt;variational parameters&lt;/em&gt; for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the &lt;em&gt;variational parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training. To be fair to the baselines, reward shaping is used while training baselines as well. Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion. Each task distribution has 100 meta-training tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions. In the Locomotion task distributions, the different tasks correspond to the different goal positions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;• In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Relational Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Relational-Reinforcement-Learning"/>
   <updated>2019-06-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Relational Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.01830&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterative computations can be used to capture higher-order interactions between entities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entity extraction is based on the assumption that entities are things located at a particular point in space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A CNN is used to parse the pixel space observation into &lt;em&gt;k&lt;/em&gt; feature maps of size &lt;em&gt;nxn&lt;/em&gt;. The &lt;em&gt;(x, y)&lt;/em&gt; coordinates are concatenated to each &lt;em&gt;k-&lt;/em&gt;dimensional pixel feature-vector to indicate the pixel’s position in the map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting &lt;em&gt;n&lt;sup&gt;2&lt;/sup&gt; x k&lt;/em&gt; matrix acts as the entity matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-critic architecture (using distributed agent IMPALA) is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;h3 id=&quot;box-world&quot;&gt;Box-World&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;12 x 12-pixel room with keys and boxes placed randomly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Agent can move in 4 directions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal. (ii) The number of distractor branches, (iii)  Length of distractor branches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;starcraft-ii-minigames&quot;&gt;StarCraft II minigames&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;9 mini games designed as specific scenarios in the Starcraft game are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;box-world-1&quot;&gt;Box-World&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Visualising the attention scores indicate that:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;keys attend to locks they can unlock.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;all objects attend to agent’s location.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;agent and gem attend to each other (and themselves).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization capacity is tested in two ways:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Performance on levels that require opening a larger sequence of boxes than it is trained on.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Performance on levels that require key-lock combinations not seen during training.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In both the scenarios, the RRL agent significantly outperforms the RL agent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;starcraft&quot;&gt;StarCraft&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RLL agent achieves better or equal results that the RL agent in all but one game.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines. These results are not conclusive given the high variability.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Good-Enough Compositional Data Augmentation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Good-Enough-Compositional-Data-Augmentation"/>
   <updated>2019-05-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Good-Enough Compositional Data Augmentation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Synthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments. This operation is referred to as GECA (Good Enough Compositional Augmentation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.09545&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Discover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same. A lexical environment is the k-word window around each span of the fragment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands. Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of λ calculus or SQL and low resource language modeling. While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the proposed approach is relatively simple and straightforward, it appears to be quite promising.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Model-Based Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Multiple-Model-Based-Reinforcement-Learning"/>
   <updated>2019-05-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Multiple Model-Based Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents some general ideas and mechanisms for multiple model-based RL. Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful. As such, I am focusing only on high-level ideas and not the implementation details themselves.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.mitpressjournals.org/doi/abs/10.1162/089976602753712972&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The prediction error from each of the state prediction model defines the “responsibility signal” for each module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This responsibility signal is used to:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Weigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Weigh the parameter update of the environment models as well as the RL controllers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Weighing the action output - ie predicted action is a weighted sum of individual actions.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is amenable for incorporating prior knowledge about which module should be selected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Temporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exploration can be ensured by using a stochastic version of greedy action selection.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One failure mode for such modular architectures is when a single module tries to perform well across all the tasks. The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Towards a natural benchmark for continual learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Towards-a-natural-benchmark-for-continual-learning"/>
   <updated>2019-04-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Towards a natural benchmark for continual learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Continual Learning paradigm focuses on learning from a non-stationary stream of data with additional desiderata - transferring knowledge from previously seen task to unseen tasks and being resilient to catastrophic forgetting - all with a fixed memory and computational budget.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is in contrast to the IID (independent and identically distributed) assumption in statistical learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One common example of the non-iid data is setups involving sequential decision making - eg Reinforcement learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://marcpickett.com/cl2018/CL-2018_paper_48.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Many existing benchmarks use MNIST as the underlying dataset (eg Permuted MNIST, Split MNIST, etc). These benchmarks lack complexity and make it hard to observe positive and negative backward transfer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most works focus only on the catastrophic forgetting challenge and ignore the other issues (like computation and memory footprint, the capacity of the network, etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new benchmark based on Starcraft II video game to understand the different approaches for lifelong learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequence of tasks is designed to be a curriculum - the learning agent stats with learning simple skills and later move to more complex tasks. These complex tasks require remembering and composing skills learned in the earlier levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To evaluate for catastrophic forgetting, the tasks are designed such that not all the skills are needed for solving each task. Hence the learning agent needs to remember skills even though they are not needed at the current level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each level comes with a fixed computational budget of episodes and each episode has a fixed time limit. Once the budget is consumed the agent has to proceed to the next level. Hence agents with better sample efficiency would benefit.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The benchmark supports both RL and supervised learning version. In the supervised version, expert agents (pretrained on each level) are also provided.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines are provided for distillation (using experts): sequential training (fine tuning), Dropout and SER. None of the baseline methods achieve positive or negative backward transfer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When modeled as a pure RL task, the benchmark is extremely difficult to solve.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper suggests using a metric to record the amount of learning/data required to recover performance on the previous task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Meta-Learning Update Rules for Unsupervised Representation Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Meta-Learning-Update-Rules-for-Unsupervised-Representation-Learning"/>
   <updated>2019-04-02T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Meta-Learning Update Rules for Unsupervised Representation Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Standard unsupervised learning aims to learn transferable features. The paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.00222&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider training the model with supervised learning - &lt;em&gt;φ&lt;sub&gt;t+1&lt;/sub&gt; = SupervisedUpdate(φ&lt;sub&gt;t&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;, y&lt;sub&gt;t&lt;/sub&gt;, θ)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Here &lt;em&gt;t&lt;/em&gt; denotes the step, &lt;em&gt;(x, y)&lt;/em&gt; denotes the data points, &lt;em&gt;θ&lt;/em&gt; denotes the hyperparameters of the optimizer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extending this formulation for meta-learning, one could say that &lt;em&gt;t&lt;/em&gt; is the step of the inner loop, &lt;em&gt;θ&lt;/em&gt; are the parameters of the meta learning model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, the paper proposes to use &lt;em&gt;φ&lt;sub&gt;t+1&lt;/sub&gt; = UnsupervisedUpdate(φ&lt;sub&gt;t&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;, θ)&lt;/em&gt; ie &lt;em&gt;y&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; is not used (or even assumed to be available as this is unsupervised learning).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The meta update rule is used to learn the weights of a meta-model by performing SGD on the sum of &lt;em&gt;MetaObjective&lt;/em&gt; over the distribution of tasks (over the course of inner loop training).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Base model: MLP with parameters &lt;em&gt;φ&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To ensure that it generalizes across architectures, the update rule is designed to be neural-local ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each neuron &lt;em&gt;i&lt;/em&gt; in every layer &lt;em&gt;l&lt;/em&gt; (in the base model) has an update network (MLP) which takes as input the feedforward activations, feedback weights and error signals. ie &lt;em&gt;h&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i) = MLP(x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i), z&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i), v&lt;sup&gt;l+1&lt;/sup&gt;,
δ&lt;sup&gt;l&lt;/sup&gt;(i), θ)&lt;/em&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;b&lt;/em&gt; - index of the minibatch&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;x&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - pre non-linearity activations&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;z&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - post non-linearity activations&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;v&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - feedback weights&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;δ&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - error signal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the update networks share the meta parameters &lt;em&gt;θ&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is run in a standard feed-forward manner and the update network (corresponding to each unit) is used to generate the error signal &lt;em&gt;δ&lt;sup&gt;l&lt;/sup&gt;&lt;sub&gt;b&lt;/sub&gt;(i) = lin(h&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This loss is backpropogated using the set of learned backward weights &lt;em&gt;v&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; instead of the forward weights &lt;em&gt;w&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The weight update &lt;em&gt;Δw&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt; is also generated using a per-neuron update network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;meta-objective&quot;&gt;Meta Objective&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;MetaObjective&lt;/em&gt; is based on fitting a linear regression model to labeled examples with a small number of data points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;MetaObjective&lt;/em&gt; is to reduce the cosine distance between &lt;em&gt;y&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sup&gt;T&lt;/sup&gt;x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;L&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;y&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; - Actual lables on the evaluation batch&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;L&lt;/sup&gt;&lt;/em&gt; - Features of the evaluation batch (using the base model)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;v&lt;/em&gt; - parameters of the linear regression model (learned on train batch)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-considerations&quot;&gt;Practical Considerations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Meta gradients are approximated using truncated backdrop through time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing variation in the training dataset helps the meta optimization process. Data is augmented with shifts, rotations, and noise. Predicting these coefficients is an auxiliary (regression) task for training the meta-objective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training the system requires a lot of resources - 8 days with 512 workers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;With standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving. This suggests that the objective function for the two tasks starts to mismatch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; leads to a better generalization as compared to both VAE and supervised learning (followed by transfer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the meta-objective does not overfit).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; also generalizes to larger model architectures and different activation functions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GNN Explainer - A Tool for Post-hoc Explanation of Graph Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/GNN-Explainer-A-Tool-for-Post-hoc-Explanation-of-Graph-Neural-Networks"/>
   <updated>2019-03-26T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/GNN Explainer - A Tool for Post-hoc Explanation of Graph Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Neural Network (GNN) is a family of powerful machine learning (ML) models for graphs that can combine node information with the structural information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside of GNNs is that their predictions are hard to interpret.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes GNN Explainer model for solving the problem of interpretability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.03894&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desiderata-for-gnn-explanations&quot;&gt;Desiderata for GNN explanations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Local edge fidelity&lt;/strong&gt; - identify the subgraph structure (ideally the smallest) that significantly affected the predictions of the GNN. ie identify the important edges in the graph (for a given prediction).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Local node fidelity&lt;/strong&gt; - identify the import node features and correlations in the features of the neighboring nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Single instance and multi-instance explanations&lt;/strong&gt; - Support both single instance prediction tasks and multi-instance prediction tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt; - Support a large family of models (ideally all)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task Agnostic&lt;/strong&gt; - Support a large family of tasks (ideally all)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;I first describe the single instance prediction case and use that as the base to describe the multiple instance prediction cases. All the discussion in this section assumes a single instance prediction task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Trained GNN, a single instance whose prediction is to be explained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Identify the small subgraph and the small subset of features that explain the prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;: Maximize the mutual information (MI) between the GNN and the explanation by learning a &lt;em&gt;graph mask&lt;/em&gt; which can be used for selecting the relevant subgraph (from the GNN’s computational graph) and features (from all layers of the GNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Computational graph of GNN (corresponding to a node) refers to the approx L-hop neighborhood of the node in the graph ie the subgraph formed by nodes and edges whose representation affected the representation of the given node.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-instance-explanations&quot;&gt;Single-Instance Explanations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For a node &lt;em&gt;v&lt;/em&gt;, the information used to predict its label &lt;em&gt;y&lt;/em&gt; is completely described by its computation graph &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and the associated feature set &lt;em&gt;X&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;. The feature set includes the features of all the nodes in the computation graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When constructing the explaination, only &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and &lt;em&gt;X&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task can be reformulated as identifying a subgraph &lt;em&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; (subset of &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;) with associated features &lt;em&gt;X&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; which are important when predicting the label &lt;em&gt;y&lt;/em&gt; for node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Importance” is measured in terms of MI&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;MI(Y, (G&lt;sub&gt;S&lt;/sub&gt;, X&lt;sub&gt;S&lt;/sub&gt;)) = H(Y) - H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt; where &lt;em&gt;H&lt;/em&gt; is the entropy and &lt;em&gt;Y&lt;/em&gt; is a random variable representing the prediction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A further constraint, &lt;em&gt;| G&lt;sub&gt;S&lt;/sub&gt;| &amp;lt; k&lt;/em&gt; is imposed to obtain consise explaintations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since &lt;em&gt;H(Y)&lt;/em&gt; is fixed (recall that the network has already been trained and is now being used in the inference mode), maximizing MI is equivalent to minimizing the conditional entropy &lt;em&gt;H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is equivalent to selecting the subgraph that minimizes the uncertainty in the prediction of &lt;em&gt;y&lt;/em&gt; when the computational graph is &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optimiation-process&quot;&gt;Optimiation Process&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the exponentially large number of possible subgraphs, we can not directly optimize the given equation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “relaxed”-adjacency matrix (whose values are real numbers in the range 0 to 1) is introduced where each element of this fractional adjacency matrix is smaller than the corresponding element of the original adjacency matrix. Gradient descent can be performed on this adjacency matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “relaxed” &lt;em&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; can be interpreted as a variational approximation of the subgraph distributions of &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and the objective can be written as &lt;em&gt;min E&lt;sub&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/sub&gt;H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the paper makes a big approximation that the GNN is convex so as to leverage the Jensen inequality and push the expectation inside the entropy term to get an upper bound and then minimize that ie &lt;em&gt;min H(Y | G = E&lt;sub&gt;s&lt;/sub&gt;[G&lt;sub&gt;S&lt;/sub&gt;], X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that the convexity approximation (along with discreteness constraint) works in practice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, mean field approximation is used to decompose &lt;em&gt;P(G&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt; as a multivariate Bernoulli distrbitution ie product of &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;(i, j)&lt;/em&gt; for all &lt;em&gt;(i, j)&lt;/em&gt; belonging to &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;. &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; can be optimized directly and its values represent the expectation of the Bernoulli distrbitution on wether the edge &lt;em&gt;e&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; exists.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the constraints on &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt;, it is easier to learn a mask matrix &lt;em&gt;M&lt;/em&gt; and optimize that such that &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; = M * A&lt;sub&gt;c&lt;/sub&gt;* Additionally, the sigmod operator can be applied on &lt;em&gt;M&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once &lt;em&gt;M&lt;/em&gt; is learned, only the top &lt;em&gt;k&lt;/em&gt; values are retained.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;including-node-features-in-the-explanation&quot;&gt;Including Node Features in the Explanation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Similar to the previous approach, another feature mask is learned (either one for entire GNN or one per node of the GNN) and is used as a feature selector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The mask could either be learned such that same set of node features (in terms of dimensions) are selected or a different set of features are selected per node. The paper uses the former as it is more straightforward.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Just like before, a “relaxed” mask &lt;em&gt;M&lt;sub&gt;T&lt;/sub&gt;&lt;/em&gt; is trained to select features as &lt;em&gt;M&lt;sub&gt;T&lt;/sub&gt; * X&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One tricky case is where one feature is important but its value is set to 0. In the case, the value will be masked even though it should not be&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The workaround is to use Monte Carlo (MC) estimates of marginals of the missing features. This gives a way to assign importance scores to each feature dimension and a form of reparameterization trick is used to perform end-to-end learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Masks are encouraged to be discrete by regularizing their element-wise entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Resulting computation graph is valid as in it allows message passing towards the central node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-instance-explanations&quot;&gt;Multi-Instance Explanations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a set of nodes (having the label say &lt;em&gt;y&lt;/em&gt;),  the task is to obtain a global explanation of the predictions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the given class, a prototypical reference node is chosen by computing the mean of embeddings of all the nodes in the class and then selecting the node which is closest to the mean.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, compute the important computational graph corresponding to this node and align the computational subgraphs of all the other nodes (in the given class) to reference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let &lt;em&gt;A*&lt;/em&gt; be the adjacency matrix and &lt;em&gt;X*&lt;/em&gt; be the feature matrix for the explanation corresponding to the reference node. Let &lt;em&gt;A&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;X&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; be the adjacency matrix and feature matrix of the to-ber-aligned computational graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A relaed alignment matrix &lt;em&gt;P&lt;/em&gt; is optimized to align the nodes and features in the two graphs ie we minimize &lt;em&gt;|P&lt;sup&gt;T&lt;/sup&gt;A&lt;sub&gt;v&lt;/sub&gt;P - A*| + *|P&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;v&lt;/sub&gt;P - X*|&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choosing concise explanations helps in efficient graph matching.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For GNNs that compute attention over the entire graph, edges with low attention weight can be pruned to increase efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Node classification: BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Graph classification: MUTAG, Reddit-Binary&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;GRAD - Compute the gradient of the model loss with respect to the adjacency matrix and the node features to be classified and fix the edges with the highest absolute gradient.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;GAT - Graph Attention Network&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed model seems to outperform the baselines both qualitatively and quantitatively. But the results should be taken with a grain of salt as only 2 baselines are considered.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/To-Tune-or-Not-to-Tune-Adapting-Pretrained-Representations-to-Diverse-Tasks"/>
   <updated>2019-03-16T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.05987&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides useful empirical advice for adapting pretrained language models for a given target task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pre-trained models considered&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ELMo&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;BERT&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tasks considered&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Named Entity Recognition (NER) - CoNLL 2003 dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Natural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Paraphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Semantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The last 3 tasks (NLI, PD, STS) are defined for sentence pairs.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adaptation Strategies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Feature Extraction&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The pretrained model is only used for extracting features and its weights are kept fixed.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For both ELMo and BERT, the contextual representation of the words from all the layers are extracted.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;A weighted combination of these layers is used as an input to the task-specific model.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - BiLSTM with CRF layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA - bi-attentive classification network&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;NLI, PD, STS - &lt;a href=&quot;https://arxiv.org/abs/1609.06038&quot;&gt;Enhanced Sequential Inference Model (ESIM)&lt;/a&gt;&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Fine-tuning&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The pretrained model is finetuned on the target task.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models for ELMO&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - CRF on top of LSTM states&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA - Max-pool over the language model states followed by a softmax layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;NLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models for BERT&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - Extract representation of the first-word piece of each token followed by the softmax layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA, NLI, PD, STS - standard BERT training&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main observations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Feature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS). The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Adding extra parameters is helpful for feature extraction but not fine-tuning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Training a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Knowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Model Primitive Hierarchical Lifelong Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Model-Primitive-Hierarchical-Lifelong-Reinforcement-Learning"/>
   <updated>2019-03-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Model Primitive Hierarchical Lifelong Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.01567&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of learning a single transition model of the environment (aka &lt;em&gt;world model&lt;/em&gt;) that can model the transitions very well, it is sufficient to learn several (say &lt;em&gt;k&lt;/em&gt;) suboptimal models (aka &lt;em&gt;model primitives&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each &lt;em&gt;model primitive&lt;/em&gt; will be good in only a small part of the state space (aka &lt;em&gt;region of specialization&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These &lt;em&gt;model primitives&lt;/em&gt; can then be used to train a gating mechanism for selecting sub-policies to solve a given task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since these &lt;em&gt;model primitives&lt;/em&gt; are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;single-task-learning&quot;&gt;Single Task Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A gating controller is trained to choose the sub-policy whose &lt;em&gt;model primitive&lt;/em&gt; makes the best prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This requires modeling &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt; | s&lt;sub&gt;t&lt;/sub&gt;, a&lt;sub&gt;t&lt;/sub&gt;, s&lt;sub&gt;t+1&lt;/sub&gt;)&lt;/em&gt; where &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt;)&lt;/em&gt; denotes the probability of selecting the &lt;em&gt;k&lt;sup&gt;th&lt;/sup&gt; model primitive&lt;/em&gt;. This is hard to compute as the system does not have access to &lt;em&gt;s&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt;  and &lt;em&gt;a&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; before it has choosen the sub-policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Properly marginalizing &lt;em&gt;s&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;a&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt; | s&lt;sub&gt;t&lt;/sub&gt;)&lt;/em&gt;. This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper notes that technique is biased but reports that it still works for the downstream tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The gating controller composes the sub-policies as a mixture of Gaussians.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For learning, PPO algorithm is used with each &lt;em&gt;model primitives&lt;/em&gt; gradient weighted by the probability from the gating controller.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lifelong-learning&quot;&gt;Lifelong Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Domains:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Mujoco ant navigating different mazes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Stacker arm picking up and placing different boxes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation Details:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Gaussian subpolicies&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;PPO as the baseline&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Model primitives are hand-crafted using the true next state provided by the environment simulator.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Single Task&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Only maze task is considered with the start position (of the ant) and the goal position is fixed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Observation includes distance from the goal.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lifelong Learning&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Maze&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;10 random Mujoco ant mazes used as the task distribution.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pick and Place&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;8 Pick and Place tasks are created with max 3 goal locations.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Observation includes the position of the goal.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Overlapping &lt;em&gt;model primitives&lt;/em&gt; can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The order of tasks (in the 10-Mazz task) does not degrage the performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Transfering the gating controller leads to negative transfer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Notes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>TuckER - Tensor Factorization for Knowledge Graph Completion</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/TuckER-Tensor-Factorization-for-Knowledge-Graph-Completion"/>
   <updated>2019-02-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/TuckER-Tensor Factorization for Knowledge Graph Completion</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.09590&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ibalazevic/TuckER&quot;&gt;Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;knowledge-graph-as-a-tensor&quot;&gt;Knowledge Graph as a Tensor&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (e&lt;sub&gt;s&lt;/sub&gt;, r, e&lt;sub&gt;o&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element’s value corresponds to ether that element is present in the KG or not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple. The score indicates whether the triple is actually present in the KG or not.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tucker-decomposition&quot;&gt;TuckER Decomposition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor &lt;strong&gt;X&lt;/strong&gt; (of shape &lt;em&gt;IxJxK&lt;/em&gt;) can be factorized into a core tensor &lt;strong&gt;W&lt;/strong&gt; (of shape &lt;em&gt;PxQxR&lt;/em&gt;) and 3 factor matrics - &lt;strong&gt;A&lt;/strong&gt; (of shape &lt;em&gt;IxP&lt;/em&gt;), &lt;strong&gt;B&lt;/strong&gt; (of shape &lt;em&gt;JxQ&lt;/em&gt;) and &lt;strong&gt;C&lt;/strong&gt; (of shape &lt;em&gt;KxR&lt;/em&gt;) such that &lt;strong&gt;X&lt;/strong&gt; is approximately &lt;strong&gt;W&lt;/strong&gt; x&lt;sub&gt;1&lt;/sub&gt; &lt;strong&gt;A&lt;/strong&gt; x&lt;sub&gt;2&lt;/sub&gt; &lt;strong&gt;B&lt;/strong&gt; x&lt;sub&gt;3&lt;/sub&gt; &lt;strong&gt;C&lt;/strong&gt;, where X&lt;sub&gt;n&lt;/sub&gt; denotes the tensor product along the nth mode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, &lt;em&gt;P, Q, R&lt;/em&gt; are smaller than &lt;em&gt;I, J K&lt;/em&gt; (respectively) and &lt;strong&gt;W&lt;/strong&gt; can be seen as a compressed version of &lt;strong&gt;X&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tucker-decomposition-for-link-prediction&quot;&gt;TuckER Decomposition for Link Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two embedding matrics are used for embedding the entities and the relations respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entity embedding matrix &lt;strong&gt;E&lt;/strong&gt; is shared for both subject and the object ie &lt;strong&gt;E&lt;/strong&gt; = &lt;strong&gt;A&lt;/strong&gt; = &lt;strong&gt;B&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The scoring function is gives as &lt;strong&gt;W&lt;/strong&gt; x&lt;sub&gt;1&lt;/sub&gt; &lt;strong&gt;e&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt; x&lt;sub&gt;2&lt;/sub&gt; &lt;strong&gt;w&lt;sub&gt;r&lt;/sub&gt;&lt;/strong&gt; x&lt;sub&gt;3&lt;/sub&gt; &lt;strong&gt;e&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt; where &lt;strong&gt;e&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt;, &lt;strong&gt;w&lt;sub&gt;r&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;e&lt;sub&gt;o&lt;/sub&gt;&lt;/strong&gt; are the embedding vectors corresonding to e&lt;sub&gt;s&lt;/sub&gt;, e&lt;sub&gt;r&lt;/sub&gt; and e&lt;sub&gt;o&lt;/sub&gt; respectively. Note that both the core tensor and the factor matrices are to be learnt.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To speed up training and increase accuracy, 1-N scoring is used. A given (e&lt;sub&gt;s&lt;/sub&gt;, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-analysis&quot;&gt;Theoretical Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For full expressiveness, dimensionality of entity (relation) is n&lt;sub&gt;E&lt;/sub&gt; (n&lt;sub&gt;R&lt;/sub&gt;) where n&lt;sub&gt;E&lt;/sub&gt; (n&lt;sub&gt;R&lt;/sub&gt;) are the number of entities (relations). In comparsion, the required dimensionality for ComplEx is n&lt;sub&gt;E&lt;/sub&gt; * n&lt;sub&gt;R&lt;/sub&gt; (for both entity and relations) and for SimplE, it is min(&lt;sub&gt;E&lt;/sub&gt; * n&lt;sub&gt;R&lt;/sub&gt;, number of facts + 1) (for both entity and relations).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;FB15k, FB15k-237, WN18, WN18RR&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The max number of entities is around 41K and max number of relations is around 1.3K&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;BatchNorm, Dropout and Learning rate decay are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall n&lt;sub&gt;e&lt;/sub&gt; generated triples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the n&lt;sub&gt;e&lt;/sub&gt; generated triples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Higher is better for both the metrics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER outperforms all the baseline models on all but one task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER improves performance more significantly when the number of relations is large.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even with lower embedding dimensions, TuckER’s performance does not deteriorate as much as other models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Linguistic Knowledge as Memory for Recurrent Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Linguistic-Knowledge-as-Memory-for-Recurrent-Neural-Networks"/>
   <updated>2019-02-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Linguistic Knowledge as Memory for Recurrent Neural Networks</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.02620&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture. Its GRU version is referred to as MAGE-GRU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input sequence of tokens [x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, …, x&lt;sub&gt;T&lt;/sub&gt;] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the graph &lt;em&gt;G&lt;/em&gt;, two DFS orderings are computed - forward DFS and backward DFS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAGE-RNN uses separate networks for accessing the forward and backward DFS orders.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A separate hidden state is maintained for each entity type to separate memory content from addressing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For any DFS order (forward or backward), the representation at time &lt;em&gt;t&lt;/em&gt; is given as the concatenation of representation of different edge types at that time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at x&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The representation of the DFS order is given as the sequence of all the previous representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In some cases, elements across multiple sequences could be related to each other. In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all the cases, the model with MAGE-GRU works the best.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Diversity is All You Need - Learning Skills without a Reward Function</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Diversity-is-All-You-Need-Learning-Skills-without-a-Reward-Function"/>
   <updated>2019-01-29T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Diversity is All You Need - Learning Skills without a Reward Function</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.06070&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ben-eysenbach/sac&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Unsupervised “exploration” stage followed by supervised stage.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desirable-qualities-of-skills&quot;&gt;Desirable Qualities of Skills&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Skills should dictate the states that the agent visits. Different skills should visit different states to be distinguishable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skills are encouraged to be diverse and “exploratory” by learning skills that act randomly (have high entropy).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss-formulation&quot;&gt;Loss Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(S, A) - state and action&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;z ~ p(z) - latent variable to condition the policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skill - policy conditioned on a fixed z.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximize the entropy of the mixture of policies (p(z) and all the skills).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Policy π(a | s, z)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task reward replaced by the pseduoreward logq&lt;sub&gt;φ&lt;/sub&gt;(z | s) - log(p(z)).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During unsupervised training, z is sampled at the start of the episode and then not changed during the episode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;h3 id=&quot;analysis-of-learned-skills&quot;&gt;Analysis of Learned Skills&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for inverted pendulum and mountain car, the skills become increasingly diverse throughout training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies. This provides a mechanism for incorporating “supervision” in the learning setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “discovered” primitives can also be used for imitation learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Modular meta-learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Modular-meta-learning"/>
   <updated>2019-01-22T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Modular meta-learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed model is called as BOUNCEGRAD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.10166&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/FerranAlet/modular-metalearning&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Focuses on supervised learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task distribution &lt;em&gt;p(T)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each task is a joint distribution &lt;em&gt;p&lt;sub&gt;T&lt;/sub&gt;(x, y)&lt;/em&gt; over &lt;em&gt;(x, y)&lt;/em&gt; data pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given data from &lt;em&gt;m&lt;/em&gt; meta-training tasks, and a meta-test task, find a hypothesis &lt;em&gt;h&lt;/em&gt; which performs well on the unseen data drawn from the meta-test task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;structured-hypothesis&quot;&gt;Structured Hypothesis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a compositional scheme &lt;em&gt;C&lt;/em&gt;, a set of modules &lt;em&gt;F&lt;sub&gt;1&lt;/sub&gt;, …, F&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; (represented as a whole by &lt;em&gt;F&lt;/em&gt;) and the set of their respective parameters θ&lt;sub&gt;1&lt;/sub&gt;, …, θ&lt;sub&gt;k&lt;/sub&gt; (represented as a whole by θ), &lt;em&gt;(C, F, θ)&lt;/em&gt; represents the set of possible functional input-output mappings. These mappings form the hypothesis space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A structured hypothesis model is specified by what modules to use and their parametric forms (but not the values).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;examples-of-compositional-schemes&quot;&gt;Examples of compositional schemes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Choosing a single module for the task at hand.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fixed compositional structure but different modules selected every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Weight ensemble (maybe using attention mechanism)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General function composition tree&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phases&quot;&gt;Phases&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Offline Meta Learning Phase:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Take training and validation dataset for the first &lt;em&gt;k&lt;/em&gt; tasks and generate a parameterization for each module &lt;em&gt;θ&lt;sub&gt;1&lt;/sub&gt;, …, θ&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The hypothesis (or composition) to use comes from the online meta-test learning phase.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this stage, find the best θ given a structure.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Online Meta-test Learning Phase&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given a hypothesis space and θ, the output is a compositional form (or hypothesis) that specifies how to compose the models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this stage, find the best structure, given a hypothesis space and θ.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-algorithm&quot;&gt;Learning Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature &lt;em&gt;T&lt;/em&gt; decreased over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once a structure has been picked, any gradient descent based approach can be used to optimize the modules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically the state of optimization process comprises of the parameters and the temperature. Together, they are used to induce a distribution over the structures. Given a structure, θ is optimized and &lt;em&gt;T&lt;/em&gt; is annealed over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well. the resulting approach is referred to as MOMA - MOdular MAml.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;approaches&quot;&gt;Approaches&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pooled - Single network using combined data of all the tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAML - Single network using MAML&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BOUNCEGRAD - Modular Network without MAML adaptation in online learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MOMA - BOUNCEGRAD with MAML adaptation in online learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;domains&quot;&gt;Domains&lt;/h3&gt;

&lt;h4 id=&quot;simple-functional-relationships&quot;&gt;Simple Functional Relationships&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sine-function prediction problem&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, MOMA outperforms other models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;predicting-next-frame-of-a-kinematic-skeleton-motion-capture-data&quot;&gt;Predicting next frame of a kinematic skeleton (motion capture data)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;11 different objects (with different shapes) on 4 surfaces with different friction properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2 meta-learning scenarios are considered. In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For unseen combinations, all the 3 are equally good.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compositional scheme is the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;predicting-next-frame-of-a-kinematic-selection-using-motion-capture-data&quot;&gt;Predicting next frame of a kinematic selection (using motion capture data)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Composition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Again 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hierarchical-RL-Using-an-Ensemble-of-Proprioceptive-Periodic-Policies"/>
   <updated>2019-01-15T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=SJz1x20cFQ&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The state can be divided into two components: proprioceptive states s&lt;sup&gt;p&lt;/sup&gt; (measurement of agent’s own body that can be directly controlled by the agent) and the external states s&lt;sup&gt;e&lt;/sup&gt;/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;low-level-policy-training&quot;&gt;Low-Level Policy Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low-level policies should be:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Diverse: should cover all the skills that the agent might have to perform.&lt;/li&gt;
      &lt;li&gt;Effective: can make significant changes to the environment.&lt;/li&gt;
      &lt;li&gt;Controllable: easy for high-level policies to use and control&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the low-level policy, the per-time step reward is directly proportional to change in the external state. The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-conditioned-policies&quot;&gt;Phase conditioned policies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Good movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Phase conditioned policy (=f(s&lt;sup&gt;p&lt;/sup&gt;, φ)) where φ = {0, 1, …, k-1} is the phase index.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At each timestep &lt;em&gt;t&lt;/em&gt;, the model receives observation s&lt;sup&gt;p&lt;/sup&gt; and phase index φ = t%k. The phase index is represented by a vector b&lt;sub&gt;φ&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: Ant and Humanoid from Mujoco.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low-level control:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Using phase-conditioning is helpful when training low-level primitives.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;High-level control:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Cross Maze Environment with fixed goals&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;3 goals along 3 paths&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Proposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Random Goal Maze&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The goal is randomly drawn from a set of goals.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;“Cross” (shaped) maze and “skull” (shaped) mazes are considered.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Even with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Efficient Lifelong Learning with A-GEM</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Efficient-Lifelong-Learning-with-A-GEM"/>
   <updated>2019-01-08T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Efficient Lifelong Learning with A-GEM</id>
   <content type="html">&lt;h2 id=&quot;contributions&quot;&gt;Contributions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modification of &lt;a href=&quot;https://arxiv.org/abs/1706.08840&quot;&gt;Gradient Episodic Memory ie GEM&lt;/a&gt; which reduces the computational overhead of GEM without compromising on the results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.00420&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/agem/&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-protocol&quot;&gt;Learning Protocol&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two group of datasets - one for training and evaluation (D&lt;sup&gt;EV&lt;/sup&gt;) and other for cross validation (D&lt;sup&gt;CV&lt;/sup&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data can be sampled multiple times for cross-validation dataset but only once from the training dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each group of dataset (say D&lt;sup&gt;EV&lt;/sup&gt; or D&lt;sup&gt;CV&lt;/sup&gt;) is a list of task-specific datasets D&lt;sub&gt;k&lt;/sub&gt; (k is the task index).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each sample in D&lt;sub&gt;k&lt;/sub&gt; is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D&lt;sub&gt;k&lt;/sub&gt; contains B&lt;sup&gt;k&lt;/sup&gt; minibatches of data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metrics&quot;&gt;Metrics&lt;/h2&gt;

&lt;h3 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a&lt;sub&gt;k,i,j&lt;/sub&gt; = accuracy on test task j after training on ith minibatch of training task k.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A&lt;sub&gt;k&lt;/sub&gt; = mean over all j = 1 to k (a&lt;sub&gt;k, B&lt;sub&gt;k&lt;/sub&gt;, j&lt;/sub&gt;) ie train the model on data for task k and then test it on all the tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forgetting-measure&quot;&gt;Forgetting Measure&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt; = forgetting on task j after training on all minibatches upto task k.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt; = max over all l = 1 to k-1 (a&lt;sub&gt;l, B&lt;sub&gt;l&lt;/sub&gt;j&lt;/sub&gt; - a&lt;sub&gt;k, B&lt;sub&gt;k&lt;/sub&gt;j&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forgetting = F&lt;sub&gt;k&lt;/sub&gt; = mean over all j = 1 to k-1 (f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt;)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lca---learning-curve-area&quot;&gt;LCA - Learning Curve Area&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Z&lt;sub&gt;b&lt;/sub&gt; = average b shot performance where b is the minibatch number.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z&lt;sub&gt;b&lt;/sub&gt; = mean over all k = 0 to T (a&lt;sub&gt;k, b, k&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LCA&lt;sub&gt;β&lt;/sub&gt; = mean over all b = 0 to β (Z&lt;sub&gt;b&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One special case is LCA&lt;sub&gt;0&lt;/sub&gt; which is the forward transfer performance or performance on the unseen task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In experiments, β is kept small as we want the model to learn from few examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;joint-embedding-model-using-compositional-task-descriptors&quot;&gt;Joint Embedding Model Using Compositional Task Descriptors&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compositional Task Descriptors are used to speed training on the subsequent tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A matrix specifying the attribute value of objects (to be recognized in the task) are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A joint-embedding space between image features and attribute embeddings is learned.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00796&quot;&gt;Permuted MNIST&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04200&quot;&gt;Split CIFAR&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.vision.caltech.edu/visipedia/CUB-200-2011.html&quot;&gt;Split CUB&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cvml.ist.ac.at/papers/lampert-cvpr2009.pdf&quot;&gt;Split AWA&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines include &lt;a href=&quot;https://arxiv.org/abs/1706.08840&quot;&gt;GEM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1611.07725&quot;&gt;iCaRL&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1612.00796.pdf&quot;&gt;Elastic Weight Consolidation&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1606.04671&quot;&gt;Progressive Neural Networks&lt;/a&gt; etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead. One reason could be that MNIST has a large number of training examples per task. But Progressive Neural Networks lead to bad utilization of capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of task descriptors improves the accuracy for all the models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Pre-training Graph Neural Networks with Kernels</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Pre-training-Graph-Neural-Networks-with-Kernels"/>
   <updated>2019-01-02T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Pre-training Graph Neural Networks with Kernels</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a pretraining technique that can be used with the &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry&quot;&gt;GNN&lt;/a&gt; architecture for learning graph representation as induced by powerful graph kernels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.06930&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a dataset of graphs &lt;em&gt;g&lt;sub&gt;1&lt;/sub&gt;, g&lt;sub&gt;2&lt;/sub&gt;, …, g&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;, use a relevant kernel function to compute &lt;em&gt;k(g&lt;sub&gt;i&lt;/sub&gt;, g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; for all pairs of graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A siamese network is used to encode the pair of graphs into representations &lt;em&gt;f(g&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; and &lt;em&gt;f(g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; such that &lt;em&gt;dot(f(g&lt;sub&gt;i&lt;/sub&gt;), f(g&lt;sub&gt;j&lt;/sub&gt;))&lt;/em&gt; equals &lt;em&gt;k(g&lt;sub&gt;i&lt;/sub&gt;, g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The function &lt;em&gt;f&lt;/em&gt; is trained to learn the compressed representation of kernel’s feature space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf&quot;&gt;DGCNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graphlet Kernel (GK)&lt;/li&gt;
  &lt;li&gt;Random Walk Kernel&lt;/li&gt;
  &lt;li&gt;Propogation Kernel&lt;/li&gt;
  &lt;li&gt;Weisfeiler-Lehman subtree kernel (WL)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pretraining uses the WL kernel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The idea is straightforward and intuitive. In general, this kind of pretraining should help the downstream model. It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Smooth Loss Functions for Deep Top-k Classification</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Smooth-Loss-Functions-for-Deep-Top-k-Classification"/>
   <updated>2018-12-25T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Smooth Loss Functions for Deep Top-k Classification</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.07595&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/oval-group/smooth-topk&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Inspired by the multi-loss SVMs, a surrogate loss (l&lt;sub&gt;k&lt;/sub&gt;) is introduced that creates a margin between the ground truth and the kth largest score.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq1.png&quot; alt=&quot;Equation 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Here &lt;strong&gt;s&lt;/strong&gt; denotes the output of the classifier model to be learnt, &lt;em&gt;y&lt;/em&gt; is the ground truth label, &lt;em&gt;s[p]&lt;/em&gt; denotes the kth largest element of &lt;strong&gt;s&lt;/strong&gt; and &lt;strong&gt;s\p&lt;/strong&gt; denotes the vector &lt;strong&gt;s&lt;/strong&gt; without &lt;em&gt;p&lt;/em&gt;th element.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This l&lt;sub&gt;k&lt;/sub&gt; loss has two limitations:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It is continous but not differentiable in &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Its weak derivatives have at most 2-nonzero elements.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The loss can be reformulated by adding and subtracting the k-1 largest scores of &lt;strong&gt;s\y&lt;/strong&gt; and &lt;em&gt;s&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt; and by introducing a temperature parameter τ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq2.png&quot; alt=&quot;Equation 2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;properties-of-lkτ&quot;&gt;Properties of L&lt;sub&gt;kτ&lt;/sub&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For any τ &amp;gt; 0, L&lt;sub&gt;kτ&lt;/sub&gt; is infinite-differentiable and has non-sparse gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Under mild conditions, L&lt;sub&gt;kτ&lt;/sub&gt; apporachs l&lt;sub&gt;k&lt;/sub&gt; (in a pointwise sense) as τ approaches to 0+&lt;sup&gt;+&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is an upper bound on the actual loss (up to a constant factor).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is a generalization of the cross-entropy loss for different values of k, and τ and higher margins.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;computational-challenges&quot;&gt;Computational Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;nCk&lt;/em&gt; number of terms needs to be evaluated for computing the loss for one sample (n is number of classes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss L&lt;sub&gt;kτ&lt;/sub&gt; can be expressed in terms of elementary symmetric polynomials σ&lt;sub&gt;i&lt;/sub&gt;(&lt;strong&gt;e&lt;/strong&gt;) (sum of all products of i distinct elements of vector e). Thus the challenge is to compute σ&lt;sub&gt;k&lt;/sub&gt; efficiently.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forward-computation&quot;&gt;Forward Computation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute σ&lt;sub&gt;k&lt;/sub&gt;(&lt;strong&gt;e&lt;/strong&gt;) where &lt;strong&gt;e&lt;/strong&gt; is a n-dimensional vector and k« n and e[i]!=0 for all i.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;σ&lt;sub&gt;i&lt;/sub&gt;(&lt;em&gt;e&lt;/em&gt;) can be computed using the coefficients of the polynomial (X+e&lt;sub&gt;1&lt;/sub&gt;)(X+e&lt;sub&gt;2&lt;/sub&gt;)…(X+e&lt;sub&gt;n&lt;/sub&gt;) by divide and conquer approach with polynomial multiplication.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backward-computation&quot;&gt;Backward computation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The backward pass uses optimizations like computing derivative of σ&lt;sub&gt;j&lt;/sub&gt; with respect to e&lt;sub&gt;i&lt;/sub&gt; in a recursive manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Appendix of the paper describes these techniques in detail.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed on CIFAR-100 (with noise) and Imagenet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hindsight Experience Replay</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hindsight-Experience-Replay"/>
   <updated>2018-12-18T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hindsight Experience Replay</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.01495&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Assume a footballer misses the goal narrowly. Even though the player does not get any “reward”(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The same intuition is applied for the RL agent - let us say that the true goal state was &lt;em&gt;g&lt;/em&gt; while the agent ends up in the state &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the action sequence is not useful for reaching the goal state &lt;em&gt;g&lt;/em&gt;, it is indeed useful for reaching state &lt;em&gt;s&lt;/em&gt;. Hence the trajectory could be replayed with the goal as &lt;em&gt;s&lt;/em&gt;(and not &lt;em&gt;g&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;technical-details&quot;&gt;Technical Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-goal policy trained using Universal Value Function Approximation (UVFA).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every episode starts by sampling a start state and a goal state. Each goal has a different reward function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy uses both the current state and the current goal state and leads to a state transition sequence &lt;em&gt;s&lt;sub&gt;1&lt;/sub&gt;, s&lt;sub&gt;2&lt;/sub&gt;,…, s&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each of these transitions &lt;em&gt;s&lt;sub&gt;i&lt;/sub&gt; -&amp;gt; s&lt;sub&gt;i+1&lt;/sub&gt;&lt;/em&gt; are stored in a buffer with both the original goal and a subset of the other goals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the goal selection, following strategies are tried:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Future&lt;/em&gt; - goal state is the state &lt;em&gt;k&lt;/em&gt; steps after observing the state transition.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Final&lt;/em&gt; - goal state is the final state of the current episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Episode&lt;/em&gt; - &lt;em&gt;k&lt;/em&gt; random states are selected from the current episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Randon&lt;/em&gt; - &lt;em&gt;k&lt;/em&gt; states are selected randomly.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Any off-policy algorithm can be used. Specifically, DDPG is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Robotic arm simulated using MuJoCo for &lt;em&gt;push&lt;/em&gt;, &lt;em&gt;slide&lt;/em&gt; and &lt;em&gt;pick and place&lt;/em&gt; tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DDPG with and without HER evaluated on the 3 tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DDPG with the HER variant significantly outperforms the baseline in all the cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Representation Tradeoffs for Hyperbolic Embeddings</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Representation-Tradeoffs-for-Hyperbolic-Embeddings"/>
   <updated>2018-12-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Representation Tradeoffs for Hyperbolic Embeddings</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.03329&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic Spaces&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Have the “tree” like property ie the shortest path between a pair of points is almost the same as the path through the origin.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fidelity Measures&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Mean Average Precision - MAP&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;A local metric that ranks between distances of the immediate neighbors.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Distortion&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;A global metric that depends on the underlying distances and not just the local relationship between distances.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combinatorial-construction-for-embedding-hierarchies-into-hyperbolic-spaces&quot;&gt;Combinatorial Construction for embedding hierarchies into Hyperbolic spaces&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Embed the given graph &lt;em&gt;G = (V, E)&lt;/em&gt; into a tree &lt;em&gt;T&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Embed the tree &lt;em&gt;T&lt;/em&gt; into the poincare ball &lt;em&gt;H&lt;sub&gt;d&lt;/sub&gt;&lt;/em&gt; of dimensionality &lt;em&gt;d&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sarkars-construction-to-embed-points-in-a-2-d-poincare-ball&quot;&gt;Sarkar’s construction to embed points in a 2-d Poincare ball&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider two points &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; (from the tree) where &lt;em&gt;b&lt;/em&gt; is the parent of &lt;em&gt;a&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assume that &lt;em&gt;a&lt;/em&gt; is embedded as &lt;em&gt;f(a)&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; is embedded as &lt;em&gt;f(b)&lt;/em&gt; and the children of &lt;em&gt;a&lt;/em&gt; needs to be embedded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reflect &lt;em&gt;f(a)&lt;/em&gt; and &lt;em&gt;f(b)&lt;/em&gt; across a geodesic such that &lt;em&gt;f(a)&lt;/em&gt; is mapped to 0 (origin) while &lt;em&gt;f(b)&lt;/em&gt; is mapped to some new point &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Children of &lt;em&gt;a&lt;/em&gt; are placed at points &lt;em&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; which are equally placed around a circle of radius &lt;em&gt;(e&lt;sup&gt;r&lt;/sup&gt; - 1) / (e&lt;sup&gt;r&lt;/sup&gt; + 1)&lt;/em&gt; and maximally seperated from &lt;em&gt;z&lt;/em&gt;, where &lt;em&gt;r&lt;/em&gt; is the scaling factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then all the points are reflected back across the geodesic so that all children are at a distance &lt;em&gt;r&lt;/em&gt; from &lt;em&gt;f(a)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;d-dimensional-hyperbolic-spaces&quot;&gt;&lt;em&gt;d&lt;/em&gt;-dimensional hyperbolic spaces&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the &lt;em&gt;d&lt;/em&gt;-dimensional space, the points are embedded into hyperspheres (instead of circles).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The number of children node that can be placed for a particular angle grows with the dimension.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing dimension helps with bushy trees (with high node degree).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperbolic-multidimensional-scaling-h-mds&quot;&gt;Hyperbolic multidimensional scaling (h-MDS)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the pairwise distance from a set of points in the hyperbolic space, how to recover the points?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The corresponding problem in the Euclidean space is solved using MDS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A variant of MDS called as h-MDS is proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MDS makes a centering assumption that points have 0 mean. In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pseudo-euclidean-mean&quot;&gt;pseudo-Euclidean Mean&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recovery-via-matrix-factorization&quot;&gt;Recovery via matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the pairwise distances, a new matrix &lt;em&gt;Y&lt;/em&gt; is constructed by applying &lt;em&gt;cosh&lt;/em&gt; on the pairwise distances.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Running PCA on &lt;em&gt;-Y&lt;/em&gt; recovers X up to rotation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dimensionality-reduction-with-pga-principal-geodesic-analysis&quot;&gt;Dimensionality Reduction with PGA (Principal Geodesic Analysis)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PGA is the counterpart of PCA in the hyperbolic spaces.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First the &lt;em&gt;Karcher&lt;/em&gt; mean of the given points is computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All points &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; are reflected so that their mean is 0 in the Poincare disk model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Combining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Trees: fully balanced and phylogenic trees expressing genetic heritage.&lt;/li&gt;
      &lt;li&gt;Tree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships.&lt;/li&gt;
      &lt;li&gt;No-tree like disease relationships, proteins interactions etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Combinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion.&lt;/li&gt;
      &lt;li&gt;eg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Learned Optimizers that Scale and Generalize</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learned-Optimizers-that-Scale-and-Generalize"/>
   <updated>2018-11-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learned Optimizers that Scale and Generalize</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04813&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-advantage&quot;&gt;Key Advantage&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed. The learnt optimizer generalizes to much more complex tasks and setups.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A hierarchical RNN is designed to act as a learned optimizer. This RNN is the meta-learner and its parameters are shared across different tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the lowest level of hierarchical, a small “parameter RNN” ingests the gradient (and related metadata).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One level up, an intermediate “Tensor RNN” incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, the hidden states are fixed at 10, 30 and 20 respectively.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;features-inspired-from-existing-optimizers&quot;&gt;Features inspired from existing optimizers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Attention and Nesterov’s momentum&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Attention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;To incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The average gradients are rescaled (as in RMSProp and Adam)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Relative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>One-shot Learning with Memory-Augmented Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/One-shot-Learning-with-Memory-Augmented-Neural-Networks"/>
   <updated>2018-10-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/One-shot Learning with Memory-Augmented Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Here, MANN refers to neural networks that have an external memory. This includes Neural Turning Machines (NTMs) and excludes LSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06065&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;meta-learning&quot;&gt;Meta-Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In meta-learning, a learner is learning at two levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learner is shown a sequence of tasks D&lt;sub&gt;1&lt;/sub&gt;, D&lt;sub&gt;2&lt;/sub&gt;, …, D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When it is training on one of the datasets (say D&lt;sub&gt;T&lt;/sub&gt;), it learns to solve the current dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mann--meta-learning&quot;&gt;MANN + Meta Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Following are the desirable characteristics for a scalable, combined architecture:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Memory representation should be both stable and element-wise accessible.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Number of model parameters should not be tied to the size of the memory.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task-setup&quot;&gt;Task Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each dataset is presented to the model in the form (x&lt;sub&gt;1&lt;/sub&gt;, null), (x&lt;sub&gt;1&lt;/sub&gt;, y&lt;sub&gt;0&lt;/sub&gt;), …, (x&lt;sub&gt;t+1&lt;/sub&gt;, y&lt;sub&gt;t&lt;/sub&gt;) where y&lt;sub&gt;t&lt;/sub&gt; is the correct label (or value) corresponding to the inpuit x&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, the data labels are shuffled from dataset to dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses NTM as the MANN with one modification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the original formulation, the memories were addressed by both context and location. Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new access module - LRUA - Least Recent Used Access - is used to write to memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information). This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Omniglot (classification)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sampled functions from Gaussian Processes&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As baselines, following models were considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Regular NTM&lt;/li&gt;
      &lt;li&gt;LSTM&lt;/li&gt;
      &lt;li&gt;Feedforward RNN&lt;/li&gt;
      &lt;li&gt;Nearest Neighbour Classifier&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>BabyAI - First Steps Towards Grounded Language Learning With a Human In the Loop</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/BabyAI-First-Steps-Towards-Grounded-Language-Learning-With-a-Human-In-the-Loop"/>
   <updated>2018-10-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/BabyAI-First Steps Towards Grounded Language Learning With a Human In the Loop</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.08272&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Curriculum learning is incorporated by having a curriculum of levels of increasing difficulty.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interactive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside of human-in-the-loop is the poor sample complexity of the learning agent. The heuristic agent can be used to estimate the sample  efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contribution&quot;&gt;Contribution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI research platform for grounded language learning with a simulated human-in-the-loop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baseline results for performance and sample efficiency for the different tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;babyai-platform&quot;&gt;BabyAI Platform&lt;/h2&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MiniGrid - A partially observable 2D grid-world environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entities - Agent, ball, box, door, keys&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actions - pick, drop or move objects, unlock doors etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baby-language&quot;&gt;Baby Language&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Synthetic Language (a proper subset of English) - Used to give instructions to the agent&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Support for verifying if the task (and the subtasks) are completed or not&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;levels&quot;&gt;Levels&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A level is an instruction-following task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Formally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motivated by curriculum learning, the authors create a series of tasks (with increasing difficulty).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A subset of skills (competencies) is required for solving each task. The platform takes into account this constraint when creating a level.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heuristic-expert&quot;&gt;Heuristic Expert&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For any level, it can suggest actions or generate demonstrations (given the state of the environment).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An imitation learning baseline is trained for each level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GRU to encode the sentence, CNN to encode the input observation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FiLM layer to combine the two representations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM to encode the per-timestep FiLM encoding (timesteps in the environment)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two model variants are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Large Model - Bidirectional GRU + attention + large hidden state&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Small Model - Unidirectional GRU + No attention + small hidden state&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Curriculum-based pretraining and interactive learning was found to be useful in only some cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Poincaré Embeddings for Learning Hierarchical Representations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Poincare-Embeddings-for-Learning-Hierarchical-Representations"/>
   <updated>2018-10-11T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Poincare Embeddings for Learning Hierarchical Representations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Much of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This approach is not effective when data has a (latent) hierarchical structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.08039.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperbolic-geometry&quot;&gt;Hyperbolic Geometry&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;poincare-embeddings&quot;&gt;Poincare Embeddings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Poincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Distance between 2 pints change smoothly and is symmetric. Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Eventually the norm of a point represents its hierarchy and distance between the points represents similarity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RSGD (Riemannian SGD) method is used.&lt;/li&gt;
  &lt;li&gt;Riemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor.&lt;/li&gt;
  &lt;li&gt;The embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-details&quot;&gt;Training Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Initializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps.&lt;/li&gt;
  &lt;li&gt;The model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Embedding taxonomy for wordnet task&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Setup&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Reconstruction&lt;/li&gt;
          &lt;li&gt;Link Prediction&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The input data is a collection of a pair of words (u, v) which are related to each other.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For each word pair, 10 negative samples of the form (u, v’) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network Embedding&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Baselines&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Euclidean Embeddings&lt;/li&gt;
          &lt;li&gt;Translational Embedding where a relation vector corresponding to the edge type is also learnt.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Datasets&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ASTROPH&lt;/li&gt;
          &lt;li&gt;CONDMAT&lt;/li&gt;
          &lt;li&gt;GRQC&lt;/li&gt;
          &lt;li&gt;HEPPH&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lexical Entailment&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10].

* The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic embeddings are not suitable for all the datasets. Eg if the dataset is not tree-like or has cycles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>When Recurrent Models Don’t Need To Be Recurrent</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/When-Recurrent-Models-Don-t-Need-To-Be-Recurrent"/>
   <updated>2018-10-04T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/When Recurrent Models Don’t Need To Be Recurrent</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores “if a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Well behaved” is defined in terms of control-theoretic notion of stability. This roughly requires that the gradients do not explode over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference. The results are empirically validated as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.10369&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a general, non linear dynamical system given by a differential state transition map Φ&lt;sub&gt;w&lt;/sub&gt;. The hidden h&lt;sub&gt;t&lt;/sub&gt; = Φ&lt;sub&gt;w&lt;/sub&gt;(h&lt;sub&gt;t-1&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assumptions:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Φ is smooth in w and h.&lt;/li&gt;
      &lt;li&gt;h&lt;sub&gt;0&lt;/sub&gt; = 0&lt;/li&gt;
      &lt;li&gt;Φ&lt;sub&gt;w&lt;/sub&gt;(0, 0) = 0 (can be ensured by translation)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stable models are the ones where Φ is contractive ie Φ&lt;sub&gt;w&lt;/sub&gt;(h, x) - Φ&lt;sub&gt;w&lt;/sub&gt;(h’, x) is less than Λ * (h - h’)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, in RNN, stability would require that norm(w) is less than (L&lt;sub&gt;p&lt;/sub&gt;)&lt;sup&gt;-1&lt;/sup&gt; where L&lt;sub&gt;p&lt;/sub&gt; is the Lipschitz constant of the point-wise non linearity used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The feedforward approximation uses a finite context (of length k) and is a truncated model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A non-parametric function f maps the output of the recurrent model to prediction. If f is desired to be a parametric model, its parameters can be pushed to the recurrent model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-results&quot;&gt;Theoretical Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For a Λ-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space. It can be shown that this small difference does not lead to large gradient differences during subsequent update steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The stability condition is important as, without that, truncated models would be bad (even for large values of k). Further, it is difficult to show that gradient descent converges to a stationary point.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>HoME - a Household Multimodal Environment</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/HoME-a-Household-Multimodal-Environment"/>
   <updated>2018-09-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/HoME - a Household Multimodal Environment</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.11017&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Humans learn by interacting with their surroundings (environment).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Open-source and Open-AI gym compatible&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Built on top of 45000 3D house layouts from SUNCG dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Provides both 3D visual and audio recording.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic image segmentation and langauge description of objects.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;components&quot;&gt;Components&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rendering Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using Panda 3D game engine.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Renders RGB+depth scenes based on textures, multi-source lightings and shadows.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Acoustic Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using EVERT&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Supports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantics Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Provides a short textual description for each object, along with information like color, category, material size, location etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Physics Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using Bullet3 Engine&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Supports physical interaction, external forces like gravity and position and velocity information for multiple agents.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;potential-applications&quot;&gt;Potential Applications&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Visual Question Answering&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conversational Agents&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training an agent to follow instructions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-agent communication&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Emergence of Grounded Compositional Language in Multi-Agent Populations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Emergence-of-Grounded-Compositional-Language-in-Multi-Agent-Populations"/>
   <updated>2018-09-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Emergence of Grounded Compositional Language in Multi-Agent Populations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The language is quite rudimentary and is essentially a sequence of abstract discrete symbols. But it does comprise of a defined vocabulary and syntax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04908&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cooperative, partially observable Markov game (multi-agent extension of MDP).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All agents have identical action and observation spaces, use the same policy and receive a shared reward.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;grounded-communication-environment&quot;&gt;Grounded Communication Environment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The agents and the landmarks would occupy some location and would have some attributes (colour, shape).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Within the environment, the agents can &lt;em&gt;go to&lt;/em&gt; a location, &lt;em&gt;look&lt;/em&gt; at a location or &lt;em&gt;do nothing&lt;/em&gt;. Additionally, they can utter communication symbols c (from a shared vocabulary C). Agents themselves learn to assign a meaning to the symbols.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal for agent &lt;em&gt;i&lt;/em&gt; consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A model of all agent and environment state dynamics is created over time and the return gradient is computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gumbel-Softmax distribution is used to obtain categorical word emission c.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations. These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compositionality-and-vocabulary-size&quot;&gt;Compositionality and Vocabulary Size&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words. This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Along the lines of rich gets richer dynamics, the communication symbol c’s are modelled as being generated by a Dirichlet process. The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;case-i---agents-can-not-see-each-other&quot;&gt;Case I - Agents can not see each other&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Non-verbal communication is not possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When trained with just 2 agents, symbols are assigned for each landmark and action.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the number of agents is increased, additional symbols are used to refer to agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks. If distractor locations are added, the agents learn to ignore them.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-verbal-communication&quot;&gt;Non-verbal communication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Agents are allowed to observe other agents’ position, gaze etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the location can be pointed to using gaze.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If gaze is disabled, the agent could indicate the goal landmark by moving to it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically even when the communication is disabled the agents can come up with strategies to complete the task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Semantic Loss Function for Deep Learning with Symbolic Knowledge</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Semantic-Loss-Function-for-Deep-Learning-with-Symbolic-Knowledge"/>
   <updated>2018-08-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Semantic Loss Function for Deep Learning with Symbolic Knowledge</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach for using symbolic knowledge in deep learning systems. These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.11157&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable. In this process, the precise meaning of symbolic knowledge is often lost.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A differentiable “semantic loss” is derived which captures the meaning of the constraint while being independent of its syntax.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A state &lt;strong&gt;x&lt;/strong&gt; (state refers to the instantiation of boolean variables) satisfies a sentence &lt;em&gt;a&lt;/em&gt; if &lt;em&gt;a&lt;/em&gt; evaluates to true when using the variables as specified by &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A sentence &lt;em&gt;a&lt;/em&gt; entails another sentence &lt;em&gt;b&lt;/em&gt; if all states that satisfy &lt;em&gt;a&lt;/em&gt; also satisfy &lt;em&gt;b&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The row output vector of the neural network is denoted as &lt;em&gt;p&lt;/em&gt; where each value in &lt;em&gt;p&lt;/em&gt; denotes the probability of an output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Three different output constraints are studied:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Exactly-one constraint&lt;/em&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Exactly one value in &lt;em&gt;p&lt;/em&gt; should be true.&lt;/li&gt;
          &lt;li&gt;Can be expressed in boolean logic as follows: Let (x1, x2, …, xn) be variables in &lt;em&gt;p&lt;/em&gt;. Then (not xi or not xj) for all pair of variables and (x1 or x2 or … xn).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Valid Simple Path Constraint&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;Set of edges must form a valid path.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Ordering Constraint&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;Defining an ordering over the variables.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-loss&quot;&gt;Semantic Loss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p)&lt;/em&gt; is a function of a propositional logic sentence &lt;em&gt;a&lt;/em&gt; (the symbolic knowldge constraint) and &lt;em&gt;p&lt;/em&gt; (output of the neural network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;a&lt;/em&gt; is defined over variables (x1, …, xn) and &lt;em&gt;p&lt;/em&gt; is interpreted as a vector of probabilities corresponding to these variables &lt;em&gt;xi’s&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;main-axioms-and-insights&quot;&gt;Main Axioms and Insights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Monotonicity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If a sentence &lt;em&gt;a&lt;/em&gt; entails another sentence &lt;em&gt;b&lt;/em&gt; then for any given &lt;em&gt;p&lt;/em&gt;, &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p) &amp;gt; L&lt;sup&gt;s&lt;/sup&gt;(b, p)&lt;/em&gt; ie adding more constraints cannot decrease the semantic loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semantic Equivalence&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If two sentences are logically equivalent, their semantic loss is the same.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For any given sentence &lt;em&gt;a&lt;/em&gt;, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the “one-hot” constraint as a boolean expression is equivalent to a one-hot vector.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Satisfaction&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If &lt;em&gt;p&lt;/em&gt; entails the sentence &lt;em&gt;a&lt;/em&gt; then &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p) = 0&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Label-literal correspondence&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;When the constraint is defined in terms of a single variable, it can be interpreted as the supervised label.&lt;/li&gt;
      &lt;li&gt;Hence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Truth&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The semantic loss of a true sentence is 0&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-negativity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Semantic loss should always be non-negative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probabilities of variables that are not part of the constraint, do not affect the semantic loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;It can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-evaluation&quot;&gt;Experimental Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model. Further, the semantic loss does not improve the performance in case of fully supervised scenario.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further experiments are performed to evaluate the performance of the semantic loss on complex constraints. Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tractability-of-semantic-loss&quot;&gt;Tractability of Semantic Loss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss is similar to the automated reasoning task called as weight model counting (wmc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Circuit compiler techniques can be used to compute wmc while allowing backpropagation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good. It would be interesting to extend and scale this method for more complex constraints.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hierarchical Graph Representation Learning with Differentiable Pooling</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling"/>
   <updated>2018-08-16T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hierarchical Graph Representation Learning with Differentiable Pooling</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.08804&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-idea&quot;&gt;Key Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph &lt;em&gt;G(A, F)&lt;/em&gt;, where &lt;em&gt;A&lt;/em&gt; is the adjacency matrix and &lt;em&gt;F&lt;/em&gt; is the feature matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a permutation invariant GNN that follows the message passing architecture. The output of this GNN can be expressed as &lt;em&gt;Z = GNN(A, X)&lt;/em&gt; where &lt;em&gt;X&lt;/em&gt; is the current feature matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal is to stack &lt;em&gt;L&lt;/em&gt; GNN layers on top of each other such that the &lt;em&gt;l&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer uses coarsened output from the  &lt;em&gt;(l-1)&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This coarsening operation uses a cluster assignment matrix &lt;em&gt;S&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learned cluster assignment matrix at layer &lt;em&gt;l&lt;/em&gt; is denoted at &lt;em&gt;S&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given &lt;em&gt;S&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt;, the embedding matrix for the &lt;em&gt;(l+1)&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer is given as &lt;em&gt;transpose(S&lt;sub&gt;l&lt;/sub&gt;)Z&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt; and adjancecy matrix is given by &lt;em&gt;transpose(S&lt;sub&gt;l&lt;/sub&gt;)A&lt;sub&gt;l&lt;/sub&gt;S&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new GNN, called as GNN&lt;sub&gt;pool&lt;/sub&gt; is used to produce the assignment matrix &lt;em&gt;S&lt;/em&gt; by taking a softmax over &lt;em&gt;GNN&lt;sub&gt;pool&lt;/sub&gt;(A&lt;sup&gt;l&lt;/sup&gt;, X&lt;sup&gt;l&lt;/sup&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auxiliary-losses&quot;&gt;Auxiliary Losses&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baselines&quot;&gt;Baselines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GNN based models
    &lt;ul&gt;
      &lt;li&gt;GraphSage
        &lt;ul&gt;
          &lt;li&gt;Mean pooling&lt;/li&gt;
          &lt;li&gt;Set2Set pooling&lt;/li&gt;
          &lt;li&gt;Sort pooling&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Structure2vec&lt;/li&gt;
      &lt;li&gt;Edge conditioned filters in CNN&lt;/li&gt;
      &lt;li&gt;PatchySan&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kernel based models
    &lt;ul&gt;
      &lt;li&gt;Graphlet, shortest path etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-variants&quot;&gt;Model Variants&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GraphSage
    &lt;ul&gt;
      &lt;li&gt;Mean pool + Diff pool (3 or 2 layers)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Structure2Vec + Diffpool&lt;/li&gt;
  &lt;li&gt;Diffpool-Det
    &lt;ul&gt;
      &lt;li&gt;The assignment matrix &lt;em&gt;S&lt;/em&gt; are generated using graph clustering algorithms.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Diffpool-NoLP
    &lt;ul&gt;
      &lt;li&gt;The link prediction objective function is turned off.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time. But given that now there are 2 GNN models per layer, the size of the model should increase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DiffPool can capture hierarchical community structure even when trained on just the graph classification loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Imagination-Augmented Agents for Deep Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Imagination-Augmented-Agents-for-Deep-Reinforcement-Learning"/>
   <updated>2018-08-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Imagination-Augmented Agents for Deep Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I2A agent uses the predictions from a learned environment model as an additional context in deep policy networks. This leads to improved data efficiency and robustness to imperfect models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.06203&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I2A agent has two main modules - Imagination module and the Policy module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Imagination Module&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Environment Model&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;This is a recurrent model, trained in an unsupervised manner using the agent trajectories. It can be used to predict the future state given the current state and action.&lt;/li&gt;
          &lt;li&gt;The environment model can be rolled out multiple times to obtain a simulated trajectory or an “imagined” trajectory.&lt;/li&gt;
          &lt;li&gt;During each rollout, the actions are chosen using a rollout policy π&lt;sub&gt;r&lt;/sub&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Rollout Encoder&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;A rollout encoder &lt;em&gt;E&lt;/em&gt; (LSTM) is used to process the entire imagined rollout.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The imagination module is used to generate &lt;em&gt;n&lt;/em&gt; trajectories. Each trajectory is a sequence of outputs of the environment model.&lt;/li&gt;
      &lt;li&gt;These &lt;em&gt;n&lt;/em&gt; trajectories are concatenated into a single “imagination” vector.&lt;/li&gt;
      &lt;li&gt;The training data for the environment model is generated from trajectories of a partially trained model-free agent.&lt;/li&gt;
      &lt;li&gt;Pretraining the environment model (instead of joint training with policy) leads to faster runtime.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Policy Module&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This module uses the output of both model-based path and model-free path as its input. It generates the policy vector and value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rollout Strategy&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;One rollout is performed for each possible action in the environment ie, the first action in the i&lt;sup&gt;th&lt;/sup&gt; rollout is the i&lt;sup&gt;th&lt;/sup&gt; action in the action set.&lt;/li&gt;
      &lt;li&gt;Subsequent actions are generated using a shared rollout policy π&lt;sub&gt;’&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;An effective strategy was to create a small model-free network π&lt;sub&gt;’&lt;/sub&gt;(o&lt;sub&gt;t&lt;/sub&gt;) and then add a KL loss component that encourages π&lt;sub&gt;’&lt;/sub&gt;(o&lt;sub&gt;t&lt;/sub&gt;)to be similar to the imagination augmented policy π(o&lt;sub&gt;t&lt;/sub&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baselines&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Model-free agent&lt;/li&gt;
      &lt;li&gt;Copy-model agent - same as I2A but the environment model is replaced by a “copy” model that just returns the input observations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Sokoban
        &lt;ul&gt;
          &lt;li&gt;Task is to push a number of boxes onto given target locations.&lt;/li&gt;
          &lt;li&gt;I2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate).&lt;/li&gt;
          &lt;li&gt;In case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate.&lt;/li&gt;
          &lt;li&gt;Monte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder.&lt;/li&gt;
          &lt;li&gt;Predicting the reward along with value function and action seems to speed up training.&lt;/li&gt;
          &lt;li&gt;If a near-perfect model is available, I2A agent’s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy. The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;MiniPacman&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;I2A agent is evaluated to see if a single model can be used to solve multiple tasks.&lt;/li&gt;
          &lt;li&gt;A new environment is designed to define multiple tasks in an environment with shared state transitions.&lt;/li&gt;
          &lt;li&gt;Each task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost.&lt;/li&gt;
          &lt;li&gt;A single environment model is trained to predict both observations (frames) and events (eg “eating a pill”). This way, the environment model is shared across all tasks.&lt;/li&gt;
          &lt;li&gt;Baseline agents and I2As are trained on each task separately. I2A architecture outperforms the standard agent in all tasks and the copy-model
baseline in all but one task.&lt;/li&gt;
          &lt;li&gt;The improvement in performance is higher for tasks where rewards are sparse and where the anticipation
of ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Kronecker Recurrent Units</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Kronecker-Recurrent-Units"/>
   <updated>2018-07-19T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Kronecker Recurrent Units</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent Neural Networks have two key issues:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Over parameterization&lt;/strong&gt; which increases the time for training and inference.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Ill conditioned&lt;/strong&gt; recurrent weight matrix which makes training difficult due to vanishing or exploding gradients.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.10142&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;h3 id=&quot;existing-solutions-for-overparameterization&quot;&gt;Existing solutions for overparameterization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low-rank decomposition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training a neural network on the soft targets predicted by a big pre-trained network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low-bit precision training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hashing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;existing-solutions-for-vanishing-and-exploding-gradients&quot;&gt;Existing solutions for vanishing and exploding gradients&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Gating mechanism like in LSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient Clipping.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Orthogonal Weight Initialization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Parameterizing recurrent weight matrix.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kru&quot;&gt;KRU&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vanishing and exploding gradients are taken care of by using a soft unitary constraint.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why not use strict unitary constraint:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Restricts the search space and makes learning process unstable.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Makes forgetting (irrelevant) information difficult.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Relaxing the strict constraint has shown to improve the convergence speed and generalization performance.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KRU can be easily plugged into RNNs, LSTMs and other variants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The recurrent matrix &lt;em&gt;W&lt;/em&gt; is paramterized as a kronecker product of &lt;em&gt;F&lt;/em&gt; matrices &lt;em&gt;W&lt;sub&gt;0&lt;/sub&gt;, …, W&lt;sub&gt;F-1&lt;/sub&gt;&lt;/em&gt; where each &lt;em&gt;W&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; is a complex matrix of shape &lt;em&gt;P&lt;sub&gt;f&lt;/sub&gt; x Q&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; and the product of all &lt;em&gt;P&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; and producto of all &lt;em&gt;Q&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; are both equal to &lt;em&gt;N&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why is &lt;em&gt;W&lt;/em&gt; a complex matrix?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In the real space, the set of all unitary matrices have the determinant as 1 or -1.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Given that determinant is a continuous function, the unitary set in the real space is disconnected.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The unitary set in the complex space is connected as its determinants are points on the unit circle.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;soft-unitary-constraint&quot;&gt;Soft Unitary Constraint&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;A soft unitary constraint is introduced in the form of regularization term&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;W&lt;sub&gt;f&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt;W&lt;sub&gt;f&lt;/sub&gt; - I&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;&lt;sup&gt;2&lt;/sup&gt; (per kronecker factored recurrent matrix).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If each of the Kronecker factors is unitary, the resulting matrix &lt;em&gt;W&lt;/em&gt; would also be unitary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is computationally inefficient to apply this constraint over the recurrent matrix &lt;em&gt;W&lt;/em&gt; itself as the complexity of the regularizer is given as &lt;em&gt;O(N&lt;sup&gt;3&lt;/sup&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Use of Kronecker factorisation makes it computationally feasible to use this regulariser.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output. The recurrent dynamics should be implemented via a low capacity model.s per task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning Independent Causal Mechanisms</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-Independent-Causal-Mechanisms"/>
   <updated>2018-07-11T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning Independent Causal Mechanisms</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.00961&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We start with a given data distribution &lt;em&gt;P&lt;/em&gt; (say the MNIST dataset) where each x ε R&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider N transformations M&lt;sub&gt;1&lt;/sub&gt;, …, M&lt;sub&gt;N&lt;/sub&gt; (functions that map input x to transformed input x’). Note that N need not be known before hand.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These transformations can be thought of as independent (from other transformations) causal mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Applying these transformation would give N new distributions Q&lt;sub&gt;1&lt;/sub&gt;, …, Q&lt;sub&gt;N&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At training time, two datasets are created. One dataset corresponds to untransformed objects (sampled from &lt;em&gt;P&lt;/em&gt;), referred to as &lt;em&gt;D&lt;sub&gt;P&lt;/sub&gt;&lt;/em&gt;. The other dataset corresponds to samples from the transformed distribution &lt;em&gt;Q&lt;/em&gt; and is referred to as &lt;em&gt;D&lt;sub&gt;Q&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that all the samples in &lt;em&gt;D&lt;sub&gt;P&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;D&lt;sub&gt;Q&lt;/sub&gt;&lt;/em&gt; are sampled independently and no supervising information is needed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A series of N’ parametric models, called as experts, are initialized and would be trained to learn the different mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For simplicity, assume that N = N’. If N &amp;gt; N’, some experts would learn more than one transformation or certain transformations would not be learnt. If N &amp;lt; N’, some experts would not learn anything or some experts would learn the same distribution. All of these cases can be diagnosed and corrected by changing the number of experts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The experts are trained with the goal of maximizing an objective parameter &lt;em&gt;c&lt;/em&gt;: R&lt;sup&gt;d&lt;/sup&gt; to R. &lt;em&gt;c&lt;/em&gt; takes high values on the support of  &lt;em&gt;P&lt;/em&gt; and low values outside.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, an example x&lt;sub&gt;Q&lt;/sub&gt; (from D&lt;sub&gt;Q&lt;/sub&gt;) is fed to all the experts at the same time. Each expert produces a value &lt;em&gt;c&lt;sub&gt;j&lt;/sub&gt; = c(E&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;Q&lt;/sub&gt;))&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The winning expert is the one whose output is the max among all the outputs. Its parameters are updated to maximise its output while the other experts are not updated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This forces the best performing model to become even better and hence specialize.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The objective &lt;em&gt;c&lt;/em&gt; comes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each expert can be thought of as a GAN that conditions on the input x&lt;sub&gt;Q&lt;/sub&gt; (and not on a noise vector). The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximate-identity-initialization&quot;&gt;Approximate Identity Initialization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This ensures that the experts start from a similar level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, it seems necessary for the success of the proposed approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The approximate quality of reconstructed output was also evaluated using a downstream task.
    &lt;ul&gt;
      &lt;li&gt;3 type of inputs were created:
        &lt;ul&gt;
          &lt;li&gt;Untransformed images&lt;/li&gt;
          &lt;li&gt;Transformed images&lt;/li&gt;
          &lt;li&gt;Transformed images a being processed by experts.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;These inputs are fed to a pretrained MNISTN classifier.&lt;/li&gt;
      &lt;li&gt;The classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The experts E&lt;sub&gt;i&lt;/sub&gt; generalize on the data points from a different dataset as well.
    &lt;ul&gt;
      &lt;li&gt;To test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST).&lt;/li&gt;
      &lt;li&gt;Each expert consistently applies the same transformation even though the inputs are outside the training domain.&lt;/li&gt;
      &lt;li&gt;This suggests that the experts have generalized to different transformations irrespective of the underlying dataset.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Appendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all. This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Memory-based Parameter Adaptation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Memory-Based-Parameter-Adaption"/>
   <updated>2018-07-04T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Memory-Based Parameter Adaption</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Standard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.10542&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA consists of 3 components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Embedding Network &lt;em&gt;f&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Memory &lt;em&gt;M&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Output network &lt;em&gt;g&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;f&lt;/em&gt; and &lt;em&gt;g&lt;/em&gt; are parametric components while &lt;em&gt;M&lt;/em&gt; is a non-parametric component.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;M&lt;/em&gt; is a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a new training tuple (x&lt;sub&gt;j&lt;/sub&gt;, y&lt;sub&gt;j&lt;/sub&gt;) is fed as input to the model, a key-value pair (h&lt;sub&gt;j&lt;/sub&gt;, v&lt;sub&gt;j&lt;/sub&gt;) is added to the memory. h&lt;sub&gt;j&lt;/sub&gt; = f(x&lt;sub&gt;j&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The memory has a fixed size and acts as a circular buffer. When it gets filled up, earlier examples are dropped.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When accessing the memory using a key &lt;em&gt;h&lt;sub&gt;key&lt;/sub&gt;&lt;/em&gt;, the k-nearest neighbours (in terms of distance from the given key) are retrieved.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-phase&quot;&gt;Training Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;During the training phase, the memory is only used to store the input examples and does not interfere with the training procedure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;testing-phase&quot;&gt;Testing Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, the memory is used to adapt the parameters of the output network &lt;em&gt;g&lt;/em&gt; while the embedding network &lt;em&gt;f&lt;/em&gt; remains the same.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each retrived neighbour is a tuple of the form (h&lt;sub&gt;k&lt;/sub&gt;, v&lt;sub&gt;k&lt;/sub&gt;, w&lt;sub&gt;k&lt;/sub&gt;) where w&lt;sub&gt;k&lt;/sub&gt; is propotional to the closeness between the input query and the key corresponding to the retrived example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The collection of all the retrieved examples are referred to as the context &lt;em&gt;C&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameters of the output network &lt;em&gt;g&lt;/em&gt; are adapted from θ to θ&lt;sub&gt;x&lt;/sub&gt; where θ&lt;sub&gt;x&lt;/sub&gt; = θ + δ&lt;sub&gt;M&lt;/sub&gt;(x, θ)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;δ&lt;sub&gt;M&lt;/sub&gt;(x, θ) is referred to as the contextual update of parameters of the output network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;interpretation-of-mbpa&quot;&gt;Interpretation of MbPA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The expression corresponding to  δ&lt;sub&gt;M&lt;/sub&gt;(x, θ) can be obtained by performing gradient descent to minimise the max a posterior over the context C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This idea can be thought of as a generalisation of attention. Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model. Refer appendix E in the paper for further details.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continual Learning:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setting, the model encounters a sequence of tasks and cannot revisit a previous task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Permuted MNIST dataset was used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incremental Learning:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes. The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Imagenet dataset with Resnet V1 model is used. It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unbalanced Dataset:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Language Modelling:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks. PTB and WikiText datasets were used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Born Again Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Born-Again-Neural-Networks"/>
   <updated>2018-06-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Born Again Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.04770&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The standard KD setting is as follows:
    &lt;ul&gt;
      &lt;li&gt;Start with an untrained network (or ensemble of networks) and train them for the given task. This network is referred to as the teacher network.&lt;/li&gt;
      &lt;li&gt;Now start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network. This network is referred to as the student network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks. The student tried to predict the correct answer while matching the output distribution of the teacher.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting student network is referred to as BAN - Born Again Network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dark-knowledge&quot;&gt;Dark Knowledge&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network&quot;&gt;Hinton et al&lt;/a&gt; suggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes. This information is referred to as the “dark knowledge”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor. This sample specific weight is defined by the value of the teacher’s max output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This suggests distillation may be performing some kind of importance weighing. To explore this further, the paper considers 2 cases:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Confidence Weighted By Teacher Max (CWTM) - where each example in the student’s loss function is weighted by the confidence that the teacher has on the prediction for that sample. The student incurs a higher loss if the teacher was more confident about the example.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Dark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher’s predictive distribution are permuted thus destroying the information about which output classes are related.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;image-data&quot;&gt;Image Data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets
    &lt;ul&gt;
      &lt;li&gt;CIFAR10&lt;/li&gt;
      &lt;li&gt;CIFAR100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;ResNets&lt;/li&gt;
      &lt;li&gt;DenseNets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BAN Variants
    &lt;ul&gt;
      &lt;li&gt;BAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets. Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc.&lt;/li&gt;
      &lt;li&gt;Two settings with CWTM and DKPP as explained earlier.&lt;/li&gt;
      &lt;li&gt;BAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-data&quot;&gt;Text Data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets:
    &lt;ul&gt;
      &lt;li&gt;PTB Dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;CNN-LSTM model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BAN Variant
    &lt;ul&gt;
      &lt;li&gt;LSTM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;BAN student models improved over their teachers in most of the configurations.&lt;/li&gt;
  &lt;li&gt;Training BANs across multiple generations leads to saturating improvements.&lt;/li&gt;
  &lt;li&gt;The student models exhibit improvements even in the control settings (CWTM and DKPP).
    &lt;ul&gt;
      &lt;li&gt;One reason could be that the permutation procedure did not remove the higher order moments of output distribution.&lt;/li&gt;
      &lt;li&gt;Improvements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher’s output distribution is more spread.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Net2Net-Accelerating Learning via Knowledge Transfer</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Net2Net-Accelerating-Learning-via-Knowledge-Transfer"/>
   <updated>2018-05-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Net2Net - Accelerating Learning via Knowledge Transfer</id>
   <content type="html">&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.05641&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/paengs/Net2Net&quot;&gt;Link to an implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach works as follows - Let us say that the teacher network was represented by the transformation &lt;em&gt;y = f(x, θ)&lt;/em&gt; where &lt;em&gt;θ&lt;/em&gt; refer to the parameters of the network. The task is to choose a new set of parameters &lt;em&gt;θ’&lt;/em&gt; for the student network &lt;em&gt;g(x, θ’)&lt;/em&gt; such that for all &lt;em&gt;x, f(x, θ) = g(x, θ’)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To start, we can assume that &lt;em&gt;f&lt;/em&gt; and &lt;em&gt;g&lt;/em&gt; are composed of standard linear layers. Layer &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;i+1&lt;/em&gt; are represented by weights &lt;em&gt;W&lt;sub&gt;mxn&lt;/sub&gt;&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;W&lt;sub&gt;nxp&lt;/sub&gt;&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We want to grow layer &lt;em&gt;i&lt;/em&gt; to have &lt;em&gt;q&lt;/em&gt; output units (where &lt;em&gt;q&lt;/em&gt; &amp;gt; &lt;em&gt;n&lt;/em&gt;) and layer &lt;em&gt;i+1&lt;/em&gt; to have &lt;em&gt;q&lt;/em&gt; input units. The new weight matrix would be &lt;em&gt;U&lt;sub&gt;mxq&lt;/sub&gt;&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;U&lt;sub&gt;qxp&lt;/sub&gt;&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first &lt;em&gt;q&lt;/em&gt; columns (rows) of &lt;em&gt;W&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; (&lt;em&gt;W&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;) would be copied as it is into &lt;em&gt;U&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt;(&lt;em&gt;U&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For filling the remaining &lt;em&gt;n-q&lt;/em&gt; slots, columns (rows) would be sampled randomly from &lt;em&gt;W&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; (&lt;em&gt;W&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, each layer in &lt;em&gt;U&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The benefits of using this approach are the following:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The newly created student network performs at least as good as the teacher network.&lt;/li&gt;
      &lt;li&gt;Any changes to the network are guaranteed to be an improvement.&lt;/li&gt;
      &lt;li&gt;It is safe to optimize all the parameters in the network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The variant discussed above is called the &lt;strong&gt;Net2WiderNet&lt;/strong&gt; variant. There is another variant called&lt;strong&gt;Net2DeeperNet&lt;/strong&gt; that enables the network to grow in depth.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In that case, a new matrix, &lt;em&gt;U&lt;/em&gt;, initialized as the identity matrix, is added to the network. Note that unlike the &lt;strong&gt;Net2WiderNet&lt;/strong&gt;, this approach would not work with arbitrary activation function between the layers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;strengths&quot;&gt;Strengths&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The function preserving transformations need to be worked out manually. Extra care needs to be taken when operations like concatenation or batch norm are present.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning to Count Objects in Natural Images for Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-to-Count-Objects-in-Natural-Images-for-Visual-Question-Answering"/>
   <updated>2018-05-06T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning to Count Objects in Natural Images for Visual Question Answering</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Most of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image. The main reasons are:
    &lt;ul&gt;
      &lt;li&gt;Most VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector. These aggregated features helps in most category of questions but seems to hurt for counting based questions.&lt;/li&gt;
      &lt;li&gt;For the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image. This limits the scope of supervision.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate &lt;strong&gt;count&lt;/strong&gt; module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05766&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object. This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image.&lt;/p&gt;

&lt;p&gt;Even multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to). Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis.&lt;/p&gt;

&lt;p&gt;The proposed &lt;strong&gt;count&lt;/strong&gt; module is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions.&lt;/p&gt;

&lt;p&gt;The inputs to the &lt;strong&gt;count&lt;/strong&gt; module are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question.&lt;/p&gt;

&lt;p&gt;The top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap). The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily.&lt;/p&gt;

&lt;p&gt;To explain their methodology, the paper simplifies the setting by making two assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal).&lt;/li&gt;
  &lt;li&gt;The second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the &lt;strong&gt;count&lt;/strong&gt; module.&lt;/p&gt;

&lt;p&gt;Given the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects.&lt;/p&gt;

&lt;p&gt;As the first step, the attention weights (&lt;strong&gt;a&lt;/strong&gt;) are used to generate an attention matrix (&lt;strong&gt;A&lt;/strong&gt;) by performing an outer product between &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;a&lt;sup&gt;T&lt;/sup&gt;&lt;/strong&gt;. This corresponds to the step of creating a graph from the input.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt; corresponds to the adjacency matrix of that graph. The attention weight for the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; proposal corresponds to the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; node in the graph and the edge between the nodes &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; has the weight &lt;strong&gt;a&lt;sub&gt;i&lt;/sub&gt;*a&lt;sub&gt;j&lt;/sub&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition &lt;strong&gt;a&lt;sub&gt;i&lt;/sub&gt;&lt;/strong&gt; = 1 is a complete directed graph with self-loops. Given such a graph, the number of vertices, &lt;em&gt;V = sqrt(E)&lt;/em&gt; where &lt;em&gt;E&lt;/em&gt; could be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix.&lt;/p&gt;

&lt;p&gt;The objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph. This requires removing two type of duplicate edges - intra-object edges and inter-object edges.&lt;/p&gt;

&lt;p&gt;Intra-object edges can be removed by computing a distance matrix, &lt;strong&gt;D&lt;/strong&gt;, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix. A modified adjacency matrix &lt;strong&gt;A’&lt;/strong&gt; is obtained by performing the element-wise product between f&lt;sub&gt;1&lt;/sub&gt;(&lt;strong&gt;A&lt;/strong&gt;) and f&lt;sub&gt;2&lt;/sub&gt;(&lt;strong&gt;D&lt;/strong&gt;) where f&lt;sub&gt;1&lt;/sub&gt; and f&lt;sub&gt;2&lt;/sub&gt; are piece-wise linear functions that are learnt via backpropogation.&lt;/p&gt;

&lt;p&gt;The inter-object edges are removed in the following manner:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Count the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number.&lt;/li&gt;
  &lt;li&gt;This creates the effect of reducing the weights of multiple proposals equivalent to a single proposal.&lt;/li&gt;
  &lt;li&gt;The number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights &lt;strong&gt;a&lt;/strong&gt;, adjacency matrix &lt;strong&gt;A&lt;/strong&gt; and distance matrix &lt;strong&gt;D&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;The matrix corresponding to the similarity between proposals  (&lt;strong&gt;sim&lt;sub&gt;i, j&lt;/sub&gt;&lt;/strong&gt;) is transformed into a vector corresponding to the scaling factor of each node (&lt;strong&gt;s&lt;sub&gt;i&lt;/sub&gt;&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;s&lt;/strong&gt; can be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges. The self edges (which were removed while computing &lt;strong&gt;A’&lt;/strong&gt; are added back (after scaling with &lt;strong&gt;s&lt;/strong&gt;) to obtain a new transformed matrix &lt;strong&gt;C&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The transformed matrix &lt;strong&gt;C&lt;/strong&gt; is a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals. The actual count can be obtained from &lt;strong&gt;C&lt;/strong&gt; by performing a sum over all its values as described earlier. The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues. The network produces a &lt;strong&gt;k&lt;/strong&gt;-hot &lt;strong&gt;n&lt;/strong&gt;-dimensional vector called &lt;strong&gt;o&lt;/strong&gt; where &lt;strong&gt;n&lt;/strong&gt; is the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count). In the ideal setting, &lt;strong&gt;k&lt;/strong&gt; should be one, as the network would produce an integer value but in practice, the network produces a real number so &lt;strong&gt;k&lt;/strong&gt; can be upto 2. If &lt;strong&gt;c&lt;/strong&gt; is an exact integer, the output is a 1-hot vector with the value in index corresponding to &lt;strong&gt;c&lt;/strong&gt; set to 1. If &lt;strong&gt;c&lt;/strong&gt; is a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which &lt;strong&gt;c&lt;/strong&gt; lies).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;count&lt;/strong&gt; module supports computing the confidence of a prediction by defining two variables p&lt;sub&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/sub&gt; and p&lt;sub&gt;&lt;strong&gt;D&lt;/strong&gt;&lt;/sub&gt; which compute the average distance of f&lt;sub&gt;6&lt;/sub&gt;(&lt;strong&gt;a&lt;/strong&gt;) and $f&lt;sub&gt;7&lt;/sub&gt;(&lt;strong&gt;D&lt;/strong&gt;) from 0.5. The final output &lt;strong&gt;o’&lt;/strong&gt; is defined as f&lt;sub&gt;8&lt;/sub&gt;(p&lt;sub&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/sub&gt; + p&lt;sub&gt;&lt;strong&gt;D&lt;/strong&gt;&lt;/sub&gt;) . &lt;strong&gt;o&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the different f functions are piece wise linear functions and are learnt via backpropagation.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;The authors created a new category of count-based questions by filtering the number-type questions to remove questions like “What is the time right now”. These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the &lt;strong&gt;count&lt;/strong&gt; model.&lt;/p&gt;

&lt;p&gt;The authors augmented a state of the art &lt;a href=&quot;https://arxiv.org/abs/1704.03162&quot;&gt;VQA model&lt;/a&gt; with their &lt;strong&gt;count&lt;/strong&gt; module and show substantial gains over the count-type questions for the &lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;&gt;VQA-v2 dataset&lt;/a&gt;. This augmentation does not drastically impact the performance on non-count questions.&lt;/p&gt;

&lt;p&gt;The overall idea is quite crisp and intutive and the paper is easy to follow. It would be even better if there were some more abalation studies. For example, why are the piece-wise linear functions assumed to have 16 linear components? Would a smaller or larger number be better?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Neural Message Passing for Quantum Chemistry</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry"/>
   <updated>2018-04-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Message Passing for Quantum Chemistry</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Variants of the MPNN model achieve very good performance on the task of predicting the property of the molecules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.01212&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mpnn&quot;&gt;MPNN&lt;/h1&gt;

&lt;h2 id=&quot;setting&quot;&gt;Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The input to the model is an undirected graph &lt;em&gt;G&lt;/em&gt; where node features are represented as &lt;em&gt;x&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; (corresponding to node &lt;em&gt;v&lt;/em&gt;) and edge features are &lt;em&gt;e&lt;sub&gt;v, w&lt;/sub&gt;&lt;/em&gt; (corresponding to edge between nodes &lt;em&gt;v, w&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model can be easily extended to the setting of directed graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model works in 2 phases:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-passing-phase&quot;&gt;Message Passing Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;All nodes send a &lt;em&gt;message&lt;/em&gt; to their neighbouring nodes. The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes. The feature vectors can be combined to form the message using the &lt;em&gt;message function&lt;/em&gt; which can be implemented as a neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message. The function used to aggregate and update the feature vector is called as the &lt;em&gt;update function&lt;/em&gt; and can be implemented as a neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After updating the feature vectors, the graph could initiate another round of message passing. After a sufficient number of message passing rounds, the Readout phase is invoked.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;readout-phase&quot;&gt;Readout Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the &lt;em&gt;readout function&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;readout function&lt;/em&gt; can also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;existing-variants-in-literature&quot;&gt;Existing Variants in literature&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;The paper provides various examples where the existing architectures could be explained in terms of the message passing framework. This includes examples like &lt;a href=&quot;https://arxiv.org/abs/1509.09292&quot;&gt;Convolutional Networks on Graphs for Learning Molecular Fingerprints&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1511.05493&quot;&gt;
Gated Graph Sequence Neural Networks&lt;/a&gt;, &lt;a href=&quot;http://tkipf.github.io/graph-convolutional-networks/&quot;&gt;Graph Convolutional Networks&lt;/a&gt; etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Broadly speaking, the task is to predict the properties of given molecules (regression problem).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Properties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two benchmarks for error:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;DFT Error - Estimated average error of DFT approximation&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Chemical Accuracy - As established by the chemistry community&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Following variants of &lt;em&gt;message function&lt;/em&gt; are explored:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Matrix multiplication between &lt;em&gt;A&lt;sub&gt;evw&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;h&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; where &lt;em&gt;A&lt;/em&gt; is the adjacency matrix &lt;em&gt;h&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; is the feature corresponding to node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Edge Network which is same as matrix multiplication case with the difference that &lt;em&gt;A&lt;/em&gt; is a learned matrix for each edge type.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;virtual-elements&quot;&gt;Virtual Elements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Since all messages are shared via edges, it could take a long time for the message to move between two ends of the graph. To fasten this process, virtual elements are provided.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the first setting, “virtual edges” are inserted between nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the second setting, a “master” node connects to all the other nodes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-passing-complexity&quot;&gt;Message Passing Complexity&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a graph with &lt;em&gt;n&lt;/em&gt; nodes and &lt;em&gt;d&lt;/em&gt; dimensional feature vectors, a single step of message passing would have the worst case time complexity of &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;d&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This complexity can be reduced by breaking the &lt;em&gt;d&lt;/em&gt; dimensional embedding into &lt;em&gt;k&lt;/em&gt; different groups of &lt;em&gt;d/k&lt;/em&gt; embeddings which can be updated in parallel. The complexity of the modified approach is &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;d&lt;sup&gt;2&lt;/sup&gt;/k&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Best performing MPNN model uses edge network as the &lt;em&gt;message function&lt;/em&gt; and &lt;a href=&quot;https://arxiv.org/abs/1511.06391&quot;&gt;set2set&lt;/a&gt; as the &lt;em&gt;readout function&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using group of embeddings helps to improve generalization. This effect could also be because of ensemble-like nature of the modified architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model performs worse without the virtual elements.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Long range interaction between vertices is necessary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Unsupervised Learning by Predicting Noise</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Unsupervised-Learning-By-Predicting-Noise"/>
   <updated>2018-04-02T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Unsupervised Learning By Predicting Noise</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.05310&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;h3 id=&quot;unsupervised-setting&quot;&gt;Unsupervised Setting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given a collection of image X (x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, …, x&lt;sub&gt;n&lt;/sub&gt;), we want to learn a parameterized mapping &lt;em&gt;f&lt;/em&gt; such that &lt;em&gt;f(x&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; gives the features of image &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;. We would jointly learn the target vectors &lt;em&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; (more on it later).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Squared L2 norm is used as the distance measure while making sure that final activations are unit normalized.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fixed-target-representation&quot;&gt;Fixed Target Representation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation. No discriminative features are learned in this case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is an assumption that k &amp;gt; n so that each image is assigned a different target.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other. But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation. That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3). An optimisation is to take a batch of b images and update the square matrix P&lt;sub&gt;B&lt;/sub&gt; for dimension bXb (made of the images and their corresponding targets). This reduces the overall complexity of O(nb^2).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-detail&quot;&gt;Implementation Detail&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Used AlexNet with NATs to train the unsupervised model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An MLP is trained on these features to learn the classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard preprocessing techniques like random cropping/flipping are used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experimental-details&quot;&gt;Experimental Details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ImageNet for training the AlexNet architecture with the proposed approach.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pascal VOC 2007 for transfer learning experiments.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Unsupervised approaches like autoencoder, GAN, BiGAN&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Self-supervised&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;SOTA models using hand-made features SIFT with Fisher Vector.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observation&quot;&gt;Observation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using squared loss instead of softmax does not deteriorate the performance too much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors compare the effect of using discrete vs continuous target representations for transfer learning. For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used. Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task. The test accuracy increases up to around 100 iterations then saturate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline. But it is still far behind the model using handcrafted features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality. It is interesting to see that such a simple strategy is able to give such good results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>The Lottery Ticket Hypothesis - Training Pruned Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/The-Lottery-Ticket-Hypothesis-Training-Pruned-Neural-Networks"/>
   <updated>2018-03-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/The Lottery Ticket Hypothesis - Training Pruned Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Empirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper purposes a hypothesis called the &lt;em&gt;lottery ticket hypothesis&lt;/em&gt; to explain this behaviour.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is the following - Successful training of a neural network depends on a &lt;em&gt;lucky&lt;/em&gt; random initialization of a subcomponent of the network. Such components are referred to as &lt;em&gt;lottery tickets&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Larger networks are more likely to have these &lt;em&gt;lottery tickets&lt;/em&gt; and hence are easier to train.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.03635&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Various aspects of the hypothesis are explored empirically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two tasks are considered - MNIST and XOR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting network is the &lt;em&gt;lottery-ticket&lt;/em&gt; in the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size. Further, it is more likely to match the original, larger network in terms of performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores different aspects of this experiment:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Pruning Strategies:
        &lt;ul&gt;
          &lt;li&gt;One-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively.&lt;/li&gt;
          &lt;li&gt;Though the latter is computationally more intensive, it is more likely to find a lottery ticket.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Size of the pruned network affects the speed of convergence when training the &lt;em&gt;lottery ticket&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;If only the architecture or only the initial weights of the &lt;em&gt;lottery ticket&lt;/em&gt; are used, the resulting network tends to converge more slowly and achieves a lower level of performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;This indicates that the lottery ticket depends on both the network architecture and the weight initialization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper includes some more interesting experiments. For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again. This performance should be compared with the performance of the initial large network and the performance of the &lt;em&gt;lottery ticket&lt;/em&gt; after training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis. The proposition itself is very interesting and could enhance our understanding of how the neural networks work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Cyclical Learning Rates for Training Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Cyclical-Learning-Rates-for-Training-Neural-Networks"/>
   <updated>2018-03-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Cyclical Learning Rates for Training Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Conventional wisdom says that when training neural networks, learning rate should monotonically decrease. This insight forms the basis of the different type of adaptive learning rates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also provides a simple approach to estimate the lower and upper bound for CLR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/bckenstler/CLR&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;intution&quot;&gt;Intution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Difficulty in minimizing the loss arises from saddle points and not from local minima. &lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;[Ref]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing the learning rate allows for rapid traversal of saddle points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameter-estimation&quot;&gt;Parameter Estimation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;step_size should be set to 2-10 times the number of iterations in an epoch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Estimating the CLR boundary values:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Run the model for several epochs while increasing the learning rate between the allowed low and high values.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This gives a good candidate value for upper and lower bound. Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound. But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.&lt;/li&gt;
  &lt;li&gt;The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Improving-Information-Extraction-by-Acquiring-External-Evidence-with-Reinforcement-Learning"/>
   <updated>2018-03-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Information Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes the use of Reinforcement Learning (RL) to solve this task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.07954&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/karthikncode/DeepRL-InformationExtraction&quot;&gt;Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-aspect&quot;&gt;Key Aspect&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use of Reinforcement Learning to resolve the ambiguity inherent in the textual documents.&lt;/li&gt;
  &lt;li&gt;Given a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine). It would further resolve and combine the result for the query from the set of retrieved documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Database of Mass Shootings in the United States.&lt;/li&gt;
  &lt;li&gt;Food Shield database of illegal food adulteration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Information extraction task is modelled as a Markov Decision Process (MDP) &amp;lt;S, A, T, R&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; - Set of all possible states
    &lt;ul&gt;
      &lt;li&gt;The state consists of:
        &lt;ul&gt;
          &lt;li&gt;Extractor’s confidence in predicted entity values.&lt;/li&gt;
          &lt;li&gt;Context from which values are extracted.&lt;/li&gt;
          &lt;li&gt;Similarity between the new document (extracted just now from the search engine) and the original document accompanying the given query.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; - Set of all possible actions
    &lt;ul&gt;
      &lt;li&gt;Reconciliation decision - d
        &lt;ul&gt;
          &lt;li&gt;Accept all entities values.&lt;/li&gt;
          &lt;li&gt;Reject all entities values.&lt;/li&gt;
          &lt;li&gt;Stop the current episode.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Query choice - q
        &lt;ul&gt;
          &lt;li&gt;Choose the next query from a set of automatically generated alternatives.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;R&lt;/strong&gt; - Rewards
    &lt;ul&gt;
      &lt;li&gt;Maximise the final extraction accuracy while minimising the number of queries.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; - Queries
    &lt;ul&gt;
      &lt;li&gt;Generated using a template.&lt;/li&gt;
      &lt;li&gt;The query is searched on a search engine and the top k links are retrieved.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transition&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Start with a single source article x&lt;sub&gt;i&lt;/sub&gt; and extract the initial set of entities.&lt;/li&gt;
      &lt;li&gt;At each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q). The episode stops whenever the action is a stop action.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep Q Network is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameters are learned using SGD and RMSProp.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt;

&lt;h3 id=&quot;extraction-model&quot;&gt;Extraction Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Max Entropy Classifier is used as the base extraction system.&lt;/li&gt;
  &lt;li&gt;First, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Extractors&lt;/li&gt;
  &lt;li&gt;Aggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values.&lt;/li&gt;
  &lt;li&gt;Meta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN.&lt;/li&gt;
  &lt;li&gt;Oracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rl-models&quot;&gt;RL Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RL Basic - Only reconciliation decision.&lt;/li&gt;
  &lt;li&gt;RL Query - Only query decision with a fixed reconciliation strategy.&lt;/li&gt;
  &lt;li&gt;RL Extract - the full system with both reconciliation and query decision.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RL Extract obtains substantial gains eg up to 11% over Maxnet.&lt;/li&gt;
  &lt;li&gt;Simple aggregation schemes do not handle the task well.&lt;/li&gt;
  &lt;li&gt;In terms of reward structure, providing rewards after each step works better than a single delayed reward.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/An-Empirical-Investigation-of-Catastrophic-Forgetting-in-Gradient-Based-Neural-Networks"/>
   <updated>2018-03-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Catastrophic Forgetting&lt;/em&gt; refers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper investigates this behaviour for different learning activations in presence and absence of dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6211&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/goodfeli/forgetting&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment-formulation&quot;&gt;Experiment Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For each experiment, two tasks are defined - “old” task and “new” task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The network is first trained on the “old” task until the validation set error has not improved for the last 100 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “best” performing model is then trained for the “new” task until the combined error on the “old” and the “new” validation datasets has not improved in the last 100 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the tasks used the same model architecture - 2 hidden layers followed by a softmax layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Following activations were tested:
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Hard Local Winner Takes It All&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models were trained using SGD with or without dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The authors took care to keep the hyperparams and other settings consistent and comparable across different experiments. Deviations, wherever applicable, and their reasons were documented.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of the relationship between the “old” and the “new” tasks, three kinds of settings are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The tasks are very very similar but the input is processed in a different format. For this setting, MNIST dataset was used with a different permutation of pixels for the “old” and the “new” task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The tasks are similar but not exactly the same. For this setting, the task was to predict sentiments of reviews across 2 different product categories.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In the last setting, 2 dissimilar tasks were used. One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Dropout improved the overall validation performance for all the models for all the tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting. It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting. The paper recommends performing cross-validation for the choice of the activation function. If that is not feasible, maxout activation function with dropout could be used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning an SAT Solver from Single-Bit Supervision</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-a-SAT-Solver-from-Single-Bit-Supervision"/>
   <updated>2018-02-24T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning a SAT Solver from Single-Bit Supervision</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved. As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.03685&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The expression itself can be written as a conjunction of disjunctions (“and” over “or”) where each conjunct is called a clause and each variable within a clause is called a literal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invariants&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The variables or clauses or literals (within the clauses) can be permuted.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Every occurrence of a variable can be negated.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Put an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform message passing between nodes to obtain vector representations corresponding to each node. Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings. Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After T iterations, the nodes vote to decide the prediction of the model as a whole.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is trained end-to-end using the cross-entropy loss between logit and the true label.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Permutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoding-satisfying-assignment&quot;&gt;Decoding Satisfying Assignment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the early iterations, all the nodes vote “unsolvable” with low confidence. Then a few nodes start voting “solvable” and then a phase transition happens where most of the nodes start voting “solvable” with high confidence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model never becomes highly confident that problem is “unsolvable” and almost never guesses “solvable” on an “unsolvable” problem. So in some sense, the model is looking for the combination of literals that actually solves the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Based on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The other strengths of the paper includes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Generalizing to longer and more difficult SAT problems (than those seen during training).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generalizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the “unsatisfiability” of an expression at test time. Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Neural Relational Inference for Interacting Systems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Relational-Inference-for-Interacting-Systems"/>
   <updated>2018-02-17T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Relational Inference for Interacting Systems</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For instance, consider a simulated system where the particles are connected to each other by springs. The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.04687&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ethanfetaya/nri&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model starts by assuming that a full connected interaction graph exists between the objects in the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For this latent graph &lt;strong&gt;z&lt;/strong&gt;, &lt;em&gt;z&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; denotes the (discrete) edge type between object &lt;em&gt;v&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; with the assumption that there are &lt;em&gt;K&lt;/em&gt; edge types.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The object &lt;em&gt;v&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; has a feature vector &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;t&lt;/sup&gt;&lt;/em&gt; associated with it at time &lt;em&gt;t&lt;/em&gt;. This feature vector captures information like location and velocity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A Graph Neural Network (GNN) acts on the fully connected latent graph &lt;em&gt;z&lt;/em&gt;, performs message passing from node to node via edges and predicts the discrete label for each edge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns &lt;em&gt;p&lt;sub&gt;θ&lt;/sub&gt;(x|z)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The overall model is a VAE that optimizes the ELBO given as:&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E&lt;sub&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/sub&gt;[log p&lt;sub&gt;θ&lt;/sub&gt;(x|z)] − KL[q&lt;sub&gt;φ&lt;/sub&gt;(z|x)||p&lt;sub&gt;θ&lt;/sub&gt;(z)]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;p&lt;sub&gt;θ&lt;/sub&gt;(x)&lt;/em&gt; is the prior which is assumed to be uniform distribution over the edge types.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future. This ensures that the interactions can have a significant effect on the dynamics of the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the dynamical system, run the encoder to obtain &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sample &lt;em&gt;z&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; from &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the decoder to predict the future dynamics for the next T timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimise the ELBO loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Stylistic Transfer in Natural Language Generation Systems Using Recurrent Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Stylistic-Transfer-in-Natural-Language-Generation-Systems-Using-Recurrent-Neural-Networks"/>
   <updated>2018-02-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Stylistic Transfer in Natural Language Generation Systems Using Recurrent Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclweb.org/anthology/W/W16/W16-6010.pdf&quot;&gt;This workshop paper&lt;/a&gt; explores the problem of style transfer in natural language generation (NLG).&lt;/li&gt;
  &lt;li&gt;One possible manifestation would be rewriting technical articles in an easy-to-understate manner.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identifying relevant stylistic cues and using them to control text generation in NLG systems.&lt;/li&gt;
  &lt;li&gt;Absence of a large amount of training data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pitch&quot;&gt;Pitch&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Using Recurrent Neural Networks (RNNs) to disentangle the style from semantic content.&lt;/li&gt;
  &lt;li&gt;Autoencoder model with two components - one for learning style and another for learning content.&lt;/li&gt;
  &lt;li&gt;This allows for “style” component to be replaced while keeping the “content” component same, resulting in a style transfer.&lt;/li&gt;
  &lt;li&gt;One way to think about this is - the encoder generates a 100-dimensional vector. In this, the first 50 entries, correspond to the “style” component and remaining to the “content” component.&lt;/li&gt;
  &lt;li&gt;The proposal is that the loss function should be modified to include a cross-covariance term for ensuring disentanglement.&lt;/li&gt;
  &lt;li&gt;I think one way of doing this is to have two loss functions:
    &lt;ul&gt;
      &lt;li&gt;The &lt;strong&gt;first loss&lt;/strong&gt; function ensures that the input sentence is decoded properly into the target sentence. This loss is computed for each sentence.&lt;/li&gt;
      &lt;li&gt;The &lt;strong&gt;second loss&lt;/strong&gt; ensures that the first 50 entries across all the encoded represenations are are correlated. This loss operates at the batch level.&lt;/li&gt;
      &lt;li&gt;The &lt;strong&gt;total loss&lt;/strong&gt; is the weighted sum of these 2 losses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-datasets&quot;&gt;Possible Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://norvig.com/ngrams/shakespeare.txt&quot;&gt;Complete works of Shakespeare&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/wikichallenge/data&quot;&gt;Wikpedia Kaggle dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ota.ox.ac.uk/&quot;&gt;Oxford Text Archive&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Twitter data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-metrics&quot;&gt;Possible Metrics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Soundness - is the generated text entailed with the input sentence.&lt;/li&gt;
  &lt;li&gt;Coherence - free of grammatical errors, proper word usage etc.&lt;/li&gt;
  &lt;li&gt;Effectiveness - how effective was the style transfer&lt;/li&gt;
  &lt;li&gt;Since some of the metrics are subjective, human evaluators also need to be employed.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Get To The Point - Summarization with Pointer-Generator Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"/>
   <updated>2018-02-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Get To The Point-Summarization with Pointer-Generator Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f&quot;&gt;Sequence-to-Sequence models&lt;/a&gt; have made abstract summarization viable but they still suffer from issues like &lt;em&gt;out of vocabulary&lt;/em&gt; words and repetitive sentences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a &lt;em&gt;coverage&lt;/em&gt; vector that keeps track of content that has already been summarized so as to discourage repetition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;h3 id=&quot;pointer-generator-network&quot;&gt;Pointer Generator Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is a hybrid model between the Sequence-to-Sequence network and &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Pointer-Networks&quot;&gt;Pointer Network&lt;/a&gt; such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the model can choose a word from the source vocabulary, the issue of &lt;em&gt;out of vocabulary&lt;/em&gt; words is handled.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;coverage-mechanism&quot;&gt;Coverage Mechanism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model maintains a &lt;em&gt;coverage&lt;/em&gt; vector which is the sum of attention distributions over all previous decoder timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This &lt;em&gt;coverage&lt;/em&gt; vector is fed as an input to the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;coverage loss&lt;/em&gt; is added to prevent the model from repeatedly attending to the same word.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to capture how much coverage different words have already received from the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observation&quot;&gt;Observation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is initially trained without coverage and then finetuned with the coverage loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the model first learns how to copy words and then how to generate words (p&lt;sup&gt;gen&lt;/sup&gt; starts from 0.3 and converges to 0.53).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, the model strongly prefers copying over generating (p&lt;sup&gt;gen&lt;/sup&gt; = 0.17).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The overall model is very simple, neat and interpretable and also performs well in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>StarSpace - Embed All The Things!</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/StarSpace-Embed-All-The-Things"/>
   <updated>2018-01-29T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/StarSpace - Embed All The Things</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A similarity function is learnt to compare these entities in a meaningful way and score their similarity. The definition of the similarity function could depend on the downstream task where the embeddings are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.03856&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/StarSpace&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each entity is described as a set of discrete features. For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked. For the search use case, the document may be described as a bag-of-words of words they are made up of.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a dataset and a task at hand, generate a set of positive samples &lt;em&gt;E = (a, b)&lt;/em&gt; such that &lt;em&gt;a&lt;/em&gt; is the input to the task (from the dataset) and &lt;em&gt;b&lt;/em&gt; is the expected label(answer/entity) for the given task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, generate another set of negative samples &lt;em&gt;E &lt;sup&gt;-&lt;/sup&gt; = (a, b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; such that &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;&lt;/em&gt; is one of the incorrect label(answer/entity) for the given task. The incorrect entity can be sampled randomly from the set of candidate entities. Multiple incorrect samples could be generated for each positive example. These incorrect samples are indexed using &lt;em&gt;i&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, in case of supervised learning problem like document classification, &lt;em&gt;a&lt;/em&gt; would be one of the documents (probably described in terms of words), &lt;em&gt;b&lt;/em&gt; is the correct label and &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; is one of the randomly sampled label from set of all the labels (excluding the correct label).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of collaborative filtering, &lt;em&gt;a&lt;/em&gt; would be the user (either described as a discrete entity like a userid or in terms of items purchased so far), &lt;em&gt;b&lt;/em&gt; is the next item the user purchases and &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; is one of the randomly sampled item from the set of all the items.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A similarity function is chosen to compare the representation of entities of type &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;. The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A loss function compares the similarity between positive pairs &lt;em&gt;(a, b)&lt;/em&gt; and &lt;em&gt;(a, b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt;. The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The norm of embeddings is capped at 1.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Emotional Chatting Machine - Emotional Conversation Generation with Internal and External Memory</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Emotional-Chatting-Machine-Emotional-Conversation-Generation-with-Internal-and-External-Memory"/>
   <updated>2018-01-22T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Emotional Chatting Machine-Emotional Conversation Generation with Internal and External Memory</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.01074&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Much of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words). This is the so-called seq2seq family of models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ECM model can sit within this framework and introduces 3 new components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Emotion Category Embedding&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Embed the emotion categories into a real-valued, low-dimensional vector space.&lt;/li&gt;
          &lt;li&gt;These embeddings are used as input to the decoder and are learnt along with rest of the model.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Internal Memory&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Physiological, emotional responses are relatively short-lived and involve changes.&lt;/li&gt;
          &lt;li&gt;ECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding.&lt;/li&gt;
          &lt;li&gt;It starts with “full” emotions in the beginning and keeps decaying the emotion value over time.&lt;/li&gt;
          &lt;li&gt;How much of the emotion value is to be decayed is determined by a sigmoid gate.&lt;/li&gt;
          &lt;li&gt;By the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;External Memory&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Emotional responses are expected to carry emotionally strong words along with generic, neutral words.&lt;/li&gt;
          &lt;li&gt;An external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies - &lt;em&gt;generic&lt;/em&gt; vocabulary and the &lt;em&gt;emotion&lt;/em&gt; vocabulary (read from the external memory).&lt;/li&gt;
          &lt;li&gt;Both these vocabularies are assigned different generation probabilities and an output gate controls the weights of &lt;em&gt;generic&lt;/em&gt; and &lt;em&gt;emotion&lt;/em&gt; words.&lt;/li&gt;
          &lt;li&gt;This way the &lt;em&gt;emotion&lt;/em&gt; words are included in an otherwise neutral response.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first component is the cross-entropy loss between predicted and target token distribution.&lt;/li&gt;
      &lt;li&gt;A regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process.&lt;/li&gt;
      &lt;li&gt;Another regularization term on external memory to supervise the probability of selection of a &lt;em&gt;generic&lt;/em&gt; vs &lt;em&gt;emotion&lt;/em&gt; word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;*Dataset&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;STC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier. Any error on the part of the classifier degrades the quality of the training dataset.&lt;/li&gt;
      &lt;li&gt;NLPCC Dataset - Emotion classification dataset with 23105 sentences.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Perplexity to evaluate the model at the content level.&lt;/li&gt;
      &lt;li&gt;Emotion accuracy to evaluate the model at the emotional level.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ECM achieves a perplexity of 65.9 and emotional accuracy of 0.773.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Based on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Notes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It is an interesting idea to let the sigmoid gate decide how the emotion “value” be spent while decoding. It seems similar to the idea of how much do we want to “attend” to the emotion value the key difference being that your total attention is limited. It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step. If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Exploring Models and Data for Image Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Exploring-Models-and-Data-for-Image-Question-Answering"/>
   <updated>2018-01-14T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Exploring Models and Data for Image Question Answering</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Problem Statement&lt;/strong&gt;: Given an image, answer a given question about the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1505.02074&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vis-lstm-model&quot;&gt;VIS-LSTM Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Treat the input image as the first word in the question.&lt;/li&gt;
  &lt;li&gt;Obtain the vector representation (skip-gram) for words in the question.&lt;/li&gt;
  &lt;li&gt;Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.&lt;/li&gt;
  &lt;li&gt;Keep image embedding frozen during training and use an LSTM to combine the word vectors.&lt;/li&gt;
  &lt;li&gt;LSTM outputs are fed into a softmax layer which generates the answer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DAtaset for QUestion Ansering on Real-world images (DAQUAR)
    &lt;ul&gt;
      &lt;li&gt;1300 images and 7000 questions with 37 object classes.&lt;/li&gt;
      &lt;li&gt;Downside is that even guess work can yield good results.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The paper proposed an algorithm for generating questions using MS-COCO dataset.
    &lt;ul&gt;
      &lt;li&gt;Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;object&lt;/em&gt; questions, &lt;em&gt;number&lt;/em&gt; questions, &lt;em&gt;colour&lt;/em&gt; questions and &lt;em&gt;location&lt;/em&gt; questions can be generated by searching for nouns, numbers, colours and prepositions respectively.&lt;/li&gt;
      &lt;li&gt;Resulting dataset has ~120K questions across above 4 semantic types.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;VIS+LSTM - explained above&lt;/li&gt;
  &lt;li&gt;2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM&lt;/li&gt;
  &lt;li&gt;IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).&lt;/li&gt;
  &lt;li&gt;FULL - Simple average of above 2 models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.&lt;/li&gt;
  &lt;li&gt;Also includes a KNN model where the system finds the nearest (image, question) pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Accuracy&lt;/li&gt;
  &lt;li&gt;Wu-Palmer similarity measure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.&lt;/li&gt;
  &lt;li&gt;Some useful information seems to be lost when downsizing the VGG vectors.&lt;/li&gt;
  &lt;li&gt;Fine tuning the word vectors helps with performance.&lt;/li&gt;
  &lt;li&gt;Normalising CNN hidden image features into zero mean and unit variance leads to faster training.&lt;/li&gt;
  &lt;li&gt;Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How transferable are features in deep neural networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/How-transferable-are-features-in-deep-neural-networks"/>
   <updated>2018-01-06T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/How transferable are features in deep neural networks</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs). The first layer features are “general” irrespective of the task/optimizer etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final layer features tend to be “specific” in the sense that they strongly depend on the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies the transition of generalization property across layers in the network. This could be useful in the domain of transfer learning where features are reused across tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;setup&quot;&gt;Setup&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Degree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B). Each group has 500 classes and half the total number of examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now choose a layer numbered n from {1, 2…7}.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each layer n, train the following two networks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Selffer Network BnB&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Copy (and freeze) first n layers from baseB. The remaining layers are initialized randomly and trained on B.&lt;/li&gt;
          &lt;li&gt;This serves as the control group.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Transfer Network AnB&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Copy (and freeze) first n layers from baseA. The remaining layers are initialized randomly and trained on B.&lt;/li&gt;
          &lt;li&gt;This corresponds to transferring features from A to B.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If AnB performs well, n&lt;sup&gt;th&lt;/sup&gt; layer features are “general”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another setting, the transferred layers are also fine-tuned (BnB&lt;sup&gt;+&lt;/sup&gt; and AnB&lt;sup&gt;+&lt;/sup&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;observation&quot;&gt;Observation&lt;/h1&gt;

&lt;h2 id=&quot;dataset-a-and-b-are-similar&quot;&gt;Dataset A and B are similar&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For n = {1, 2}, the performance of the BnB model is same as baseB model. For n = {3, 4, 5, 6}, the performance of BnB model is worse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This indicates the presence of “fragile co-adaption” features on successive layers where features interact with each other in a complex way and can not be easily separated across layers. This is more prominent across middle layers and less across the first and the last layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For model AnB, the performance of baseB for n = {1, 2}. Beyond that, the performance begins to drop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transfer learning of features followed by fine-tuning gives better results than training the network from scratch.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-a-and-b-are-dissimilar&quot;&gt;Dataset A and B are dissimilar&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Effectiveness of feature transfer decreases as the two tasks become less similar.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;random-weights&quot;&gt;Random Weights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of using transferred weights in BnB and BnA, the first n layers were initialized randomly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The performance falls for layer 1 and 2. It further drops to near-random level for layers 3 and beyond.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another interesting insight is that even for dissimilar tasks, transferring features is better than using random features.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Distilling the Knowledge in a Neural Network</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network"/>
   <updated>2017-12-31T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Distilling the Knowledge in a Neural Network</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to transfer the knowledge from such “cumbersome” models into a single, “simpler” model which is more suitable for deployment. This transfer of knowledge is referred to as “distillation”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;idea&quot;&gt;Idea&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train the cumbersome model using the given training data in the usual way.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target. Thus, the simpler model is trained to generalise the same way as the cumbersome model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model. This approach was pursued by &lt;a href=&quot;https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf&quot;&gt;Buciluǎ et al.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a more general solution which they name “distillation”. The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer). These soft targets are then used to train the simpler model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also shows that the proposed approach is, in fact, a more general case of the first approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model. The temperature is set to 1 when making predictions using the simpler model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels. The second objective function should be given a much lower weight though. Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports favourable results for distillation task for the following domains:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Image Classification (on MNIST dataset)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;An extra experiment is performed where the simpler model is not shown any images of “3” but the model fails for only 133 cases out of 1010 cases involving “3”.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Automatic Speech Recognition (ASR)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;An extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively. Further, only 3% of the total dataset is used.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy. This shows the regularizing effect of soft targets.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Training ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>PTE - Predictive Text Embedding through Large-scale Heterogeneous Text Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/PTE-Predictive-Text-Embedding-through-Large-scale-Heterogeneous-Text-Networks"/>
   <updated>2017-12-24T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/PTE - Predictive Text Embedding through Large-scale Heterogeneous Text Networks</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised text embeddings can be generalized for different tasks but they have weaker predictive powers (as compared to end-to-end trained deep learning methods) for any particular task. But the deep learning techniques are expensive and need a large amount of supervised data and a large number of parameters to tune.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces Predictive Text Embedding (PTE) - a semi-supervised approach which learns an effective low dimensional representation using a large amount of unsupervised data and a small amount of supervised data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The work can be extended to general information networks as well as classic techniques like MDS, Iso-map, Laplacian EigenMaps etc do not scale well for large graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, this model can be applied to heterogeneous networks as well unlike the previous works &lt;a href=&quot;https://arxiv.org/abs/1503.03578&quot;&gt;LINE&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1403.6652&quot;&gt;DeepWalk&lt;/a&gt; which work on homogeneous networks only.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.00200&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes 3 different kinds of networks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Word-Word Network&lt;/strong&gt; which captures the word co-occurrence information (local level).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Word-Document Network&lt;/strong&gt; which captures the word-document co-occurrence information (local + document level).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Word-Label Network&lt;/strong&gt; which captures the word-label co-occurrence information (bipartite graph).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All 3 graphs are integrated into one heterogeneous text network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First, the authors extend their previous work, LINE, for heterogenous bipartite text networks as explained:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given a bipartite graph &lt;em&gt;G = (V&lt;sub&gt;A&lt;/sub&gt; \bigcup V&lt;sub&gt;B&lt;/sub&gt;, E)&lt;/em&gt; , where &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt; and V&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; are disjoint set of vertices, the conditional probability of &lt;em&gt;v&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; (in set &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt;&lt;/em&gt;) being generated by &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; (in set &lt;em&gt;V&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;) is given as the softmax score between embeddings of &lt;em&gt;v&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; and normalised by the sum of exponentials of dot products between &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt;  and all nodes in &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;The second order proximity can be determined by the conditional distributions *p(.&lt;/td&gt;
              &lt;td&gt;v&lt;sub&gt;j&lt;/sub&gt;)*p(.&lt;/td&gt;
              &lt;td&gt;v&lt;sub&gt;j&lt;/sub&gt;)*.&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The objective to be minimised the KL divergence between the conditional distribution &lt;em&gt;p(.\v&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; and the emperical distribution &lt;em&gt;p&lt;sup&gt;^&lt;/sup&gt;(.\v&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; (given as w&lt;sub&gt;i, j&lt;/sub&gt;/deg&lt;sub&gt;j&lt;/sub&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;The objective can be further simplified and optimised using SGD with edge sampling and negative sampling.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, the 3 individual networks can all be interpreted as bipartite networks. So node representation of all the 3 individual networks is obtained as described above.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the word-label network, since the training data is sparse, one could either train the unlabelled networks first and then the labelled network or they all could be trained jointly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the case of joint training, the edges are sampled from the 3 networks alternatively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the fine-tuning case, the edges are first sampled from the unlabelled network and then from the labelled network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once the word embeddings are obtained, the text embeddings may be obtained by simply averaging the word embeddings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Baseline Models&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Local word co-occurence based methods - SkipGram, LINE(Gww)&lt;/li&gt;
      &lt;li&gt;Document word co-occurence based methods - LINE(Gwd), PV-DBOW&lt;/li&gt;
      &lt;li&gt;Combined method - LINE (Gww + Gwd)&lt;/li&gt;
      &lt;li&gt;CNN&lt;/li&gt;
      &lt;li&gt;PTE&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For long documents, PTE (joint) outperforms CNN and other PTE variants and is around 10 times faster than CNN model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For short documents, PTE (joint) does not always outperform CNN model probably because the word sense ambiguity is more relevant in the short documents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Revisiting Semi-Supervised Learning with Graph Embeddings</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Revisiting-Semi-Supervised-Learning-with-Graph-Embeddings"/>
   <updated>2017-12-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Revisiting Semi-Supervised Learning with Graph Embeddings</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context. Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is called &lt;a href=&quot;https://github.com/kimiyoung/planetoid&quot;&gt;Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.08861&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph G = (V, E) and x&lt;sub&gt;L&lt;/sub&gt; and x&lt;sub&gt;U&lt;/sub&gt; as feature vectors for labelled and unlabelled nodes and y&lt;sub&gt;L&lt;/sub&gt; as labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -&amp;gt; y&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two settings possible:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Transductive&lt;/strong&gt; - Predictions are made only for those nodes which are already observed in the graph at training time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Inductive&lt;/strong&gt; - Predictions are made for nodes whether they have been observed in the graph at training time or not.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The general semi-supervised learning loss would be &lt;em&gt;L&lt;sub&gt;S&lt;/sub&gt; + λL&lt;sub&gt;U&lt;/sub&gt;&lt;/em&gt; where &lt;em&gt;L&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; is the supervised learning loss while &lt;em&gt;L&lt;sub&gt;U&lt;/sub&gt;&lt;/em&gt; is the unsupervised learning loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The unsupervised loss is a variant of the Skip-gram loss with negative edge sampling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, first a random walk sequence S is sampled. Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transductive-formulation&quot;&gt;Transductive Formulation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two separate fully connected networks are applied over the node features and node embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These 2 representations are then concatenated and fed to a softmax classifier to predict the class label.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inductive-formulation&quot;&gt;Inductive Formulation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the inductive setting, it is difficult to obtain the node embeddings at test time. One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This provides a simple way to extend the original approach to the inductive setting.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension"/>
   <updated>2017-11-28T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is the following:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;We have a domain D&lt;sub&gt;S&lt;/sub&gt; for which we have labelled dataset of question-answer pairs and another domain D&lt;sub&gt;T&lt;/sub&gt; for which we do not have any labelled dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We use the data for domain D&lt;sub&gt;S&lt;/sub&gt; to train SynNet and use that to generate synthetic question-answer pairs for domain D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Now we can train a machine comprehension model M on D&lt;sub&gt;S&lt;/sub&gt; and finetune using the synthetic data for D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/two-stage-synthesis-networks-transfer-learning-machine-comprehension/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;synnet&quot;&gt;SynNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Works in two stages:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Answer Synthesis - Given a text paragraph, generate an answer.&lt;/li&gt;
      &lt;li&gt;Question Synthesis - Given a text paragraph and an answer, generate a question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;answer-synthesis-network&quot;&gt;Answer Synthesis Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given the labelled dataset for D&lt;sub&gt;S&lt;/sub&gt;, generate a labelled dataset of &amp;lt;word, tag&amp;gt; pair such that each word in the given paragraph is assigned one of the 4 tags:
    &lt;ul&gt;
      &lt;li&gt;IOB&lt;sub&gt;start&lt;/sub&gt; - if it is the starting word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;mid&lt;/sub&gt; - if it is the intermediate word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;end&lt;/sub&gt; - if it is the ending word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;none&lt;/sub&gt; - if it is not part of any answer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For training, map the words to their GloVe embeddings and pass through a Bi-LSTM. Next, pass them through two-FC layers followed by a softmax layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For the target domain D&lt;sub&gt;T&lt;/sub&gt;, all the consecutive word spans where no label is IOB&lt;sub&gt;none&lt;/sub&gt; are returned as candidate answers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;question-synthesis-network&quot;&gt;Question Synthesis Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Map each word in the paragraph to their GloVe embedding. After the word vector, append a ‘1’ if the word was part of the candidate answer else append a ‘0’.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far. Decoding is stopped when “END” token is produced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary. To account for such words, a copying mechanism is also incorporated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At each time step, a Pointer Network (C&lt;sub&gt;P&lt;/sub&gt;) and a Vocabulary Predictor (V&lt;sub&gt;P&lt;/sub&gt;) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;machine-comprehension-model&quot;&gt;Machine Comprehension Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given any MC model, first train it over domain D&lt;sub&gt;S&lt;/sub&gt; and then fine-tune using the artificial questions generated using D&lt;sub&gt;T&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Regularization&lt;/strong&gt; - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Checkpoint Averaging&lt;/strong&gt; - Use the different checkpointed models to average the answer likelihood before running DP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -&amp;gt; NewsQA). Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Higher-order organization of complex networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Higher-order-organization-of-complex-networks"/>
   <updated>2017-11-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Higher-order organization of complex networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://science.sciencemag.org/content/353/6295/163&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Network-Motifs-Simple-Building-Blocks-of-Complex-Networks&quot;&gt;motif M&lt;/a&gt;, the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mathematically, the aim is to minimise the motif conductance metric given as &lt;em&gt;cut&lt;sub&gt;M&lt;/sub&gt;(S, S’) / min[vol&lt;sub&gt;M&lt;/sub&gt;(S), vol&lt;sub&gt;M&lt;/sub&gt;(S’)]&lt;/em&gt; where &lt;em&gt;S’&lt;/em&gt; is complement of &lt;em&gt;S&lt;/em&gt;, &lt;em&gt;cut&lt;sub&gt;M&lt;/sub&gt;(S, S’)&lt;/em&gt; = number of instances of M which have atleast one node from both &lt;em&gt;S&lt;/em&gt; and &lt;em&gt;S’&lt;/em&gt; and &lt;em&gt;vol&lt;sub&gt;M&lt;/sub&gt;(S)&lt;/em&gt; = Number of nodes in instances of M that belong only to S.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the network and motif M, form a motif adjacency matrix W&lt;sub&gt;M&lt;/sub&gt; where W&lt;sub&gt;M&lt;/sub&gt;(i, j) is the number of instances of M that contains i and j.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute spectral ordering of the nodes from normalized motif laplacian matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute prefix set of spectral ordering with small motif conductance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Worst case &lt;em&gt;O(m&lt;sup&gt;1.5&lt;/sup&gt;)&lt;/em&gt;, based on experiments &lt;em&gt;O(m&lt;sup&gt;1.2&lt;/sup&gt;)&lt;/em&gt; where &lt;em&gt;m&lt;/em&gt; is the number of edges.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case the motif is not known beforehand, the framework can be used to compute significant motifs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Network Motifs - Simple Building Blocks of Complex Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Network-Motifs-Simple-Building-Blocks-of-Complex-Networks"/>
   <updated>2017-11-12T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Network Motifs-Simple Building Blocks of Complex Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper presents the concept of “network motifs” to understand the structural design of a network or a graph.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://science.sciencemag.org/content/298/5594/824&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A network motif is defined as “a pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Real-life networks exhibit properties like “small world” property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and “scale-free” property (fraction of nodes having k edges decays as a power-law).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering. Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Word Representations via Gaussian Embedding</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Word-Representations-via-Gaussian-Embedding"/>
   <updated>2017-11-05T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Word Representations via Gaussian Embedding</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Existing word embedding models like &lt;a href=&quot;https://gist.github.com/shagunsodhani/176a283e2c158a75a0a6&quot;&gt;Skip-Gram&lt;/a&gt;, &lt;a href=&quot;https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8&quot;&gt;GloVe&lt;/a&gt; etc map words to fixed sized vectors in a low dimensional vector space.&lt;/li&gt;
  &lt;li&gt;This fixed point setting cannot capture uncertainty about representation.&lt;/li&gt;
  &lt;li&gt;Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.&lt;/li&gt;
  &lt;li&gt;The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.&lt;/li&gt;
  &lt;li&gt;This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6623&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/seomoz/word2gauss&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;KL divergence is used as the asymmetric distance function for comparing the distributions.&lt;/li&gt;
  &lt;li&gt;Unlike the Word2Vec model, the proposed model uses ranking-based loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;similarity-measures-used&quot;&gt;Similarity Measures used&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Symmetric Similarity&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For two gaussian distributions, &lt;em&gt;P&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;P&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;, compute the inner product &lt;em&gt;E(P&lt;sub&gt;i&lt;/sub&gt;, P&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; as &lt;em&gt;N(0; mean&lt;sub&gt;i&lt;/sub&gt; - mean&lt;sub&gt;j&lt;/sub&gt;, sigma&lt;sub&gt;i&lt;/sub&gt; + sigma&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Compute the gradient of &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;sigma&lt;/em&gt; with respect to &lt;em&gt;log(E)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Asymmetric Similarity&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Use KL divergence to encode the context distribution.&lt;/li&gt;
  &lt;li&gt;The benefit over the symmetric setting is that now entailment type relations can also be modeled.&lt;/li&gt;
  &lt;li&gt;For example, a low KL divergence from x to y indicates that y can be encoded as x or that y “entails” x.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning&quot;&gt;Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;One of the two notions of similarity is chosen and max-margin is used as the loss function.&lt;/li&gt;
  &lt;li&gt;Mean is regularized by adding a simple constraint on the L2-norm.&lt;/li&gt;
  &lt;li&gt;For covariance matrix, the eigenvalues are constrained to lie within a hypercube. This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Polysemous words have higher variance in their word embeddings as compared to specific words.&lt;/li&gt;
  &lt;li&gt;KL divergence (with diagonal covariance) outperforms other models.&lt;/li&gt;
  &lt;li&gt;Simple tree hierarchies can also be modeled by embedding into the Gaussian space. A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.&lt;/li&gt;
  &lt;li&gt;For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use combinations of low rank and diagonal matrices for covariances.&lt;/li&gt;
  &lt;li&gt;Improved optimisation strategies.&lt;/li&gt;
  &lt;li&gt;Trying other distributions like Student’s-t distribution.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>HARP - Hierarchical Representation Learning for Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/HARP-Hierarchical-Representation-Learning-for-Networks"/>
   <updated>2017-10-28T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/HARP - Hierarchical Representation Learning for Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.07845&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph &lt;em&gt;G = (V, E)&lt;/em&gt;, compute a series of successively smaller (coarse) graphs &lt;em&gt;G&lt;sub&gt;0&lt;/sub&gt;, …, G&lt;sub&gt;L&lt;/sub&gt;&lt;/em&gt;. Learn the node representations in &lt;em&gt;G&lt;sub&gt;L&lt;/sub&gt;&lt;/em&gt; and successively refine the embeddings for larger graphs in the series.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The architecture is independent of the algorithms used to embed the nodes or to refine the node representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graph coarsening technique that preserves global structure&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Collapse edges and stars to preserve first and second order proximity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Edge collapsing&lt;/strong&gt; - select the subset of &lt;em&gt;E&lt;/em&gt; such that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Star collapsing&lt;/strong&gt; - given star structure, collapse the pairs of neighboring nodes (of the central node).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In practice, first apply star collapsing, followed by edge collapsing.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Extending node representation from coarse graph to finer graph&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Lets say &lt;em&gt;node1&lt;/em&gt; and &lt;em&gt;node2&lt;/em&gt; were merged into &lt;em&gt;node12&lt;/em&gt; during coarsening. First copy the representation of &lt;em&gt;node12&lt;/em&gt; into &lt;em&gt;node1&lt;/em&gt;, &lt;em&gt;node2&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Additionally, if hierarchical softmax was used, extend the B-tree such that &lt;em&gt;node12&lt;/em&gt; is replaced by 2 child nodes &lt;em&gt;node1&lt;/em&gt; and &lt;em&gt;node2&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Time complexity for HARP + DeepWalk is &lt;em&gt;O(number of walks * |V|)&lt;/em&gt; while for HARP + LINE is &lt;em&gt;O(number of iterations * |E|)&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The asymptotic complexity remains the same as the HARP-less version for the two cases.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Swish - a Self-Gated Activation Function</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Swish-A-self-gated-activation-function"/>
   <updated>2017-10-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Swish-A self gated activation function</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a new activation function called Swish with formulation &lt;em&gt;f(x) = x.sigmod(x)&lt;/em&gt; and its parameterised version called Swish-β where &lt;em&gt;f(x, β) = 2x.sigmoid(β.x)&lt;/em&gt; and β is a training parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.05941&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;properties-of-swish&quot;&gt;Properties of Swish&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/Swish/plot.png&quot; alt=&quot;Plot Of Swish&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smooth, non-monotonic function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Swish-β can be thought of as a smooth function that interpolates between a linear function and RELU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Uses self-gating mechanism (that is, it uses its own value to gate itself). Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Being unbounded on the x&amp;gt;0 side, it avoids saturation when training is slow due to near 0 gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the Swish function is smooth, the output landscape and the loss landscape are also smooth. A smooth landscape should be more traversable and less sensitive to initialization and learning rates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;criticism&quot;&gt;Criticism&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Reading Wikipedia to Answer Open-Domain Questions</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Reading-Wikipedia-to-Answer-Open-Domain-Questions"/>
   <updated>2017-10-15T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Reading Wikipedia to Answer Open-Domain Questions</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.00051&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unique-aspects-of-the-dataset&quot;&gt;Unique Aspects of the dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Existing machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans). MARCO questions are sampled from real, anonymized user queries.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most datasets would provide a comparatively small and clean context to answer the question. In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents. As such the questions and the context documents are noisy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the answer to the questions are restricted to an entity or text span within the document. In case of MARCO, the human judges are encouraged to generate complete sentences as answers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-description&quot;&gt;Dataset Description&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First release consists of 100K questions with the aim of releasing 1M questions in the future releases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All questions are tagged with segment information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A subset of questions has multiple answers and another subset has no answers at all.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each record in the dataset contains the following information:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Query&lt;/strong&gt; - The actual question&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Passage&lt;/strong&gt; - Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Document URLs&lt;/strong&gt; - URLs for the top documents (which are the source of the contextual passages).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Answer&lt;/strong&gt; - Answer synthesised by human evaluators.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Segment&lt;/strong&gt; - Query type, description, neumeric, entity, location, person.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Accuracy and precision/recall for numeric questions&lt;/li&gt;
      &lt;li&gt;ROGUE-L/paraphrasing aware evaluation framework for long, textual answers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Among generative models, Memory Networks performed better than seq-to-seq.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the cloze-style test, &lt;a href=&quot;https://arxiv.org/abs/1609.05284&quot;&gt;ReasoNet&lt;/a&gt; achieved an accuracy of approx. 59% while &lt;a href=&quot;ASR&quot;&gt;Attention Sum Reader&lt;/a&gt; achieved an accuracy of approx 55%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Current QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Imagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy. Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition. Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Task-Oriented Query Reformulation with Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Task-Oriented-Query-Reformulation-with-Reinforcement-Learning"/>
   <updated>2017-10-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Task-Oriented Query Reformulation with Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper introduces a query reformulation system that rewrites a query to maximise the number of “relevant” documents that are extracted from a given black box search engine.&lt;/li&gt;
  &lt;li&gt;A Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04572&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nyu-dl/QueryReformulator&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-aspect&quot;&gt;Key Aspect&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval. This means relevant documents could be missed if there is no exactly matching words between the query and the document.&lt;/li&gt;
  &lt;li&gt;This problem can be handled at two levels: First, the search engine itself takes care of query semantics. Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation).&lt;/li&gt;
  &lt;li&gt;The paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;TREC - Complex Answer Retrieval (TREC-CAR)&lt;/li&gt;
  &lt;li&gt;Jeopardy Q&amp;amp;A dataset&lt;/li&gt;
  &lt;li&gt;Microsoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Query Reformulation task is modeled as an RL problem where:
    &lt;ul&gt;
      &lt;li&gt;Environment is the search engine.&lt;/li&gt;
      &lt;li&gt;Actions are whether a word is to be added to the query or not and if yes, then what word is added.&lt;/li&gt;
      &lt;li&gt;Reward is the retrieval accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The input to the system is a query q&lt;sub&gt;0&lt;/sub&gt; consisting of a sequence of words w&lt;sub&gt;1&lt;/sub&gt;, …, w&lt;sub&gt;n&lt;/sub&gt; and a candidate term t&lt;sub&gt;i&lt;/sub&gt; with some context words.&lt;/li&gt;
  &lt;li&gt;Candidate terms are all the terms that appear in the original query and the documents retrieved using the query.&lt;/li&gt;
  &lt;li&gt;The words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN’s or RNNs.&lt;/li&gt;
  &lt;li&gt;Similarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs.&lt;/li&gt;
  &lt;li&gt;Finally, a sigmoidal score is computed for all the candidate words.&lt;/li&gt;
  &lt;li&gt;An RNN sequentially applies this model to emit query words till an end token is emitted.&lt;/li&gt;
  &lt;li&gt;Vocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The model is trained using REINFORCE algorithm which minimizes the &lt;em&gt;C&lt;sub&gt;a&lt;/sub&gt; = (R − R~) * sum(log(P(t|q))) where R~ is the baseline.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Value network minimises &lt;em&gt;C&lt;sub&gt;b&lt;/sub&gt; = &amp;amp;\alpha(||R-R~||&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;C&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;C&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; are minimised using SGD.&lt;/li&gt;
  &lt;li&gt;An entropy regulation term is added to prevent the probability distribution from reaching the peak.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;baseline-methods&quot;&gt;Baseline Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Raw&lt;/strong&gt; - Original query is fed to the search engine without any modification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pseudo-Relevance Feedback (PRF-TFIDF)&lt;/strong&gt; - The query is expanded using the top-N TF-IDF terms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PRF-Relevance Model (PRF-RM)&lt;/strong&gt; - Probability of adding token &lt;em&gt;t&lt;/em&gt; to the query &lt;em&gt;q0&lt;/em&gt; is given by &lt;em&gt;P(t|q0) = (1 − λ)P′(t|q0) + λ sum (P(d)P(t|d)P(q0|d))&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-methods&quot;&gt;Proposed Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Assumes that the query words contribute indepently to the query retrival performace. (Too strong an assumption).&lt;/li&gt;
      &lt;li&gt;A term is marked as relevant if &lt;em&gt;(R(new_query) - R(old_query))/R(old_query) &amp;gt; 0.005&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;RL-RNN/CNN - RL Framework + RNN/CNN to encode the input features.&lt;/li&gt;
      &lt;li&gt;RL-RNN-SEQ - Add a sequential generator.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall@K&lt;/li&gt;
      &lt;li&gt;Precision@K&lt;/li&gt;
      &lt;li&gt;Mean Average Precision@K&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt; - The paper uses Recall@K as a reward when training the RL-based models with the argument that the “metric has shown to be effective in improving the other metrics as well”, without any justification though.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SL-Oracle&lt;/strong&gt; - classifier that perfectly selects terms that will increase performance based on the supervised learning approach.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RL-Oracle&lt;/strong&gt; - Produces a conservative upper-bound for the performance of the RL Agent. It splits the test data into N subsets and trains an RL agent for each subset. Then, the reward is averaged over all the N subsets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Reformulation based methods &amp;gt; original query&lt;/li&gt;
  &lt;li&gt;RL methods &amp;gt; Supervised methods &amp;gt; unsupervised methods&lt;/li&gt;
  &lt;li&gt;RL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries).&lt;/li&gt;
  &lt;li&gt;RL-based model benefits from more candidate terms while the classical PRF method quickly saturates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Interestingly, for each raw query, they carried out the reformulation step just once and not multiple times. The number of times a query is reformulated could also have become a part of the RL framework.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Refining Source Representations with Relation Networks for Neural Machine Translation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Refining-Source-Representations-with-Relation-Networks-for-Neural-Machine-Translation"/>
   <updated>2017-09-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Refining Source Representations with Relation Networks for Neural Machine Translation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).&lt;/li&gt;
  &lt;li&gt;This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.03980&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations-of-existing-nmt-models&quot;&gt;Limitations of existing NMT models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information.&lt;/li&gt;
  &lt;li&gt;In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.&lt;/li&gt;
  &lt;li&gt;While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Learn the relationship between the source words using the context (neighboring words).&lt;/li&gt;
  &lt;li&gt;Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-network&quot;&gt;Relation Network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Neural network which is desgined for relational reasoning.&lt;/li&gt;
  &lt;li&gt;Given a set of inputs * O = o&lt;sub&gt;1&lt;/sub&gt;, …, o&lt;sub&gt;n&lt;/sub&gt; *, RN is formed as a composition of inputs:
   RN(O) = f(sum(g(o&lt;sub&gt;i&lt;/sub&gt;, o&lt;sub&gt;j&lt;/sub&gt;))), f and g are functions used to learn the relations (feed forward networks)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;g&lt;/em&gt; learns how the objects are related hence the name “relation”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;CNN Layer
        &lt;ul&gt;
          &lt;li&gt;Extract information from the words surrounding the given word (context).&lt;/li&gt;
          &lt;li&gt;The final output of this layer is the sequence of vectors for different kernel width.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Graph Propagation (GP) Layer
        &lt;ul&gt;
          &lt;li&gt;Connect all the words with each other in the form of a graph.&lt;/li&gt;
          &lt;li&gt;Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.&lt;/li&gt;
          &lt;li&gt;The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Multi-Layer Perceptron (MLP) Layer
        &lt;ul&gt;
          &lt;li&gt;The representation from the GP Layer is fed to the MLP layer.&lt;/li&gt;
          &lt;li&gt;The layer uses residual connections from previous layers in form of concatenation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;IWSLT Data - 44K sentences from tourism and travel domain.&lt;/li&gt;
  &lt;li&gt;NIST Data - 1M Chinese-English parallel sentence pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MOSES - Open source translation system - http://www.statmt.org/moses/&lt;/li&gt;
  &lt;li&gt;NMT - Attention based NMT&lt;/li&gt;
  &lt;li&gt;NMT+ - NMT with improved decoder&lt;/li&gt;
  &lt;li&gt;TRANSFORMER - Google’s new NMT&lt;/li&gt;
  &lt;li&gt;RNMT+ - Relation Network integrated with NMT+&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation-metric&quot;&gt;Evaluation Metric&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;case-insensitive 4-gram BLEU score&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;As sentences become larger (more than 50 words), RNMT clearly outperforms other baselines.&lt;/li&gt;
  &lt;li&gt;Qualitative evaluation shows that RNMT+ model captures the word alignment better than the NMT+ models.&lt;/li&gt;
  &lt;li&gt;Similarly, NMT+ system tends to miss some information from the source sentence (more so for longer sentences). While both CNNs and RNNs are weak at capturing long-term dependency, using the relation layer mitigates this issue to some extent.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Pointer Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Pointer-Networks"/>
   <updated>2017-08-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Pointer Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Such a problem can not be solved using &lt;a href=&quot;https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f&quot;&gt;Seq2Seq&lt;/a&gt; or Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Traditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector. This attention vector is used to compute a fixed size softmax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So Pointer Net is a very simple modification of the attention model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;application&quot;&gt;Application&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Any problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;eg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers the following 3 problems:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Convex Hull&lt;/li&gt;
      &lt;li&gt;Delaunay triangulations&lt;/li&gt;
      &lt;li&gt;Travelling Salesman Problem (TSP)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, the order in which the inputs are fed to the system affects its performance. The authors discussed this apsect in their subsequent paper titled &lt;a href=&quot;https://arxiv.org/pdf/1511.06391v4.pdf&quot;&gt;Order Matters: Sequence To Sequence for Sets&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning to Compute Word Embeddings On the Fly</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-to-Compute-Word-Embeddings-On-the-Fly"/>
   <updated>2017-08-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning to Compute Word Embeddings On the Fly</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning representations for OOV words directly on the end task often results in poor representation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a rare word &lt;em&gt;w&lt;/em&gt;, let &lt;em&gt;d(w) = &amp;lt;x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;…&amp;gt;&lt;/em&gt; denote its defination where &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; are words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;d(w)&lt;/em&gt; is fed to a &lt;em&gt;defination reader&lt;/em&gt; network &lt;em&gt;f&lt;/em&gt; (LSTM) and its last state is used as the &lt;em&gt;defination embedding e&lt;sub&gt;d&lt;/sub&gt;(w)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case &lt;em&gt;w&lt;/em&gt; has multiple definitions, the embeddings are combined using mean pooling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach can be extended to in-vocabulary words as well by using the &lt;em&gt;definition embedding&lt;/em&gt; of such words to update their original embeddings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Auxiliary data sources
    &lt;ul&gt;
      &lt;li&gt;Word definitions from WordNet&lt;/li&gt;
      &lt;li&gt;Spelling of words&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach was tested on following tasks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Extractive Question Answering over SQuAD
        &lt;ul&gt;
          &lt;li&gt;Base model from &lt;a href=&quot;https://arxiv.org/abs/1611.01604&quot;&gt;Xiong et al. 2016&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Entailment Prediction over SNLI corpus
        &lt;ul&gt;
          &lt;li&gt;Base models from &lt;a href=&quot;https://nlp.stanford.edu/pubs/snli_paper.pdf&quot;&gt;Bowman et al. 2015&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1609.06038&quot;&gt;Chen et al. 2016&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;One Billion Words Language Modelling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-token words like “San Francisco” are not accounted for now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model does not handle the rare words which appear in the definition and just replaces them by the &lt;UNK&gt; token. Making the model recursive would be a useful addition.&lt;/UNK&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>R-NET - Machine Reading Comprehension with Self-matching Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/R-NET-Machine-Reading-Comprehension-with-Self-matching-Networks"/>
   <updated>2017-08-07T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/R-NET - Machine Reading Comprehension with Self-matching Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;R-NET is an end-to-end trained neural network model for machine comprehension.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lastly, it uses pointer networks to determine the position of the answer in the passage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/mrc/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MS-MARCO&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Question / Passage Encoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Concatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gated Attention based RNN&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Self Matching Attention&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The passage representation obtained so far would not capture most of the context.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;So the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output Layer&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Use pointer network (initialized using attention pooling over answer representation) to predict the position of the answer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Loss function is the sum of negative log probabilities of start and end positions.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;R-NET is ranked second on &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD Leaderboard&lt;/a&gt; as of 7th August, 2017 and achieves best-published results on MS-MARCO dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>ReasoNet - Learning to Stop Reading in Machine Comprehension</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/ReasoNet-Learning-to-Stop-Reading-in-Machine-Comprehension"/>
   <updated>2017-07-24T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/ReasoNet - Learning to Stop Reading in Machine Comprehension</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached. If termination state is reached, the answer module is triggered to generate the answer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the termination state is discrete and not connected to the final output, RL approach is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.05284&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNN, DailyMail Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Reachability Dataset&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;2 synthetic datasets to test if the network can answer questions like “Is node_1 connected to node_12”?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Memory (M)&lt;/strong&gt; - Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; - Attention vector (&lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;) is a function of current internal state &lt;strong&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; and external memory &lt;strong&gt;M&lt;/strong&gt;. The state and memory are passed through FCs and fed to a similarity function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Internal State (s&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; - Vector representation of the question state computed by a RNN using the previous internal state and the attention vector &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Termination Gate (T&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; - Uses a logistic regression model to generate a random binary variable using the current internal state &lt;strong&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Answer&lt;/strong&gt; - Answer module is triggered when &lt;strong&gt;T&lt;sub&gt;t&lt;/sub&gt; = 1&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;For CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities.&lt;/li&gt;
      &lt;li&gt;For SQuAD, the position of the first and the last word from the answer span are predicted.&lt;/li&gt;
      &lt;li&gt;For Graph Reachability, a logistic regression module is used to predict yes/no as the answer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; - For the RL setting, reward at time &lt;strong&gt;t&lt;/strong&gt;, &lt;strong&gt;r&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; = 1 if &lt;strong&gt;T&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; = 1 and answer is correct. Otherwise &lt;strong&gt;r&lt;sub&gt;t&lt;/sub&gt; = 0&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Workflow&lt;/strong&gt; - Given a passage p, query q and answer a:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Extract memory using p&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extract initial hidden state using q&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;These episodes generate actions and answers that are used to train the ReasoNet.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CNN, DailyMail Corpus&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;SQuAD&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;At the time of submission, ReasoNet was ranked 2nd on the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD leaderboard&lt;/a&gt; and as of 9th July 2017, it is ranked 4th.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Graph Reachability Dataset&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet - Standard ReasoNet as described above.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet-Last - Use the prediction from the &lt;strong&gt;T&lt;sub&gt;max&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet &amp;gt; ReasoNet-Last &amp;gt; Deep LSTM Reader&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;As such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage.&lt;/li&gt;
      &lt;li&gt;In fact, the modal value of the number of passes = upper bound on the number of passes.&lt;/li&gt;
      &lt;li&gt;This effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes.&lt;/li&gt;
      &lt;li&gt;It would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Principled Detection of Out-of-Distribution Examples in Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Principled-Detection-of-Out-of-Distribution-Examples-in-Neural-Networks"/>
   <updated>2017-07-17T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Principled Detection of Out of Distribution Examples in Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes &lt;em&gt;ODIN&lt;/em&gt; which can detect such out-of-distribution examples without changing the pre-trained model itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.02690&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;odin&quot;&gt;ODIN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses 2 major techniques&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Temperature Scaling&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Softmax classifier for the classification network can be written as:&lt;/p&gt;

            &lt;p&gt;&lt;em&gt;p&lt;sub&gt;i&lt;/sub&gt;(x, T) = exp(f&lt;sub&gt;i&lt;/sub&gt;(x)/T) / sum(exp(f&lt;sub&gt;j&lt;/sub&gt;(x)/T))&lt;/em&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;where &lt;em&gt;x&lt;/em&gt; is the input, &lt;em&gt;p&lt;/em&gt; is the softmax probability and &lt;em&gt;T&lt;/em&gt; is the temperature scaling parameter.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Increasing &lt;em&gt;T&lt;/em&gt; (up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Input Preprocessing&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Add small perturbations to the input (image) before feeding it into the network.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;em&gt;x_perturbed = x - ε * sign(-δ&lt;sub&gt;x&lt;/sub&gt;log(p&lt;sub&gt;y&lt;/sub&gt;(x, T)))&lt;/em&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;where ε is the perturbation magnitude&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;The perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Given an input (image), first perturb the input.&lt;/li&gt;
  &lt;li&gt;Feed the perturbed input to the network to get its softmax score.&lt;/li&gt;
  &lt;li&gt;If the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification.&lt;/li&gt;
  &lt;li&gt;Otherwise, mark the input as out-of-distribution.&lt;/li&gt;
  &lt;li&gt;For detailed mathematical treatment, refer section 6 and appendix in the &lt;a href=&quot;https://arxiv.org/abs/1706.02690&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Code available on &lt;a href=&quot;https://github.com/ShiyuLiang/odin-pytorch&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;DenseNet with depth L = 100 and growth rate k = 12&lt;/li&gt;
      &lt;li&gt;Wide ResNet with depth = 28 and widen factor = 10&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In-Distribution Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;CIFAR-10&lt;/li&gt;
      &lt;li&gt;CIFAR-100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Out-of-Distribution Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;TinyImageNet&lt;/li&gt;
      &lt;li&gt;LSUN&lt;/li&gt;
      &lt;li&gt;iSUN&lt;/li&gt;
      &lt;li&gt;Gaussian Noise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;False Positive Rate at 95% True Positive Rate&lt;/li&gt;
      &lt;li&gt;Detection Error - minimum misclassification probability over all thresholds&lt;/li&gt;
      &lt;li&gt;Area Under the Receiver Operating Characteristic Curve&lt;/li&gt;
      &lt;li&gt;Area Under the Precision-Recall Curve&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ODIN outperforms the baseline across all datasets and all models by a good margin.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Very simple and straightforward approach with theoretical justification under some conditions.&lt;/li&gt;
  &lt;li&gt;Limited to examples from Vision so can not judge its applicability for NLP tasks.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Ask Me Anything -  Dynamic Memory Networks for Natural Language Processing</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing"/>
   <updated>2017-07-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Ask Me Anything- Dynamic Memory Networks for Natural Language Processing</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.07285&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;input-module&quot;&gt;Input Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Concatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;question-module&quot;&gt;Question Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Similarly, feed the question to a GRU to obtain its representation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;episodic-memory-module&quot;&gt;Episodic Memory Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Episodic memory consists of an attention mechanism and a recurrent network with which it updates its memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During each iteration, the network generates an episode &lt;em&gt;e&lt;/em&gt; by attending over the representation of the sentences, question and the previous memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The episodic memory is updated using the current episode and the previous memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Depending on the amount of supervision available, the network may perform multiple passes. eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass. For others, a fixed number of passes are made.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple passes allow the network to perform transitive inference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-mechanism&quot;&gt;Attention Mechanism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the input representation &lt;em&gt;c&lt;/em&gt;, memory &lt;em&gt;m&lt;/em&gt; and question &lt;em&gt;q&lt;/em&gt;, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A separate GRU encodes the input representation and weights it by the attention.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Final state of the GRU is fed to the answer module.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;answer-module&quot;&gt;Answer Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Use a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There are two possible losses:
    &lt;ul&gt;
      &lt;li&gt;Cross-entropy loss of the predicted answer (all datasets)&lt;/li&gt;
      &lt;li&gt;Cross-entropy loss of the attention supervision (for datasets like bAbI)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;question-answering&quot;&gt;Question Answering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bAbI Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most tasks, DMN either outperforms or performs as good as Memory Networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-classification&quot;&gt;Text Classification&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stanford Sentiment Treebank Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DMN outperforms all the baselines for both binary and fine-grained sentiment analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-tagging&quot;&gt;Sequence Tagging&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Wall Street Journal Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DMN archives state of the art accuracy of 97.56%&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple passes help in reasoning tasks but not so much for sentiment/POS tags.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted. A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again. So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed. If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>One Model To Learn Them All</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/One-Model-To-Learn-Them-All"/>
   <updated>2017-07-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/One Model To Learn Them All</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The current trend in deep learning is to design, train and fine tune a separate model for each problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-philosophy&quot;&gt;Design Philosophy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The joint representation is to be of variable size.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Different tasks from the same domain share the modality net.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder and decoder use the following computational blocks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Convolutional Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Attention Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Multihead, dot product based attention mechanism.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Mixture-of-Experts (MoE) Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For further details, refer the &lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Encoder&lt;/strong&gt; consists of 6 conv blocks with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I/O mixer&lt;/strong&gt; consists of an attention block and 2 conv blocks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt; consists of 4 blocks of convolution and attention with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Modality Nets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Language Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Input is the sequence of tokens ending in a termination token.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This sequence is mapped to correct dimensionality using a learned embedding.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt; and &lt;strong&gt;Categorical Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Uses residual convolution blocks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Similar to the exit flow for &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception Network&lt;/a&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Audio Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ speech corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ImageNet dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;COCO image captioning dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ parsing dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-German translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-English translation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-French translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-French translation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experimental section is not very rigorous with many details skipped (would probably be added later).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While MultiModel does not beat the state of the art models, it does outperform some recent models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Two/Too Simple Adaptations of Word2Vec for Syntax Problems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Two-Too-Simple-Adaptations-of-Word2Vec-for-Syntax-Problems"/>
   <updated>2017-06-26T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Two-Too Simple Adaptations of Word2Vec for Syntax Problems</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In the original Skip-Gram setting, the model predicts the &lt;em&gt;2c&lt;/em&gt; words in the context window (&lt;em&gt;c&lt;/em&gt; is the size of the context window). But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.&lt;/li&gt;
  &lt;li&gt;Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.&lt;/li&gt;
  &lt;li&gt;The paper proposes to use a set of &lt;em&gt;2c&lt;/em&gt; matrices each for a different word in the context window for both Skip-Gram and CBOW models.&lt;/li&gt;
  &lt;li&gt;This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.&lt;/li&gt;
  &lt;li&gt;The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Decomposable Attention Model for Natural Language Inference</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Decomposable-Attention-Model-for-Natural-Language-Inference"/>
   <updated>2017-06-17T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Decomposable Attention Model for Natural Language Inference</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.&lt;/li&gt;
  &lt;li&gt;Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01933&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given two sentences &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;, the model has to predict whether they have an “entailment” relationship, “neutral” relationship or “contradiction” relationship.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;All the words are mapped to their corresponding word vector representation. In subsequent steps, “word” refers to the word vector representation of the actual word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attend&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For each word &lt;em&gt;i&lt;/em&gt; in &lt;strong&gt;a&lt;/strong&gt; and &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;, obtain unnormalized attention weights *e(i, j)=F(i)&lt;sup&gt;T&lt;/sup&gt;F(j) where F is a feed-forward neural network.&lt;/li&gt;
      &lt;li&gt;For &lt;em&gt;i&lt;/em&gt;, compute a β&lt;sub&gt;i&lt;/sub&gt; by performing softmax-like normalization of &lt;em&gt;j&lt;/em&gt; using &lt;em&gt;e(i, j)&lt;/em&gt; as the weight and normalizing for all words &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;β&lt;sub&gt;i&lt;/sub&gt; captures the subphrase in &lt;strong&gt;b&lt;/strong&gt; that is softly aligned to &lt;em&gt;a&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Similarly compute α&lt;sub&gt;j&lt;/sub&gt; for &lt;em&gt;j&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compare&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Create two set of comparison vectors, one for &lt;strong&gt;a&lt;/strong&gt; and another for &lt;strong&gt;b&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;For &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;1, i&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(i, β&lt;sub&gt;i&lt;/sub&gt;)).&lt;/li&gt;
      &lt;li&gt;Similarly for &lt;strong&gt;b&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;2, j&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(j, α&lt;sub&gt;j&lt;/sub&gt;))&lt;/li&gt;
      &lt;li&gt;G is another feed-forward neural network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aggregate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Aggregate over the two set of comparison vectors to obtain &lt;strong&gt;v&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;v&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Feed the aggregated results through the final classifier layer.&lt;/li&gt;
      &lt;li&gt;Multi-class cross-entropy loss function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-complexity&quot;&gt;Computational Complexity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Computationally, the proposed model is asymptotically as good as LSTM with attention.&lt;/li&gt;
  &lt;li&gt;Assuming that dimensionality of word vectors &amp;gt; length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.&lt;/li&gt;
  &lt;li&gt;Further, the model has the advantage of being parallelizable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.&lt;/li&gt;
  &lt;li&gt;Adding intra-sentence attention further improve the test accuracy by 0.5 percent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation. &lt;a href=&quot;https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs&quot;&gt;Quora Duplicate Question Detection Challenege&lt;/a&gt;  would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Fast and Accurate Dependency Parser using Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Fast-and-Accurate-Dependency-Parser-using-Neural-Networks"/>
   <updated>2017-06-03T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Fast and Accurate Dependency Parser using Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.&lt;/li&gt;
  &lt;li&gt;Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;description-of-the-system&quot;&gt;Description of the system&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The system described in the paper uses &lt;a href=&quot;http://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-056-R1-07-027&quot;&gt;&lt;strong&gt;arc-standard&lt;/strong&gt; system&lt;/a&gt; (a greedy, transition-based dependency parsing system).&lt;/li&gt;
  &lt;li&gt;Words, POS tags and arc labels are represented as d dimensional vectors.&lt;/li&gt;
  &lt;li&gt;S&lt;sup&gt;w&lt;/sup&gt;, S&lt;sup&gt;t&lt;/sup&gt;, S&lt;sup&gt;l&lt;/sup&gt; denote the set of words, POS and labels respectively.&lt;/li&gt;
  &lt;li&gt;Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.&lt;/li&gt;
  &lt;li&gt;Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such.&lt;/li&gt;
  &lt;li&gt;Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).&lt;/li&gt;
  &lt;li&gt;Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.&lt;/li&gt;
  &lt;li&gt;Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.&lt;/li&gt;
  &lt;li&gt;This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.&lt;/li&gt;
  &lt;li&gt;L2-regularization term is also added to the loss.&lt;/li&gt;
  &lt;li&gt;During inference, a greedy decoding strategy is used and transition with the highest score is chosen.&lt;/li&gt;
  &lt;li&gt;The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;English Penn Treebank (PTB)&lt;/li&gt;
      &lt;li&gt;Chinese Penn Treebank (CTB)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two dependency representations used:
    &lt;ul&gt;
      &lt;li&gt;CoNLL Syntactic Dependencies (CD)&lt;/li&gt;
      &lt;li&gt;Stanford Basic Dependencies (SD)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Metrics:
    &lt;ul&gt;
      &lt;li&gt;Unlabeled Attached Scores (UAS)&lt;/li&gt;
      &lt;li&gt;Labeled Attached Scores (LAS)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Benchmarked against:
    &lt;ul&gt;
      &lt;li&gt;Greedy arc-eager parser&lt;/li&gt;
      &lt;li&gt;Greedy arc-standard parser&lt;/li&gt;
      &lt;li&gt;Malt-Parser&lt;/li&gt;
      &lt;li&gt;MSTParser&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;The system proposed in the paper outperforms all other parsers in both speed and accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Cube function gives a 0.8-1.2% improvement over tanh.&lt;/li&gt;
  &lt;li&gt;Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.&lt;/li&gt;
  &lt;li&gt;Using POS and labels gives an improvement of 1.7% and 0.4% respectively.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Neural Module Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Module-Networks"/>
   <updated>2017-05-23T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Module Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For the task of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;Visual Question Answering&lt;/a&gt;, decompose a question into its linguistic substructures and train a neural network module for each substructure.&lt;/li&gt;
  &lt;li&gt;Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.&lt;/li&gt;
  &lt;li&gt;Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.&lt;/li&gt;
  &lt;li&gt;The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.02799&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Questions tend to be compositional.&lt;/li&gt;
  &lt;li&gt;Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.&lt;/li&gt;
  &lt;li&gt;Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-module-network-for-vqa&quot;&gt;Neural Module Network for VQA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Training samples of form &lt;em&gt;(w, x, y)&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;w&lt;/em&gt; - Natural Language Question&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;x&lt;/em&gt; - Images&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;y&lt;/em&gt; - Answer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model specified by collection of modules &lt;em&gt;{m}&lt;/em&gt; and a network layout predictor &lt;em&gt;P&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model instantiates a network based on &lt;em&gt;P(w)&lt;/em&gt; and uses that to encode a distribution &lt;em&gt;P(y|w, x, model_params)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;modules&quot;&gt;Modules&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find: Finds objects of interest.&lt;/li&gt;
  &lt;li&gt;Transform: Shift regions of attention.&lt;/li&gt;
  &lt;li&gt;Combine: Merge two attention maps into a single one.&lt;/li&gt;
  &lt;li&gt;Describe: Map a pair of attention and input image to a distribution over the labels.&lt;/li&gt;
  &lt;li&gt;Measure: Map attention to a distribution over the labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;natural-language-question-to-networks&quot;&gt;Natural Language Question to Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Map question to the layout which specifies the set of modules and connections between them.&lt;/li&gt;
  &lt;li&gt;Assemble the final network using the layout.&lt;/li&gt;
  &lt;li&gt;Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.&lt;/li&gt;
  &lt;li&gt;eg “what is the colour of the truck?” becomes “colour(truck)”&lt;/li&gt;
  &lt;li&gt;The symbolic representation is mapped to a layout:
    &lt;ul&gt;
      &lt;li&gt;All leaves become &lt;em&gt;find&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All internal nodes become &lt;em&gt;transform/combine&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All root nodes become &lt;em&gt;describe/measure&lt;/em&gt; module.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;answering-natural-language-question&quot;&gt;Answering Natural Language Question&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Final model combines output from a simple LSTM question encoder with the output of the neural module network.&lt;/li&gt;
  &lt;li&gt;This helps in modelling the syntactic and semantic regularities of the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Since some modules are updated more frequently than others, adaptive per weight learning rates are better.&lt;/li&gt;
  &lt;li&gt;The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).&lt;/li&gt;
  &lt;li&gt;Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.&lt;/li&gt;
  &lt;li&gt;Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Making-the-V-in-VQA-Matter-Elevating-the-Role-of-Image-Understanding-in-Visual-Question-Answering"/>
   <updated>2017-05-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question.&lt;/li&gt;
  &lt;li&gt;For example, if the question starts with “Do you see a …”, it is more likely to be “yes” than “no”.&lt;/li&gt;
  &lt;li&gt;To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.&lt;/li&gt;
  &lt;li&gt;The authors present a balanced version of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;VQA dataset&lt;/a&gt; where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.&lt;/li&gt;
  &lt;li&gt;The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset-collection&quot;&gt;Dataset Collection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I’ which is similar to I but for which the answer to question Q is A’ (different from A).&lt;/li&gt;
  &lt;li&gt;To facilitate the search for I’, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A. In case none of the 24 images qualifies, the worker may select “not possible”.&lt;/li&gt;
  &lt;li&gt;In the second round, the workers were asked to answer Q for I’.&lt;/li&gt;
  &lt;li&gt;This 2-stage protocol results in a significantly more balanced dataset than the previous dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;observation&quot;&gt;Observation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.&lt;/li&gt;
  &lt;li&gt;Training on balanced dataset improves performance on the unbalanced dataset.&lt;/li&gt;
  &lt;li&gt;Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;counter-example-explanations&quot;&gt;Counter-example Explanations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.&lt;/li&gt;
  &lt;li&gt;Supervising signal is provided by the data collection procedure where humans pick the image I’ from the same set of candidate images.&lt;/li&gt;
  &lt;li&gt;For each image in the candidate set, compute the inner product of question-image embedding and answer embedding.&lt;/li&gt;
  &lt;li&gt;The K inner product values are passed through a fully connected layer to generate K scores.&lt;/li&gt;
  &lt;li&gt;Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).&lt;/li&gt;
  &lt;li&gt;The proposed explanation model achieves a recall@5 of 43.49%&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Conditional Similarity Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Conditional-Similarity-Networks"/>
   <updated>2017-05-07T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Conditional Similarity Networks</id>
   <content type="html">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.&lt;/li&gt;
  &lt;li&gt;But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.&lt;/li&gt;
  &lt;li&gt;What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.&lt;/li&gt;
  &lt;li&gt;The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.&lt;/li&gt;
  &lt;li&gt;It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vision.cornell.edu/se3/conditional-similarity-networks/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conditional-similarity-networks&quot;&gt;Conditional Similarity Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image, &lt;em&gt;x&lt;/em&gt;, learn a non-linear feature embedding &lt;em&gt;f(x)&lt;/em&gt; such that for any 2 images &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, the euclidean distance between &lt;em&gt;f(x&lt;sub&gt;1&lt;/sub&gt;)&lt;/em&gt; and &lt;em&gt;f(x&lt;sub&gt;2&lt;/sub&gt;)&lt;/em&gt; reflects their similarity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conditional-similarity-triplets&quot;&gt;Conditional Similarity Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given a triplet of images &lt;em&gt;(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;)&lt;/em&gt; and a condition &lt;em&gt;c&lt;/em&gt; (the notion of similarity), an oracle (say crowd) is used to determmine if &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is more similar to &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; or &lt;em&gt;x&lt;sub&gt;3&lt;/sub&gt;&lt;/em&gt; as per the given criteria &lt;em&gt;c&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;In general, for images &lt;em&gt;i, j, l&lt;/em&gt;, the triplet &lt;em&gt;t&lt;/em&gt; is ordered {i, j, l | c} if &lt;em&gt;i&lt;/em&gt; is more similar to &lt;em&gt;j&lt;/em&gt; than &lt;em&gt;l&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-from-triplets&quot;&gt;Learning From Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Define a loss function &lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;()&lt;/em&gt; to model the similarity structure over the triplets.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;(i, j, l) = max{0, D(i, j) - D(i, l) + h}&lt;/em&gt; where &lt;em&gt;D&lt;/em&gt; is the euclidean distance function and &lt;em&gt;h&lt;/em&gt; is the similarity scalar margin to prevent trivial solutions.&lt;/li&gt;
  &lt;li&gt;To model conditional similarities, masks &lt;em&gt;m&lt;/em&gt; are defined as &lt;em&gt;m = σ(β)&lt;/em&gt; where σ is the RELU unit and β is a set of parameters to be learnt.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt; denotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.&lt;/li&gt;
  &lt;li&gt;The euclidean function &lt;em&gt;D&lt;/em&gt; now computes the masked distance (&lt;em&gt;f(i, c)m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt;) between the two given images.&lt;/li&gt;
  &lt;li&gt;Two regularising terms are also added - L2 norm for &lt;em&gt;D&lt;/em&gt; and L1 norm for &lt;em&gt;m&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fonts dataset by Bernhardsson
    &lt;ul&gt;
      &lt;li&gt;3.1 million 64 by 64-pixel grey scale images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zappos50k shoe dataset
    &lt;ul&gt;
      &lt;li&gt;Contains 50,000 images of individual richly annotated shoes.&lt;/li&gt;
      &lt;li&gt;Characteristics of interest:
        &lt;ul&gt;
          &lt;li&gt;Type of the shoes (i.e., shoes, boots, sandals or slippers)&lt;/li&gt;
          &lt;li&gt;Suggested gender of the shoes (i.e., for women, men, girls or boys)&lt;/li&gt;
          &lt;li&gt;Height of the shoes’ heels (0 to 5 inches)&lt;/li&gt;
          &lt;li&gt;Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Initial model for the experiments is a ConvNet pre-trained on ImageNet&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standard Triplet Network&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learn from all available triplets jointly as if they have the same notion of similarity.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Set of Task Specific Triplet Networks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Train n separate triplet networks such that each is trained on a single notion of similarity.&lt;/li&gt;
      &lt;li&gt;Needs far more parameters and compute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - fixed disjoint masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.&lt;/li&gt;
      &lt;li&gt;Aims to learn a fully disjoint embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - learned masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learns all the components - conv filters, embedding and the masks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Refer paper for details on hyperparameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.&lt;/li&gt;
  &lt;li&gt;The learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.&lt;/li&gt;
  &lt;li&gt;Order of performance:
    &lt;ul&gt;
      &lt;li&gt;CSNs with learned masks &amp;gt; CSNs with fixed masks &amp;gt; Task-specific networks &amp;gt; standard triplet network.&lt;/li&gt;
      &lt;li&gt;Though CSNs with learned masks require more training data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.&lt;/li&gt;
  &lt;li&gt;This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple Baseline for Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Simple-Baseline-for-Visual-Question-Answering"/>
   <updated>2017-04-28T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Simple Baseline for Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/li&gt;
  &lt;li&gt;The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1512.02167.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text Features&lt;/strong&gt; - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Features&lt;/strong&gt; - Last layer activations from GoogLeNet.&lt;/li&gt;
  &lt;li&gt;Text features are concatenated with image features and fed into a softmax.&lt;/li&gt;
  &lt;li&gt;Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interpretation-of-the-model&quot;&gt;Interpretation of the model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive.&lt;/li&gt;
  &lt;li&gt;The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.&lt;/li&gt;
  &lt;li&gt;Question words generally can influence the answer given the bias in images occurring in COCO dataset.&lt;/li&gt;
  &lt;li&gt;Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.&lt;/li&gt;
  &lt;li&gt;The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.&lt;/li&gt;
  &lt;li&gt;While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>VQA-Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/VQA-Visual-Question-Answering"/>
   <updated>2017-04-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/VQA Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1505.00468v6&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vqa-challenge-and-workshop&quot;&gt;&lt;a href=&quot;http://www.visualqa.org/&quot;&gt;VQA Challenge and Workshop&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.&lt;/li&gt;
  &lt;li&gt;Interestingly, the second version is starting on 27th April 2017 (today).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;benefits-over-tasks-like-image-captioning&quot;&gt;Benefits over tasks like image captioning:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Simple, &lt;em&gt;n-gram&lt;/em&gt; statistics based methods are not sufficient.&lt;/li&gt;
  &lt;li&gt;Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.&lt;/li&gt;
  &lt;li&gt;Since only short answers are expected, evaluation is easier.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Created a new dataset of 50000 realistic, abstract images.&lt;/li&gt;
  &lt;li&gt;Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (&amp;gt;200K images) and abstract images.&lt;/li&gt;
  &lt;li&gt;Three questions per image and ten answers per question (along with their confidence) were collected.&lt;/li&gt;
  &lt;li&gt;The entire dataset contains over 760K questions and 10M answers.&lt;/li&gt;
  &lt;li&gt;The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-of-data-collection-methodology&quot;&gt;Highlights of data collection methodology&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Emphasis on questions that require an image, and not just common sense, to be answered correctly.&lt;/li&gt;
  &lt;li&gt;Workers were shown previous questions when writing new questions to increase diversity.&lt;/li&gt;
  &lt;li&gt;Answers collected from multiple users to account for discrepancies in answers by humans.&lt;/li&gt;
  &lt;li&gt;Two modalities supported:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Open-ended&lt;/strong&gt; - produce the answer&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;multiple-choice&lt;/strong&gt; - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-from-data-analysis&quot;&gt;Highlights from data analysis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Most questions range from four to ten words while answers range from one to three words.&lt;/li&gt;
  &lt;li&gt;Around 40% questions are “yes/no” questions.&lt;/li&gt;
  &lt;li&gt;Significant (&amp;gt;80%) inter-human agreement for answers.&lt;/li&gt;
  &lt;li&gt;The authors performed a study where human evaluators were asked to answer the questions without looking at the images.&lt;/li&gt;
  &lt;li&gt;Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.&lt;/li&gt;
  &lt;li&gt;The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline-models&quot;&gt;Baseline Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;random&lt;/strong&gt; selection&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;prior (“yes”)&lt;/strong&gt; - always answer as yes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;per Q-type prior&lt;/strong&gt; - pick the most popular answer per question type.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;nearest neighbor&lt;/strong&gt; - find the k nearest neighbors for the given (image, question) pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;I&lt;/strong&gt; - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;norm I&lt;/strong&gt; - : l2 normalized version of &lt;strong&gt;I&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Question Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q&lt;/strong&gt; - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q&lt;/strong&gt; - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Deeper LSTM Q&lt;/strong&gt; - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-Layer Perceptron (MLP)&lt;/strong&gt; - Combine image and question embeddings to obtain a single embedding.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q + I&lt;/strong&gt; method - concatenate BoW Q and I embeddings.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q + I, deeper LSTM Q + norm I&lt;/strong&gt; methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.&lt;/li&gt;
  &lt;li&gt;Cross-entropy loss with VGGNet parameters frozen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (&amp;gt;80% and &amp;gt;90% respectively).&lt;/li&gt;
  &lt;li&gt;The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.&lt;/li&gt;
  &lt;li&gt;Vision only model performs even worse than the model which always produces “yes” as the answer.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
