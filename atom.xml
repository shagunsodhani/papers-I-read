<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Papers I Read</title>
 <link href="https://shagunsodhani.github.io/papers-I-read/atom.xml" rel="self"/>
 <link href="https://shagunsodhani.github.io/papers-I-read/"/>
 <updated>2023-02-12T14:01:15-05:00</updated>
 <id>https://shagunsodhani.github.io/papers-I-read</id>
 <author>
   <name>Shagun Sodhani</name>
   <email>sshagunsodhani@gmail.com</email>
 </author>

 
 <entry>
   <title>Toolformer - Language Models Can Teach Themselves to Use Tools</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools"/>
   <updated>2023-02-10T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Toolformer - Language Models Can Teach Themselves to Use Tools</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents Toolformer, a language model that uses simple APIs to use external tools (calculator, QA system, search engine, translation system, and calendar).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.04761&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Starting with a language model, M, the goal is to enable the language model to use tools by invoking API calls.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An API call is denoted by the tuple $c = (api-name, api-input)$. It can be linearized as $e(c) = [api-name(api-input)]$ or as $e(c, r) = [api-name(api-input) -&amp;gt; r]$ where $r$ denotes the result of the API.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The given dataset of plain text, $C$, is converted into a dataset $C*$ augmented with the API calls using a three-step process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the first step, a position ($i$) and API call candidates (for the position $i$) are sampled.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Positions are sampled by (i) computing the probability that M assigns to starting an API call for each position and (ii) retaining the top-$k$ positions with a probability greater than a threshold value.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For each of the sampled positions (say $i$), API calls are sampled by concatenating a prompt to the tokens till index $i$ and sampling from the model M. Examples that do not generate the “end of the API” token (i.e.,”]”) are discarded.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the second step, the API calls are executed to obtain response $r$ (text sequence).&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;API calls are filtered using the following criteria: if providing M with both the input and the output of the API makes it easier for M to predict the future token, compared to not using the API call at all or just using the input to the API, then the API call is helpful for M, and the example should be retained.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the last step, the remaining API calls are merged to obtain the augmented dataset $C*$ that is used for finetuning M.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that $C*$ contains $C$, so M is finetuned on the original dataset and examples where a tool is helpful.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During inference, the model is used for decoding in the usual way. Decoding is stopped when it produces the “-&amp;gt;” token, and the corresponding API is used to generate the response. The decoding process (using the model) resumes with the API output appended to the decoded text.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tools&quot;&gt;Tools&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There are two constraints on the tools: (i) their input and output should be expressible as text, and (ii) few demonstrations can be obtained from the tools. The second constraint means that the tool should be useable or accessible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considered the following tools: a question-answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system. Of these, only the calculator and calendar are non-neural network tools.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Subset of CCNet is used as the language modeling dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPT-J is used as the language model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For finetuning, the batch size is 128, the learning rate is 1e-5, and a linear warmup for the first 10% of training is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Following models are compared:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;GPT-J: Regular GPT-J model without any finetuning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;GPT-J + CC: GPT-J finetuned on $C$ without any API calls.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Toolformer, i.e. GPT-J finetuned on $C*$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Toolformer with API calls disabled during training.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;OPT 66B&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;GPT-3&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The models are evaluated in the prompted zero-shot setup, where models are instructed to solve a task without any in-context examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One difference from the standard greedy decoding is that the API call is used whenever it is one of the top-10 most likely next tokens. This is done to increase the use of API calls.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation Tasks&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;SQuAD, GoogleRE, and T-REx subsets of the LAMA benchmark where the model has to complete a short statement with a missing fact.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Since LAMA questions are based on Wikipedia, Toolformer isn’t allowed to use Wikipedia search.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The evaluation criteria is to check if the correct word is among the first five words predicted by the model.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Toolformer uses the question-answering tool for most cases, outperforming all the baselines.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Math Dataset&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;eSDiv, SVAMP, and MAWPS benchmarks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The first number predicted by the model is considered to be the output.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Toolformer uses the calculator tool for most cases, thereby outperforming all the baselines.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Question Answering&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Web Questions, Natural Questions, and TriviaQA datasets.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The evaluation criteria is to check if the correct word is among the first 20 words predicted by the model.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Question Answering tool is disabled for this setup.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Toolformer uses the Wikipedia tool for most cases, thereby outperforming all the baselines other than the much larger GPT-3 model.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Multilingual Question Answering&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;MLQA benchmark.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The evaluation criteria is to check if the correct word is among the first ten words predicted by the model.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Toolformer uses the translation tool for most of the questions, with questions in Hindi being an exception.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;However, Toolformer does not consistently outperform the GPT-J baseline, likely because, for some languages, finetuning on CCNet could hurt performance.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Temporal Datasets&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;TEMPLAMA (cloze style queries where the answer changes with time) and DATESET (dataset generated through a series of templates and populated with random dates/durations).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;While Toolformer outperforms the baselines for both datasets, it relies on the Wikipedia search and Question Answering tools (and not the calendar tool) for the LAMA dataset. On the DATESET dataset, it uses the calendar tool in the majority.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Language Modeling&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;WikiText and a subset of 10,000 randomly selected documents from CCNet (not used during training of M).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Training on $C*$ does not increase perplexity (compared to training on C). In this experiment, the API calls are disabled during inference.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Varying the size of the underlying models show that the ability to use tools emerges only around 755M parameters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Extending Toolformer to chain the use of tools and use tools interactively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In some cases, the use of tools is very sample-inefficient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decision to use a tool does not account for the cost of using the tool.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Synthesized Policies for Transfer and Adaptation across Tasks and Environments</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Synthesized-Policies-for-Transfer-and-Adaptation-across-Tasks-and-Environments"/>
   <updated>2021-03-29T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Synthesized Policies for Transfer and Adaptation across Tasks and Environments</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies transfer learning in RL, focusing on simultaneous transfer across both tasks and environments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is to learn task and environment embeddings and compose them using a meta-rule, and the proposed approach is called SYNPO (Synthesized Policies).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.03276&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three settings considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;S1&lt;/em&gt;: Transfer to a new (environment, task) pair when the agent has been trained on the environment and the task before (but not simultaneously).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;S2&lt;/em&gt;: Transfer to a new (environment, task) pair where either the environment or the task is not seen previously.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;S3&lt;/em&gt;: Transfer to a new (environment, task) pair where neither the environment nor the task is seen previously.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the second and third settings, the agent is allowed to collect some data in the new environment or task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The (environment, task) combinations that the agent has seen during training are referred to as &lt;em&gt;seen&lt;/em&gt; combinations, while the remaining combinations are referred to as the &lt;em&gt;unseen&lt;/em&gt; combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is to:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;learn embeddings of environments and tasks&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;use these embeddings to compose a policy (parameterized as the linear combination of the policy basis).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A disentanglement objective is used to decouple the task and environment embedding.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;policy-composition&quot;&gt;Policy Composition&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an (environment, task) pair $z = (\epsilon, \tau)$, the policy is given as $\pi_z(a|s) \propto exp(\psi_s^TU(e_{\epsilon}, e_{\tau})\phi_{a} + b_{\pi} )) $.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Here $b_{\pi}$ is a scalar bias, $\psi_{s}$ and $\phi_{a}$ are state and action representations, $U$ is parameterized as the linear comination of $K$ basis matrices $\Theta_k$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$U(e_{\epsilon}, e_{\tau}) = \sum_{k=1}^{K}\alpha_k(e_{\epsilon}, e_{\tau})\Theta_k$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The basis matrices (denoted by $\Theta_k$) are shared across tasks while the coefficients ($\alpha_k$) are specific to the (environment, task) pair.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the agent also predicts rewards using the same set of basis but different coefficients.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;disentangling-environment-and-task-embeddings&quot;&gt;Disentangling environment and task embeddings&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an (environment, task) pair, the agent is trained to decode the environment (and task) given the agent’s trajectory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequence of state-action pairs (in the trajectory) is mapped to a sequence of state-action representations, given by $\psi_s^T\Theta_k\phi_{a}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment-setup&quot;&gt;Experiment Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The agent is trained (and evaluated) on imitation learning (mostly) and reinforcement learning setup.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;environments&quot;&gt;Environments&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GRIDWORLD&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Twenty $16 \times 16$ gird-aligned mazes that are similar in appearance but differ in topology.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The task is to collect colored blocks in a given order. In each task, the starting position of the agent and the position of the blocks is randomized.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Each environment has 20 tasks, leading to a total of 400 (environment, task) combinations.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.05474&quot;&gt;THOR&lt;/a&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;This is a 3D simulator where the agent is placed in indoor photo-realistic scenes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The task is the search for objects and perform actions like “put cabbage on the fridge.”&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The setup uses 19 scenes (environments), with each environment comprising of 21 tasks.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MLPs that concatenate state, environment embeddings, and task embedding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.05312&quot;&gt;Successor feature model&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.07088&quot;&gt;Module Network&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-task Learning where the distinction between the environments is ignored.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GRIDWORLD&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In the first setting (&lt;em&gt;S1&lt;/em&gt;)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;SYNPO outperforms all the baselines.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;As the agent is trained on more (environment, task) combinations, its performance on the unseen combinations improves. This trend saturates when the &lt;em&gt;seem/total&lt;/em&gt; ratio reaches about 0.4 (i.e., training on 40% of all the combinations).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task disentanglement is more important than environment disentanglement.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In the second and third setting (&lt;em&gt;S2&lt;/em&gt; and &lt;em&gt;S3&lt;/em&gt;)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The agent uses one demonstration from each test pair to finetune the embeddings.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;em&gt;S2&lt;/em&gt; is an easier setting than &lt;em&gt;S3&lt;/em&gt;.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Transfer learning across tasks is easier than transfer learning across environments.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;THOR&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;SYNPO outperforms all the baselines on both seen and unseen combinations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Deep Neural Networks for YouTube Recommendations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Deep-Neural-Networks-for-YouTube-Recommendations"/>
   <updated>2021-03-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Deep Neural Networks for YouTube Recommendations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes YouTube’s deep learning-based recommendation system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Scale - Very large number of users and videos.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Freshness - Very large number of videos uploaded every hour. The recommendation system should take these new videos into account as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Noise - User satisfaction needs to be modeled from noisy implicit feedback signal as the explicit signal is very sparse.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;system-overview&quot;&gt;System Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two neural networks: one for candidate generation and another one for ranking.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Offline metrics like precision, recall, ranking loss&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A/B testing via live experiments&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;candidate-generation&quot;&gt;Candidate Generation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Input: events from a user’s YouTube activity history.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output: small subset (hundreds) of videos.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Recommendation is modeled as extreme multiclass classification.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Predict the video (from a corpus) that a user will watch at a given time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The neural network’s task is to learn useful user embeddings, given the user’s context and history.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For each positive class (relevant video), negative classes (non-relevant videos) are sampled from the video corpus.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model Architecture&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A feedforward network with input as user embeddings and context embeddings (watch history).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Watch history is a variable-length sequence of video ids, where each video id is mapped to an embedding.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The sequence of video ids is mapped to a sequence of embeddings, and this sequence is averaged to obtain fixed-sized embedding.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Additional signals like demographic features and search query embeddings can be added along with the context embeddings.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The age of a video is also used as a feature during training to account for the freshness of the content. This feature is set to zero (or slightly negative) during inference.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other Insights&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Training examples are generated from all YouTube watches, including the watches from the videos embedded on other sites, to surface new content.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generating the same number of training examples per user is important to avoid a small set of active users from dominating the model training.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Predicting a user’s next watch leads to better results than predicting a randomly held-out watch. This can be attributed to the general consumption pattern of videos (e.g., episodes are usually watched in order).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ranking&quot;&gt;Ranking&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Input: list of candidate videos to rank from.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output: score for each video.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;A feedforward network (similar to candidate generation model) trained using logistic regression loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feature representation&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Different types of features: categorical vs. continuous, univalent vs. multivalent, describes video vs. describes user or context.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Important signals include user’s interaction with the video (or similar videos), which source/channel added the video to the candidate set.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Embeddings are shared across features. For example, the representation for a video id remains the same, irrespective of whether it is being used for representing the “video to recommend” or the “last seen video.”&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Feature normalization and transformations like exponents (square or square root) for continuous variables improve the performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To model the expected watch time, the logistic regression loss is weighted by the observed watch time. For example, if a video was watched, its weight is given by the observed watch time, and if the video was not watched, its weight is set to 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, this means that the logistic regression model learns odds that approximate the expected watch time of the video.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>The Tail at Scale</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/The-Tail-at-Scale"/>
   <updated>2021-03-15T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/The Tail at Scale</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents some causes for (temporary) high-latency episodes in large-scale online systems and techniques to mitigate their impact so that the tail of latency distribution remains short.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.google/pubs/pub40801/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-does-variability-in-response-time-exist&quot;&gt;Why does variability in response time exist&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Shared resources between processes on the same node&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Background processes (daemons) could use cause a momentary spike in resource usage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Processes running on different nodes may contend for global resources like shared file systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maintenance activities like disk compaction or garbage collection.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Others like queueing, power limits, or energy management.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the case of large-scale systems, the component-level variability is further amplified.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reducing-component-variability&quot;&gt;Reducing Component Variability&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use differentiated service classes to prioritize user requests over non-interactive requests.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reduce head-of-line blocking by breaking long-running requests into smaller requests.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Synchronize maintenance jobs across nodes to minimize the window for high latency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Caching generally does not help to address tail latency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adapting-to-latency-variability&quot;&gt;Adapting to Latency Variability&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two categories of adaptation approaches&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Within Request Short-Term Adaptations&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;These approaches are more relevant for services that perform many read queries on loosely consistent datasets.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Hedged Request&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Send the request to multiple replicas, and once one of the replicas returns the result, cancel the other requests.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;In practice, start by sending the request to only one replica. Send the secondary requests if the first request is outstanding for more than $95^{th}$ percentile of expected latency.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;This introduces an additional $5\%$ load while substantially shortening the latency tail.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;This approach work because often, the cause of latency is not the query itself but other factors like overloaded nodes.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Tied Request&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Hedged request approach makes a tradeoff regarding how long to wait before initiating requests to other replicas. The sooner the request is made, the lower should be the latency in serving the request, but more will be the overall load in the system.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;The load in the system can be reduced by “tieing” requests (sent to different replicas) so that as soon as one replica starts processing the request, it can notify the other replicas, which could drop the request or deprioritize it.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;In practice, “tieing” requests means that each replica has the identity of other replicas which may execute the request.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Note that there is a short window (of the average network message delay) when multiple replicas could start executing the request. This can be mitigated if the client (issuing the requests) introduces a delay to twice the average network message delay.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Submit the request to the least loaded replica&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;This is less effective for reasons like the load on a replica can change after the request is made but before it is executed.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Cross-Request Long-Term Adaptations&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;These approaches are more relevant for situations where different services have different throughput.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Micro-partitions&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Generate more paritions than the number of nodes.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;The partitions can be dynamically assigned to machines to ensure proper load balancing.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;In case of machine failure, many nodes can be used to quickly re-create the micro-partitions instead of waiting on one machine to read one single large partition.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Selective Replication&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;With micro-partitioning, replicas for micro-partitions can be created ahead of time to achieve good load balancing.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Latency induced probation&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;In some cases, removing a slow node can improve the overall latency of the system. The probated node can be re-incorporated when its latency improves.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Large Information Retrieval Systems&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In such systems, speed can be more critical than the quality of the result.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The system should return a “good enough” result that is available with low latency instead of waiting for the “best result” that is available with high latency.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In some cases, a request could trigger an unexpected code path or cause some other exception that could slow down the entire system.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In such cases, the &lt;em&gt;canary request&lt;/em&gt; technique can be used where the system sends the request initially to only 1 or 2 nodes. The request is sent over to the other nodes only after receiving a successful response from the initial nodes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Requests that update state are easier to handle for several reasons:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The scale of latency-critical modifications is generally small.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The update can be performed asynchronously after responding to the user.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Quorum-based approaches (often used for ensuring consistent updates) are inherently tail-tolerant.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Practical Lessons from Predicting Clicks on Ads at Facebook</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Practical-Lessons-from-Predicting-Clicks-on-Ads-at-Facebook"/>
   <updated>2021-03-08T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Practical Lessons from Predicting Clicks on Ads at Facebook</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes several design choices for developing a model for predicting user response (clicks) on ads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is trained/evaluated on offline data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation metrics:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Normalized Cross-Entropy (or Normalized Entropy, NE)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Defined as the predictive log-loss per impression, divided by the entropy of the background CTR (click-through rate).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Background CTR is the average empirical CTR of the training data.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Lower normalized cross-entropy is better.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The normalization term is important to make the metric insensitive to the background CTR. Otherwise, the log loss can easily be made low when background CTR is close to 0 or 1.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;NE can also be written as $RIG - 1$, where $RIG$ is the Relative Information Gain.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Calibration&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Ratio of average estimated CTR and empirical CTR.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Area-Under-ROC (AUC) is a good metric for measuring ranking quality (among ads). However, it is &lt;strong&gt;not used&lt;/strong&gt; as a metric to avoid over-delivery or under-delivery of ads.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Transformation&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A given add impression, $e$, is transformed into a $n-$dimensional vector, $x$, where the $i^{th}$ index denotes the value of the $i^{th}$ categorical feature.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Continous features are binned, and the bin index is used as a categorical feature, thus applying a non-linear transformation to the features.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Categorical features that are tuple-like (i.e., have a tuple of values) can be converted into new categorical features by taking a cartesian product.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Boosted decision trees can be used to implement the previous two transformations in one go.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Each tree is used as a categorical feature that takes the value of the index of the leaf node than an ad maps to.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The paper used the Gradient Boosting Machine with the $L_2-$TreeBoost algorithm.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Using the tree feature transformation improves the Normalized Cross-Entropy by $3.4\%$.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Logistic Regression (LR) or Bayesian online learning scheme for probit regression (BOPR) algorithms are used for training a linear classifier model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;While both LR and BOPR models provide similar performance, the LR model is half the BOPR model’s size and faster for performing training/inference.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;role-of-data-freshness&quot;&gt;Role of Data Freshness&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When a model is trained on the data from a particular day and evaluated on data from the subsequent days, the model’s performance degrades as the delay between training and test set increases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This highlights the importance of the freshness of the training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One straightforward approach can be to train the model every day.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alternatively, the linear classifier can be trained using online learning, while the boosted decision tree can still be trained daily.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Different choices for setting the learning rate (for online training of linear classifier) are compared, and the &lt;a href=&quot;https://research.google/pubs/pub41159/&quot;&gt;per-coordinate learning rate&lt;/a&gt; is found to perform best in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generating-real-time-training-data&quot;&gt;Generating Real-Time Training Data&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An “online joiner” system is used to generate real-time training data for the linear classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The challenging part is, while there are data points with a “positive” label (i.e., the user clicked on the ad), there are no datapoints with a “negative” label (since there is no “no-click” button that the user can click).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An impression is considered to have the “no-click” label if the user does not click on the ad within a (long) time window of seeing the ad.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Too short a time window could mislabel some impressions, while too long a time window will delay the real-time training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The online joiner performs a distributed stream-to-stream join on the stream of ad impressions and stream of ad clicks using a HashQueue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A HashQueue:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;comprises of a First-In-First-Out queue as a buffer window and a hash map for fast random access to label impressions.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;supports three operations on key-value pairs: enqueue, dequeue, and lookup.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;memory-and-latency&quot;&gt;Memory and Latency&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing the number of boosting trees shows diminishing returns, and most of the improvements come from the first 500 trees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Top 10 features account for half of the total feature importance, while the last 300 features add less than 1% feature importance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Features in the boosting model can be broadly classified as contextual or historical.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Historical feature provides much more explanatory power than the contextual features through contextual features are helpful to handle the cold start problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models trained with just the contextual features rely more heavily on data freshness than models trained with just the historical features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Uniform subsampling and negative downsampling techniques are used to limit the amount of training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the case of negative downsampling, the model needs to be re-calibrated as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Ad Click Prediction - a View from the Trenches</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Ad-Click-Prediction-a-View-from-the-Trenches"/>
   <updated>2021-03-01T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Ad Click Prediction - a View from the Trenches</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents case studies from the experience of deploying an ad click-through rate (CTR) prediction model at Google.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on themes related to memory footprint, performance analysis, calibration, confidence in the predictions, and feature engineering.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.google/pubs/pub41159/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;system-overview&quot;&gt;System Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Features (corresponding to a given ad) include search query and the metadata in the ad. The features are very sparse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Single layer, regularized Logistic Regression model is trained with Online Gradient Descent (same as Stochastic Gradient Descent, but in the online setting).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;From a memory perspective, it is important to minimize the size of the final model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding just the L1 penalty is not sufficient to produce weights that are precisely equal to 0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v15/mcmahan11b.html&quot;&gt;“Follow The (Proximally) Regularized Leader” algorithm or FTRL-Proximal algorithm&lt;/a&gt; is used to learn sparse models without losing on the accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using per-coordinate learning rates improves the performance at the cost of memory as both the sum of gradients and the sum of the square of gradients are tracked for each feature.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In practice, some of the cost can be alleviated by approximating that all the events containing a given feature have the same probability.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In such a case, the sum of the square of gradients can be approximated using the counts of positive and negative events alone.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some memory overhead can be reduced based on the following observation: the vast majority of features are extremely rare. Hence, it is not necessary to track the statistics for such rare features.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;However, in an online setting, it is not known upfront as to which features will be sparse.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper proposes to use probabilistic feature inclusion - a feature is added to the model with probability $p$. Once it is added, the feature is not removed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;An alternative approach is to use a rolling set of counting Bloom filters to check if a feature has appeared at least $n$ times in training. Bloom filters are probabilistic data structures and can return false positives.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Memory can also be saved by using fewer bits for encoding weights.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Most of the weight coefficients lie in the range $(-2, 2)$, and a $16-$ bit encoding is used in place of $32$ or $64$ bit encoding.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This quantization approach needs to account for roundoff problems. The fix is easy to implement.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When training many models with similar hyperparameters, per-model learning rate counters can be replaced by statistics shared by all the models, thus reducing memory footprint.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Single Value Structure is used to reduce the memory footprint when evaluating a very large set of model variants that differ only in addition/removal of a small subset of features.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;All the models, that use a feature, share a single value structure corresponding to the feature. This reduces the memory overhead by order of magnitude.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;During the update, each model computes the weight updates corresponding to all the features that it is using. The updated weight is averaged across all the models and used to update the single value structure.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since CTR datasets are generally highly imbalanced, the training data (for the negative class) can be subsampled to reduce the amount of data to train over. The loss component (corresponding to negative class) can be appropriately scaled up.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Offline metrics like AucLoss (1 - AUC), Log Loss, Squared Error&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Online loss is computed on the new training data (new incoming traffic)	before training on it.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The confidence in the model’s prediction is estimated using a heuristic called &lt;em&gt;uncertainty score&lt;/em&gt;. It can be measured using the dot product of the feature and the vector of learning rates.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The idea is that the learning rates already maintain a notion of uncertainty.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Features for which the learning rate is high are the features for which uncertainty is also high.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calibrating Predictions&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The calibration can be improved by applying correction functions $\tau_d(p)$ where $p$ is the predicted CTR, and $d$ is an element of a partition of the training data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\tau$ can be modeled as $\gamma^{\kappa}$ where $\gamma$ and $\kappa$ are learned using Poisson regression.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsuccessful Experiments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Aggressive feature hashing was tried to reduce the memory overhead. However, it leads to a significant loss in performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using dropout did not help, probably because the features are sparse.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using feature bagging hurt the AucLoss.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Feature vector normalization did not improve performance, probably because of per-coordinate learning rates and regularization.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Anatomy of Catastrophic Forgetting - Hidden Representations and Task Semantics</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Anatomy-of-Catastrophic-Forgetting-Hidden-Representations-and-Task-Semantics"/>
   <updated>2021-02-22T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Anatomy of Catastrophic Forgetting - Hidden Representations and Task Semantics</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies the effect of catastrophic forgetting on representations in neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.07400&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Techniques:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Representational Similarity Measures&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Layer Freezing&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Layer Reset&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Split CIFAR-10&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;CIFAR-10 dataset is split into &lt;em&gt;m&lt;/em&gt; (=2) tasks, where each task is a &lt;em&gt;n&lt;/em&gt; way classification task.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The underlying network has a shared trunk with &lt;em&gt;m&lt;/em&gt; heads, one head per task.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Split CIFAR-100 Distribution Shift&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Each task requires distinguishing between &lt;em&gt;n&lt;/em&gt; CIFAR-100 &lt;em&gt;superclasses&lt;/em&gt; with training/test data corresponding to a &lt;em&gt;subset&lt;/em&gt; of constituent classes.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network Architecture&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;VGG, ResNet and DenseNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Are all representations (throughout the network) equally responsible for forgetting?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Higher&lt;/em&gt; layer (layers closer to the output) are the primary source of catastrophic forgetting.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.00414&quot;&gt;Central Kernel Alignment (CKA)&lt;/a&gt; technique is used to compare the similarity between the layer representations, before and after training on the second task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Higher layer representations change significantly when training over two tasks while the lower layer representations remain stable.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;When finetuning on the second task, freezing the lower layers has only a minor effect on the accuracy of the second task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In &lt;em&gt;layer reset&lt;/em&gt; experiments, after training on the second task, the weights of some of the layers are reset to their values after training on the first task.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Resetting the weights of higher layers leads to significant improvement in the performance on the first task.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Do common approaches for countering catastrophic forgetting work by stabilizing the higher layers?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Yes - both &lt;a href=&quot;https://arxiv.org/abs/1612.00796&quot;&gt;EWC&lt;/a&gt; and replay-based approaches counter catastrophic forgetting work by stabilizing the higher layers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This is demonstrated by showing that as the quadratic penalty for EWC (or fraction of data from replay buffer) increases (to reduce catastrophic forgetting), the representations for higher layers change less during the second task.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When training over a sequence of tasks, are similar tasks more likely to be forgotten than different tasks?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Setup I&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Training over a sequence of two binary classification tasks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task 1: Two related classes (say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ship&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck&lt;/code&gt;)&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task 2: Two related classes, which may or may not be related to the classes for Task 1. For example, the classes could be&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;horse&lt;/code&gt; (not related to classes of the first task)&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plane&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;car&lt;/code&gt; (related to the classes of the first task)&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Training over semantically similar tasks (here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plane&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;car&lt;/code&gt;) leads to less forgetting.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Setup II&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Training over a sequence of two classification tasks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task 1: Four classes where the classes can be grouped into two groups (say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deer&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dog&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ship&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck&lt;/code&gt;)&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task 2: Two related classes, which may be related to group 1 or group 2. For example, the classes could be two animals or two objects.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;After training on the second task, classes (from Task 1), which are in the different group as classes from Task 2, are forgotten less.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Conclusion&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Task representational similarity is a function of both underlying data and optimization procedure.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Forgetting is most severe for task representations of intermediate similarity.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Representational similarity is necessary but not a sufficient condition for forgetting.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How does catastrophic forgetting change as the task similarity changes?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If the model learns different representations for dissimilar tasks, increasing dissimilarity can help to avoid forgetting.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;When training the two-task, two-class (per task) CIFAR-10 setup with an “others” class (classes not already used in the setup), the forgetting is reduced.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>When Do Curricula Work?</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/When-Do-Curricula-Work"/>
   <updated>2021-02-15T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/When Do Curricula Work</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper systematically investigates when does curriculum learning help.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.03107&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implicit-curricula&quot;&gt;Implicit Curricula&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Implicit curricula refers to the order in which a network learns data points when trained using stochastic gradient descent, with iid sampling of data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When training, let us say that the model makes a correct prediction for a given datapoint in the $i^{th}$ epoch (and correct prediction in all the subsequent epochs). The $i^{th}$ epoch is referred to as the &lt;em&gt;learned iteration&lt;/em&gt; of the datapoint  (iteration in which the datapoint was learned).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studied multiple models (VGG, ResNet, WideResNet, DenseNet, and EfficientNet) with different optimizers (Adam and SGD with momentum).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting implicit curricula are broadly consistent within the model families, making the following discussion less dependent on the model architecture.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explicit-curricula&quot;&gt;Explicit Curricula&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;When defining an explicit curriculum, three important components stand out.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scoring-function&quot;&gt;Scoring Function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Maps a data point to a numerical score of &lt;em&gt;difficulty&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choices:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Loss function for a model&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;learned iteration&lt;/em&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Estimated c-score - It captures a given model’s consistency to correctly predict a given datapoint’s label when trained on an iid dataset (not containing the datapoint).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The three scoring functions are computed for two models on the CIFAR dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting six scores have a high Spearman Rank correlation. Hence for the rest of the discussion, only the c-score is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pacing-function&quot;&gt;Pacing Function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This function, denoted by $g(t)$, controls the size of the training dataset at step $t$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At step $t$, the model would be trained on the first $g(t)$ examples (as per the ordering).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choices: logarithmic, exponential, step, linear, quadratic, and root.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;order&quot;&gt;Order&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Order in which the data points are picked:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Curriculum&lt;/em&gt; - Ordering points from lowest score to highest and training on the easiest data points first.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Anti Curriculum&lt;/em&gt; - Ordering points from highest score to lowest and training on the hardest data points first.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Random&lt;/em&gt; - Randomly selecting the data points to train on.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper performed a hyperparameter sweep over 180 pacing functions and three orderings for three random seeds over the CIFAR10 and CIFAR100 datasets. For both the datasets, the best performance is obtained with random ordering, indicating that curricula did not give any benefits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, the curriculum is useful when the number of training iterations is small.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also helps with noisy data training (which is simulated by randomly permuting the labels).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The observations for the smaller CIFAR10/100 dataset generalize to slightly larger datasets like FOOD101 and FOOD101N.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Continual learning with hypernetworks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Continual-learning-with-hypernetworks"/>
   <updated>2021-02-08T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Continual learning with hypernetworks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes the use of task-conditioned &lt;a href=&quot;https://shagunsodhani.com/papers-I-read/HyperNetworks&quot;&gt;HyperNetworks&lt;/a&gt; for lifelong learning / continual learning setups.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is, the HyperNetwork would only need to remember the task-conditioned weights and not the input-output mapping for all the data points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.00695&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/chrhenning/hypercl&quot;&gt;Author’s Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$f$ denotes the network for the given $t^{th}$ task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$h$ denotes the HyperNetwork that generates the weights for $f$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\Theta_{h}$ denotes the parameters of $h$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$e^{t}$ denotes the input task-embedding for the $t^{th}$ task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When training on the $t^{th}$ task, the HyperNetworks generates the weights for the network $f$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The current task loss is computed using the generated weights, and the candidate weight update ($\Delta \Theta_{h}$) is computed for $h$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The actual parameter change is computed by the following expression:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$L_{total} = L{task}(\Theta_{h}, e^{T}, X^{T}, Y^{T}) + \frac{\beta_{output}}{T-1} \sum_{t=1}^{T-1} | f_{h}(e^{t}, \Theta_{h}^*) - f_{h}(e^{(t)}, \Theta_{h} + \Delta \Theta_{h} ))|^2$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$L_{task}$ is the loss for the current task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$(X^{T}, Y^{T})$ denotes the training datapoints for the $T^{th}$ task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\beta_{output}$ is a hyperparameter to control the regularizer’s strength.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\Theta_{h}^*$ denotes the optimal parameters after training on the $T-1$ tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\Theta_{h} + \Delta \Theta_{h}$ denotes the one-step update on the current $h$ model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, the task encoding $e^{t}$ is chunked into smaller vectors, and these vectors are fed as input to the HyperNetwork.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This enables the HyperNetwork to produce weights iteratively, instead of all at once, thus helping to scale to larger models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also considers the problem of inferring the task embedding from a given input pattern.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, the paper uses task-dependent uncertainty, where the task embedding with the least predictive uncertainty is chosen as the task embedding for the given unknown task. This approach is referred to as HNET+ENT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also considers using HyperNetworks to learn the weights for a task-specific generative model. This generative model will be used to generate pseudo samples for rehearsal-based approaches. The paper considers two cases:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;HNET+R where the replay model (i.e., the generative model) is parameterized using a HyperNetwork.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HNET+TIR, where an auxiliary task inference classifier is used to predict the task identity.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three setups are considered&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CL1 - Task identity is given to the model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;CL2 - Task identity is not given, but task-specific heads are used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;CL3 - Task identity needs to be explicitly inferred.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the permuted MNIST task, the proposed approach outperforms baselines like Synaptic Intelligence and Online EWC, and the performance gap is more significant for larger task sequences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forward knowledge transfer is observed with the CIFAR datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One potential limitation (which is more of a limitation of HyperNetworks) is that HyperNetworks may be harder to scale for larger models like ResNet50 or transformers, thus limiting their usefulness for lifelong learning use cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Zero-shot Learning by Generating Task-specific Adapters</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Zero-shot-Learning-by-Generating-Task-specific-Adapters"/>
   <updated>2021-02-01T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Zero-shot Learning by Generating Task-specific Adapters</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces HYPTER - a framework for zero-shot learning (ZSL) in text-to-text transformer models by training a &lt;a href=&quot;https://shagunsodhani.com/papers-I-read/HyperNetworks&quot;&gt;&lt;strong&gt;Hyp&lt;/strong&gt;erNetwork&lt;/a&gt; to generate task-specific &lt;a href=&quot;https://arxiv.org/abs/1902.00751&quot;&gt;adap&lt;strong&gt;ter&lt;/strong&gt;s&lt;/a&gt; from task descriptions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The focus is on &lt;em&gt;in-task&lt;/em&gt; zero-shot learning (e.g., learning to predict an unseen class or relation) and not on &lt;em&gt;cross-task&lt;/em&gt; learning (e.g., training on sentiment analysis and evaluating on question-answering task).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2101.00420&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Task&lt;/em&gt; - a NLP task, like classification or question answering.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Sub-task&lt;/em&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A class/relation/question within a task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Denotes by a tuple $(d, D)$ where $d$ is the language description while $D$ represents the subtask’s dataset.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Develop ZSL approach for transfer to new subtasks within a task, using the task description available for each subtask.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HYPTER has two main parts:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Main network&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;A pretrained text-to-text network&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Instantiated as a BERT-Base/Large&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HyperNetwork&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Generates the weights for adapter networks that will be plugged into the main network.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HyperNetwork has two parts:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Encoder&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Encodes the task description&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Instantiated as a RoBERTa-Base model&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Decoder&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Decodes the encoding into weights for multiple adapters (in parallel)&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Instantiated as a Feedforward Network&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model trains in two phases:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Main network is trained on all the data by concatenating the task description with the input.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Adapters are trained by sampling a task from the train set while keeping the main network frozen.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;While the idea is very promising and interesting, the evaluation felt quite limited. It uses just two datasets &lt;a href=&quot;https://leaderboard.allenai.org/zest/submissions/public&quot;&gt;Zero-shot learning from Task Descriptions&lt;/a&gt; and &lt;a href=&quot;https://eval.ai/web/challenges/challenge-page/689/overview&quot;&gt;Zero-shot Relation Extraction&lt;/a&gt; and shows some improvements over the baseline of directly finetuning with task descriptions as the prompt.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>HyperNetworks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/HyperNetworks"/>
   <updated>2021-01-25T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/HyperNetworks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores HyperNetworks. The idea is to use one network (HyperNetwork) to generate the weights for another network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.09106&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/hardmaru/supercell/blob/master/supercell.py&quot;&gt;Author’s implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;h3 id=&quot;static-hypernetworks---hypernetworks-for-cnns&quot;&gt;Static HyperNetworks - HyperNetworks for CNNs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a $D$ layer CNN where the parameters for the $j^{th}$ layer are stored in a matrix $K^j$ of the shape $N_{in}f_{size} \times N_{out}f_{size}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The HyperNetwork is implemented as a two-layer linear network where the input is a layer embedding $z^j$, and the output is $K^j$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first layer (of the HyperNetwork) maps the input to $N_{in}$ different outputs using $N_{in}$ weight matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second layer maps the different $N_{in}$ inputs to $K_{i}$ using a shared matrix. The resulting $N_{in}$ (number of) $K_{i}$ matrices are concatenated to obtain $K^j$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a side note, HyperNetworks have much fewer params than the network for which it produces weights.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a general case, the kernel dimensions (across layers) are not of the same size but integer multiples of some basic sizes. In that case, the HyperNetwork can generate kernels for the basic size, which can be concatenated to form larger kernels. This would require additional input embeddings but not require a change in the architecture of HyperNetwork.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-hypernetworks---hypernetworks-for-rnns&quot;&gt;Dynamic HyperNetworks - HyperNetworks for RNNs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HyperRNNs/HyperLSTMs denote HyperNetworks that generates weights for RNNs/LSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HyperRNNs implement a form of relaxed weight sharing - an alternative to the full weight sharing of the traditional RNNs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At any timestamp $t$, the input to the HyperRNN is the concatenated vector $x_{t}$ (input to the RNN at time $t$) and the hidden state $h_{t-1}$ of the RNN. The output is the weight for the main RNN at timestep $t$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, a &lt;em&gt;weight scaling vector&lt;/em&gt; $d$ is used to reduce the memory footprint, which would otherwise be $dim$ times the memory of a standard RNN. $dim$ is the dimensionality of the embedding vector $z_j$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HyperNetworks are used to train standard CNNs for MNIST and ResNets for CIFAR 10. In these experiments, HyperNetworks slightly underperform the best performing models but uses much fewer parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HyperLSTMs trained on the Penn Treebank dataset and Hutter Prize Wikipedia dataset outperform the stacked LSTMs and perform similar to layer-norm LSTMs. Interestingly, using HyperLSTMs with layer-norm improves performance over HyperLSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the similar performance of HyperLSTMs and layer-norm LSTMs, the paper conducted an ablation study to understand if HyperLSTMs learned a weight adjustment policy similar to the statistics-based approach used by layer-norm LSTMs.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;However, the analysis of the histogram of the hidden states suggests that using layer-norm reduces the saturation effect while in HyperLSTMs, the cell is saturated most of the time. This indicates that the two models are learning different policies.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HyperLSTMs are also evaluated for handwriting sequence generation by training in the IAM online handwriting dataset.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;While HyperLSTMs are quite effective on this task, combining them with layer-norm degrades the performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the WMT’14 En-to-Fr machine translation task, HyperLSTMs outperform LSTM based approaches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Energy-based Models for Continual Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Energy-based-Models-for-Continual-Learning"/>
   <updated>2021-01-18T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Energy-based Models for Continual Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to use Energy-based Models (EBMs) for Continual Learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In classification tasks, the standard approach uses a cross-entropy objective function along with a normalized probability distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, cross-entropy reduces all negative classes’ likelihood when updating the model for a given sample, potentially leading to catastrophic forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Classification can be seen as learning an EBM across separate classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During an update, the energy for a pair of samples and its ground truth class decreases while the energy corresponding to the pairs of sample and negative classes increases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike the cross-entropy loss, EBMs allow choosing the negative classes to update.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.12216&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;applications-of-ebms-for-continual-learning&quot;&gt;Applications of EBMs for Continual Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;EBMs can be used for class-incremental learning without requiring a replay-buffer or generative model for replay.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EBMs can be used for continual learning in setups without task boundaries, i.e., setups where the data distribution can change without a clear separation between tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ebms&quot;&gt;EBMs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Boltzman distribution is used to define the conditional likelihood of label $y$, given an input $x$. ie, $p(y|x) = \frac{exp(E(x, y))}{Z(x)}$ where $Z(x) = \sum_{y \in Y}(-E(x, y))$. Here $E$ is the learnt energy function that maps an input-label pair to a scalar energy value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the contrastive divergence loss is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During inference, the class, for which the input-class pair has the least energy, is selected as the predicted class.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ebms-for-continual-learning&quot;&gt;EBMs for Continual Learning&lt;/h2&gt;

&lt;h3 id=&quot;selection-of-negative-samples&quot;&gt;Selection of Negative Samples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers several strategies for the selection of negative samples:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;one negative class per sample. The negative class is sampled from the current batch of data. This selection approach performs best.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;all the negative classes in a batch are used for creating the negative samples.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;all the classes seen so far in training are used as the negative samples. This approach works the worst in practice.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the flexibility of sampling the negative classes, EBMs can be used in the boundary-agnostic setups (where the data distribution can change smoothly without an explicit task boundary).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;energy-network&quot;&gt;Energy Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;EBMs take both the sample and the class as the input. The class can be treated as an attention filter to select the most relevant information between the sample and the class.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In theory, EBMs can train for any number of classes without knowing the number of classes beforehand. This is an advantage over the softmax-based approaches, where adding new classes requires changing the size of the softmax output layer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;During inference, all the classes seen so far are evaluated via the energy function. The class, which corresponds to the least energy sample-class pair, is returned as the selected class.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Split MNIST&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Permuted MNIST&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CIFAR-10&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CIFAR-100&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results-in-boundary-aware-setting&quot;&gt;Results in Boundary-Aware Setting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper outperforms the standard continual learning approaches that neither uses a replay-buffer nor a generative model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, the paper shows that for the same number of parameters, the effective capacity of EMB models is higher than the effective capacity of standard classification models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also shows that standard classification models tend to assign a high probability to new classes for both old and new data. EBMs assign the probability more uniformly (and correctly) across the classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In an ablation study, the paper shows that both label conditioning and contrastive divergence loss help in improving the performance of EBMs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results-in-boundary-agnostic-setting&quot;&gt;Results in Boundary-Agnostic Setting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The performance gains in the boundary-agnostic setting are even more significant than the improvements in the boundary-aware setting.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GPipe - Easy Scaling with Micro-Batch Pipeline Parallelism</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/GPipe-Easy-Scaling-with-Micro-Batch-Pipeline-Parallelism"/>
   <updated>2021-01-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/GPipe - Easy Scaling with Micro-Batch Pipeline Parallelism</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces GPipe, a pipeline parallelism library for scaling networks that can be expressed as a sequence of layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.06965&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider training a deep neural network with &lt;em&gt;L&lt;/em&gt; layers using &lt;em&gt;K&lt;/em&gt; accelerators (say GPUs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each of the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer has its &lt;em&gt;forward&lt;/em&gt; function &lt;em&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;, &lt;em&gt;backward&lt;/em&gt; function &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;, weights &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and a cost &lt;em&gt;c&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; (say the memory footprint or computational time).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPipe partitions this network into &lt;em&gt;K&lt;/em&gt; cells and places the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; cell on the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; accelerator. Output from the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; accelerator is passed to the &lt;em&gt;i+1&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; accelerator as input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the forward pass, the input batch (of size &lt;em&gt;N&lt;/em&gt;) is divided into &lt;em&gt;M&lt;/em&gt; equal micro-batches. These micro-batches are pipelined through the &lt;em&gt;K&lt;/em&gt; accelerators one after another.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the backward pass, gradients are computed for each micro-batch. The gradients are accumulated and applied at the end of each minibatch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In batch normalization, the statistics are computed over each micro-batch (used during training) and mini-batch (used during evaluation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Micro-batching improves over the naive mode parallelism approach by reducing the underutilization of resources (due to the network’s sequential dependencies).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-optimization&quot;&gt;Performance Optimization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GPipe supports re-materialization (or checkpointing), i.e., during the forward pass, only the output activations (at partition boundaries) are stored.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During backward pass, the forward function is recomputed at each accelerator. This trades off the memory requirement with increased time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One potential downside is that partitioning can introduce some idle time per accelerator (referred to as the bubble overhead). However, with a sufficiently large number of micro-batches ( more than 4 times the number of partitions), the bubble overhead is negligible.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-analysis&quot;&gt;Performance Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two different types of model architectures are compared: AmoebaNet convolutional model and Transformer sequence-to-sequence model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For AmoebaNet, the size of the largest trainable model (on a single 8GB Cloud TPU v2) increases from 82M to 318M. Further, a 1.8 billion parameter model can be trained on 8 accelerators (25x improvement in size using GPipe).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For transformers, GPipe scales the model size to 83.9 B parameters with 128 partitions (298x improvement in size compared to a single accelerator).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the computation is evenly distributed across transformer layers, the training throughput scales almost linearly with the number of devices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Quantitative experiments on ImageNet and multilingual machine translation show that models can be effectively trained using GPipe.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Compositional Explanations of Neurons</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Compositional-Explanations-of-Neurons"/>
   <updated>2021-01-04T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Compositional Explanations of Neurons</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes a method to explain/interpret the representations learned by individual neurons in deep neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The explanations are generated by searching for logical forms defined by a set of composition operators (like OR, AND, NOT) over primitive concepts (like water).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.14032&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generating-compositional-explanations&quot;&gt;Generating compositional explanations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a neural network &lt;em&gt;f&lt;/em&gt;, the goal is to explain a neuron’s behavior (of this network) in human-understandable terms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://netdissect.csail.mit.edu/&quot;&gt;Previous work&lt;/a&gt; builds on the idea that a good explanation is a description that identifies the inputs for which the neuron activates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a set of pre-defined atomic concepts $c \in C$ and a similarity measure $\delta(n, c)$ where $n$ represents the activation of the $n^{th}$ neuron, the explanation, for the $n^{th}$ neuron, is the concept most similar to $n$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For images, a concept could be represented as an image segmentation map. For example, the water concept can be represented by the segments of the images that show water.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The similarity can be measured by first thresholding the neuron activations (to get a neuron mask) and then computing the IoU score (or Jaccard Similarity) between the neuron mask and the concept.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One limitation of this approach is that the explanations are restricted to pre-defined concepts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper expands the set of candidate concepts by considering the logical forms of the atomics concepts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In theory, the search space would explode exponentially. In practice, it is restricted to explanations with at most $N$ atomics concepts, and beam search is performed (instead of exhaustive search).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Image Classification Setup&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Neurons from the final 512-unit convolutional layer of a ResNet-18 trained on the &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/7968387&quot;&gt;Places365 dataset&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Probing for concepts from &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Scene_Parsing_Through_CVPR_2017_paper.html&quot;&gt;ADE20k scenes dataset&lt;/a&gt; with atomic concepts defined by annotations in the &lt;a href=&quot;http://netdissect.csail.mit.edu/&quot;&gt;Broden dataset&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;NLI Setup&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;BiLSTM baseline followed by MLP layers trained on &lt;a href=&quot;https://nlp.stanford.edu/projects/snli/&quot;&gt;Stanford Natural Language Inference (SNLI) corpus&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Probing the penultimate hidden layer (of the MLP component) for sentence-level explanations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Concepts are created using the 2000 most common words in the validation split of the SNLI dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Additional concepts are created based on the lexical overlap between premise and hypothesis.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;do-neurons-learn-compositional-concepts&quot;&gt;Do neurons learn compositional concepts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Image Classification Setup&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;As $N$ increases, the mean IoU increases (i.e., the explanation quality increases) though the returns become diminishing beyond $N=10$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Manual inspection of 128 neurons and their length 10 explanations show that 69% neurons learned some meaningful combination of concepts, while 31% learned some unrelated concepts.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The meaningful combination of concepts include:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;perceptual abstraction that is also lexically coherent (e.g., “skyscraper OR lighthouse OR water tower”).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;perceptual abstraction that is not lexically coherent (e.g., “cradle OR autobus OR fire escape”).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;specialized abstraction of the form L1 AND NOT L2 (e.g. (water OR river) AND NOT blue).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;NLI Setup&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;As $N$ increases, the mean IoU increases (as in the image classification setup) though the IoU keeps increasing past $N=30$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Many neurons correspond to lexical features. For example, some neurons are gender-sensitive or activate for verbs like sitting, eating or sleeping. Some neurons are activated when the lexical overlap between premise and hypothesis is high.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;do-interpretable-neurons-contribute-to-model-accuracy&quot;&gt;Do interpretable neurons contribute to model accuracy?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In image classification setup, the more interpretable the neuron is, the more accurate is the model (when the neuron is active).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, the opposite trend is seen in NLI models. i.e., the more interpretable neurons are less accurate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Key takeaway - interpretability (as measured by the paper) is not correlated with performance. Given a concept space, the identified behaviors may be correlated or anti-correlated with the model’s performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;targeting-explanations-to-change-model-behavior&quot;&gt;Targeting explanations to change model behavior&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to construct examples that activate (or inhibit) certain neurons, causing a change in the model’s predictions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These adversarial examples are referred to as “copy-paste” adversarial examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, the neuron corresponding to “(water OR river) AND (NOT blue)” is a major contributor for detecting “swimming hole” classes. An adversarial example is created by making the water blue. This prompts the model to predict “grotto” instead of “swimming hole.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, in the NLI model, a neuron detects the word “nobody” in the hypothesis as highly indicative of contradiction. An adversarial example can be created by adding the word “nobody” to the hypothesis, prompting the model to predict contradiction while the true label should be neutral.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These observations support the hypothesis that one can use explanations to create adversarial examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Design patterns for container-based distributed systems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Design-patterns-for-container-based-distributed-systems"/>
   <updated>2020-12-21T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Design patterns for container-based distributed systems</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper describes three design patterns for container-based distributed systems: single-container pattern, single-node pattern, and multi-node pattern.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;single-container-management-patterns&quot;&gt;Single-container management patterns&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Traditionally, containers have exposed three functions: run, pause and stop.&lt;/li&gt;
  &lt;li&gt;A richer API can be implemented to provide fine-grained control to system developers and operators.&lt;/li&gt;
  &lt;li&gt;Similarly, much more application information (including monitoring metrics) can be exposed.&lt;/li&gt;
  &lt;li&gt;The container interface can be used to define a contract for a complex lifecycle. For example, instead of arbitrarily shutting down the container, the system could signal that it will be terminated, giving the container some time to perform some cleanup/follow-up actions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;single-node-multi-container-pattern&quot;&gt;Single-node, multi-container pattern&lt;/h2&gt;

&lt;h3 id=&quot;sidecar-pattern&quot;&gt;Sidecar pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Multiple containers extend and enhance the main container.&lt;/li&gt;
  &lt;li&gt;For example, a web-server serves from the local disk (main container) while a side container updates the data.&lt;/li&gt;
  &lt;li&gt;Benefits:
    &lt;ul&gt;
      &lt;li&gt;independent development, deployment, and scaling of containers&lt;/li&gt;
      &lt;li&gt;possibility of combining different type of containers&lt;/li&gt;
      &lt;li&gt;failure containment boundary, i.e., one failing container, need not bring down the entire system.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ambassador-pattern&quot;&gt;Ambassador pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Proxy communication to and from the main container with the ambassador hiding the complexities of communication with a distributed (multi-shard system) that may be written in a different language.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adapter-pattern&quot;&gt;Adapter pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Standardize output and interfaces across the containers to provide a simple, homogenized view to external applications.&lt;/li&gt;
  &lt;li&gt;A common example is using a single tool for collecting/processing metrics from multiple applications.&lt;/li&gt;
  &lt;li&gt;This is different from the adapter pattern, which aims to provide a simplified view of the external world to an application.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-node-application-patterns&quot;&gt;Multi-node application patterns&lt;/h2&gt;

&lt;h3 id=&quot;leader-election-pattern&quot;&gt;Leader election pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In a sharded (or replication-based) system, the system may have to elect a leader (or multiple leaders) among the replicas (or shards).&lt;/li&gt;
  &lt;li&gt;Instead of using a leader election library, a leader election container can be used (that communicates with other containers over, say, HTTP). This removes the restriction of using a leader election library compatible with the containers (e.g., using the same language).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;work-queue-pattern&quot;&gt;Work queue pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A work coordinator container can queue different containers, each of which may have a different implementation or dependencies, thus removing the restriction that all the works use the same runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scattergather-pattern&quot;&gt;Scatter/gather pattern&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;An external client sends a request to a root container.&lt;/li&gt;
  &lt;li&gt;This container fans out the request to many containers that may perform the computation in parallel.&lt;/li&gt;
  &lt;li&gt;The root container gathers these parallel computations’ results and aggregates them into a response to the external client.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Cassandra - a decentralized structured storage system</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Cassandra-a-decentralized-structured-storage-system"/>
   <updated>2020-12-14T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Cassandra - a decentralized structured storage system</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Cassandra is a distributed storage system that runs over cheap commodity servers and handles high write throughput while maintaining low latency for read operations.&lt;/li&gt;
  &lt;li&gt;At the time of writing, it was used to support the search for Facebook Inbox.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/1773912.1773922&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cassandra.apache.org/&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-model&quot;&gt;Data Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A table is a distributed multidimensional map.&lt;/li&gt;
  &lt;li&gt;The key is a string (generally 16-36 bytes long), while the value is a structured object.&lt;/li&gt;
  &lt;li&gt;Every operation under a single row key is atomic per replica.&lt;/li&gt;
  &lt;li&gt;Columns are grouped together into sets called column families.&lt;/li&gt;
  &lt;li&gt;There are two types of columns families:
    &lt;ul&gt;
      &lt;li&gt;Simple families.&lt;/li&gt;
      &lt;li&gt;Super column families: visualized as a column family within a column family.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Columns can be sorted by name or time (used to display results in time sorted order).&lt;/li&gt;
  &lt;li&gt;The API supports insert, get and delete operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;system-architecture&quot;&gt;System Architecture&lt;/h2&gt;

&lt;h3 id=&quot;handling-requests&quot;&gt;Handling Requests&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Any read/write request gets routed to any node in the cluster. The node determines the replicas for a given key and routes the request.&lt;/li&gt;
  &lt;li&gt;For write query, the system waits for a quorum of replicas to acknowledge the writes’ completion.&lt;/li&gt;
  &lt;li&gt;For read query, the system either routes the requests to the closest replica (might fetch stale results) or routes the requests to all replicas and waits for a quorum of responses.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;partitioning&quot;&gt;Partitioning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Cassandra partitions data across the cluster using consistent hashing with an order-preserving hash function.&lt;/li&gt;
  &lt;li&gt;The hash function’s output range is treated as a fixed circular ring, and each node is assigned a random position on the ring.&lt;/li&gt;
  &lt;li&gt;An incoming request specifies a key used to route requests.&lt;/li&gt;
  &lt;li&gt;One benefit of this approach is that the addition/removal of a node only affects its immediate neighbors.&lt;/li&gt;
  &lt;li&gt;However, randomly assigning nodes leads to non-uniform data and load distribution.&lt;/li&gt;
  &lt;li&gt;Cassandra uses the load information and moves lightly loaded nodes to reduce the load on other nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;replication&quot;&gt;Replication&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Each data item is replicated at N hosts, where N is the per-instance replication factor.&lt;/li&gt;
  &lt;li&gt;Cassandra supports the following replication policies: Rack Unaware, Rack Aware (within a datacenter), and Datacenter Aware.&lt;/li&gt;
  &lt;li&gt;For “Rack Aware” and “Datacenter Aware” strategies, Zookeeper elects a leader among the nodes and holds metadata about which range a node is responsible for.&lt;/li&gt;
  &lt;li&gt;In case of node failure and network partitions, the quorum requirements are relaxed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;membership&quot;&gt;Membership&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Cluster membership is based on Scuttlebutt, a very efficient anti-entropy Gossip based mechanism.&lt;/li&gt;
  &lt;li&gt;Cassandra uses a modified version of $\phi$ Accrual Failure Detector for detecting failures, which provides the suspicion level (of failure) for each node.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bootstrapping&quot;&gt;Bootstrapping&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A node, starting for the first time, chooses a random position in the ring.&lt;/li&gt;
  &lt;li&gt;This information is persisted on the local disk, on Zookeeper, and gossiped around the cluster (so any node can route any query in the cluster).&lt;/li&gt;
  &lt;li&gt;During bootstrapping, the newly joined node reads a list of contact points (within the cluster) using a configuration file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-persistence&quot;&gt;Local Persistence&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Generally, a write operation involves a write into a commit log (for durability and recoverability), followed by a write into the in-memory data structures.&lt;/li&gt;
  &lt;li&gt;A read operation starts with querying the in-memory data and then looks into the filesystem.&lt;/li&gt;
  &lt;li&gt;Read queries on the filesystem use bloom filters.&lt;/li&gt;
  &lt;li&gt;Column indices are maintained to make it faster to look up relevant columns.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Components implemented in Java.&lt;/li&gt;
  &lt;li&gt;System control messages use UDP while messages for replication and request routing uses TCP.&lt;/li&gt;
  &lt;li&gt;A new commit log is rolled out after the older one exceeds 128MB of size.&lt;/li&gt;
  &lt;li&gt;All the data is indexed using a primary key.&lt;/li&gt;
  &lt;li&gt;Data on the disk is chunked into sequences of blocks. Each block contains at most 128 keys and is demarcated by a block index.&lt;/li&gt;
  &lt;li&gt;When the data is written to the disk, a block index is generated and maintained in the memory for faster access.&lt;/li&gt;
  &lt;li&gt;A compaction process is performed to merge multiple files (on disk) into one file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-experience&quot;&gt;Practical Experience&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data from MySQL servers is added to Cassandra using MapReduce processes.&lt;/li&gt;
  &lt;li&gt;Although Cassandra is a completely decentralized system, adding some coordination (via Zookeeper) is helpful.&lt;/li&gt;
  &lt;li&gt;For Inbox Search, a per-user index is maintained for all the messages.&lt;/li&gt;
  &lt;li&gt;For “term search”, the key is the userid, and the words in the message become the super column.&lt;/li&gt;
  &lt;li&gt;For searching all the messages ever sent/received by a user, the key is the userid, and the recipient ids are the super columns.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>CAP twelve years later - How the rules have changed</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/CAP-twelve-years-later-How-the-rules-have-changed"/>
   <updated>2020-12-07T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/CAP twelve years later - How the rules have changed</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The CAP theorem states that any system sharing data over the network can only have at most two (out of three) desirable properties:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;consistency (C), i.e., a single, up-to-date copy of the data;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;high availability (A) of that data (for updates); and&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;tolerance to network partitions (P).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This “2 of 3” formulation is misleading as it oversimplifies the interplay between properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6133253&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acid-vs-base&quot;&gt;ACID vs. BASE&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ACID is a design philosophy that focuses on consistency as reflected in the traditional relational databases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The four properties in ACID are:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Atomicity (A), i.e., the operations are atomic, and either the entire operation succeeds or none of it succeeds.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Consistency (C), i.e., a transaction preserves all the rules. Note that the consistency in CAP is a subset of consistency in ACID.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Isolation (I), i.e., transactions occur in isolation and do not affect each other.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Durability (D), i.e., the transactions are durable irrespective of system failure.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BASE is an alternate design philosophy that focuses on availability as reflected in the NoSQL databases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The four properties in BASE are:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Basic Availability (BA), i.e., the database appears to work most of the time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Soft state (S), i.e., the system’s state can change over time as it becomes eventually consistent.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Eventual consistency (E), i.e., the system will eventually become consistent over time.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cap-confusion&quot;&gt;CAP confusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, partitionability is seen as a must-have, thus reducing the choice to be between availability and consistency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This view is somewhat misleading because the choice between C, A, and P is not binary but granular.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The choice between C and A can occur at various granularity levels, and different components (of a larger system) can prioritize different aspects.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, the CAP theorem generally ignores latency even though it is closely related to partitionability. For example, failing to achieve consistency within a time-bound (i.e., latency) implies a partition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, there is no global notion of partition - some subset of nodes may experience a partition, and others may not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once a partition is detected, the system can then choose between C and A.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;managing-partitions&quot;&gt;Managing Partitions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three-step process for managing partitions:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Detect the start of a partition.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Enter an explicit partition mode that may limit some operations.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Possible strategies:&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Reduce availability by limiting some operations.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Record extra information that can be used during partition recovery.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The strategy depends on the invariants that the system should maintain.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For example, if the invariant is that the keys (in a table) should be unique, the system could allow duplicate keys for some time and perform a de-duplication step during partition recovery.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;A counterexample is a monetary transaction (e.g., charging a credit card). In such cases, the system could disable the operation and record it for performing later. Sometimes this “unavailability” is not visible to the user.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;History of operations (over replicas across different partitions) can be tracked using version vectors of the form (node, logical time). The system can easily recreate the order in which they were executed (or mark them as being concurrent).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Initiate partition recovery when communication is restored and make the state across the partitions consistent.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;One common approach is to revert to the state when the partition was detected and apply the operations consistently across all the replicas.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This may require some extra effort to merge conflicts.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;One workaround can be to constrain the use of certain operations so that the system does not encounter merge conflicts during recovery.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sometimes, certain invariants may be violated when the system is in the partition mode and needs to be fixed during recovery.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The key takeaway is that when partitions exist, the choice between availability and consistency is not binary, and both can be optimized for.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Consistency Tradeoffs in Modern Distributed Database System Design</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Consistency-Tradeoffs-in-Modern-Distributed-Database-System-Design"/>
   <updated>2020-11-30T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Consistency Tradeoffs in Modern Distributed Database System Design</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CAP theorem has been influential in the design decisions for distributed databases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, designers incorrectly assume that the CAP theorem “always” imposes restrictions in terms of the tradeoff between availability and consistency. In contrast, the tradeoff is applicable only in the case of partitions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CAP theorem led to the development of highly available systems with reduced consistency models (and reduced ACID guarantees).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another tradeoff - between latency and consistency - has also been influential for database design.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper unifies CAP and latency-consistency tradeoffs into a single formulation called PACELC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that some of the observations, especially ones about the databases, may be outdated now (the paper was written in 2012). However, the core message is still relevant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;latency-consistency-tradeoff&quot;&gt;Latency-Consistency Tradeoff&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low latency (or high availability) means that the system must replicate data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of an update query, three possibilities arise:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The system can choose to send data updates to all the replicas at once. This leads to two possibilities:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;A replica can receive the update queries in an arbitrary order, thus breaking consistency with other replicas.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Alternatively, the replicas could use some protocol to agree on the order of updates. However, this can introduce latency.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The update queries can be first sent to a master replica.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The master replica can apply the updates and send them to the other replicas using one of the following strategies:&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;Synchronous replication where the master waits for all the updates to be applied to a replica(s). However, this approach introduces latency.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Asynchronous replication where the master assumes the update to be complete before it completes. In this case, the latency-consistency tradeoff depends on how read queries are handled:&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;p&gt;The system can send all read queries to the master. In this case, there are no consistency issues, but additional latency is introduced because all the read queries go to the same replica, thus potentially overloading it.&lt;/p&gt;
                  &lt;/li&gt;
                  &lt;li&gt;
                    &lt;p&gt;Alternatively, the read query can be served from any replica. While this improves read latency, the results can be inconsistent now.&lt;/p&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;Use a mix of Synchronous and Asynchronous replication - i.e., some of the write queries are Synchronous, and others are Asynchronous. In this case, the latency-consistency tradeoff depends on how read queries are handled:&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;p&gt;If the read is routed to at least one replica that has been Synchrnously updated, the consistency can be preserved, with additional latency for discovering the updated replica, etc.&lt;/p&gt;
                  &lt;/li&gt;
                  &lt;li&gt;
                    &lt;p&gt;If the read query can not be routed to an updated replica (maybe because none of the replicas is updated), then either latency suffers or inconsistent read can be performed.&lt;/p&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The update query is first sent to an arbitrary replica.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;This is the same as the previous case, with the query going to an arbitrary replica instead of the master replica, and suffers from the same latency issues as the last case.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a nutshell, the tradeoff between latency and consistency  is always present, irrespective of network failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This contrasts with the CAP theorem, which imposes the tradeoff between availability and consistency only in the case of a network partition.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pacelc&quot;&gt;PACELC&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If there is a partition (P), how does the system tradeoff availability (A) and consistency (C); else (E), when the system is running without failures, how does the system tradeoff latency (L) and consistency (C)?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The latency-consistency tradeoff (ELC) is relevant only when the data is replicated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Default versions of Dynamo, Cassandra, and Riak were PA/EL systems, i.e., if a partition occurs, availability is prioritized. In the absence of partition, lower latency is prioritized.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fully ACID systems (VoltDB, H-Store, and Megastore) and others like BigTable and HB are PC/EC, i.e., they prioritize consistency and give up availability and latency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MongoDB can be classified as a PA/EC system, while PNUTS is a PC/EL system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Exploring Simple Siamese Representation Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Exploring-Simple-Siamese-Representation-Learning"/>
   <updated>2020-11-23T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Exploring Simple Siamese Representation Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that Siamese networks can be used for unsupervised learning with images without needing techniques like negative sample pairs, large batch training, or momentum encoders. The training mechanism is referred to as the SimSiam method.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.10566&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input image &lt;em&gt;x&lt;/em&gt;, create two augmented views &lt;em&gt;x1&lt;/em&gt; and &lt;em&gt;x2&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These views are processed by an encoder network &lt;em&gt;f&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One of the views (say &lt;em&gt;x1&lt;/em&gt;) is processed by the encoder &lt;em&gt;f&lt;/em&gt; as well as a predictor MLP &lt;em&gt;h&lt;/em&gt; to obtain a projection &lt;em&gt;p1&lt;/em&gt; ie &lt;em&gt;p1 = h(f(x1))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second view (&lt;em&gt;x2&lt;/em&gt;) is processed only by the encoder &lt;em&gt;f&lt;/em&gt; to obtain an encoding &lt;em&gt;z2&lt;/em&gt; i.e., &lt;em&gt;z2 = f(x2)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Negative cosine similarity is minimized between &lt;em&gt;p1&lt;/em&gt; and &lt;em&gt;z2&lt;/em&gt; with the catch that the resulting gradients are not used to update the encoder via &lt;em&gt;z2&lt;/em&gt;. I.e., Loss = &lt;em&gt;D(p1, stopgrad(z2))&lt;/em&gt; where &lt;em&gt;D&lt;/em&gt; is the negative cosine similarity and &lt;em&gt;stopgrad&lt;/em&gt; is an operation that stops the flow of gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, both &lt;em&gt;p1, z2&lt;/em&gt; and &lt;em&gt;p2, z1&lt;/em&gt; pairs are used for computing the loss. ie  Loss = &lt;em&gt;0.5 * (D(p1, stopgrad(z2)) + D(p2, stopgrad(z1)))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder uses batch norm in all the layers (including output) while projection MLP uses batch norm only in the hidden layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SGD optimizer with learning rate as &lt;em&gt;0.05 * batchsize / 256&lt;/em&gt;, cosine learning rate decay schedule and SGD momentum = 0.9.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised pretraining on the ImageNet dataset followed by training a supervised linear classifier on the frozen representations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stop-gradient operation is necessary to avoid a degenerate solution. Without stop-gradient, the model maps all inputs to a constant &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the projection layer is removed, the method does not work (because of the loss’s symmetric nature). If the loss is also made asymmetric, the method still does not work without the projection layer. However, asymmetric loss + projection layer works.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Keeping the projection layer fixed (i.e., not updating during training) avoids collapse but leads to poor validation performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training the projection layer with a constant learning rate works better in practice, likely because the projection layer needs to keep adapting before the encoder layer is sufficiently trained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The method works well across different batch sizes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing batch norm layers from all the layers in all the networks does not lead to collapse, though the model’s performance degrades on the validation dataset. Adding batch norm to the hidden layers alone is sufficient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding batch norm to the encoder’s output further improves the performance but adding batch norm to all the layers of all the networks makes the training unstable, with the loss oscillating.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, while batch norm helps to improve performance, it is not sufficient to avoid collapse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup does not collapse when the cross-entropy loss replaces the cosine loss.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-simsiam-solving&quot;&gt;What is SimSiam solving?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the stop-gradient operation seems to be the critical ingredient for avoiding collapse, the paper hypothesizes that SimSiam is solving a different optimization problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hypothesis is that SimSiam is implementing an Expectation-Maximisation (EM) algorithm with two sets of variables and two underlying sub-problems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper performs several experiments to test this hypothesis. For example, they consider &lt;em&gt;k&lt;/em&gt; SGD steps for the first problem before performing an update for the second problem, showing that the alternating optimization is a valid formulation, of which SimSiam is a particular case.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comparison-to-other-methods&quot;&gt;Comparison to other methods&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SimSiam achieves the highest accuracy among SimCLR, MoCo, BYOL, and SwAV for training under 100 epochs. However, it lags behind other methods when trained longer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SimSiam’s representations are transferable beyond the ImageNet tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding projection layer and stop-gradient operator to SimCLR does not improve its performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Data Management for Internet-Scale Single-Sign-On</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Data-Management-for-Internet-Scale-Single-Sign-On"/>
   <updated>2020-11-16T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Data Management for Internet-Scale Single-Sign-On</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes the architecture of an erstwhile single-sign-on (SSO) service used by Google, called Google Accounts (2006).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that some of the metrics and design decisions may be outdated now (the paper was written in 2006). However, the core message is still relevant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.usenix.org/legacy/event/worlds06/tech/prelim_papers/perl/perl.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;operational-constraints&quot;&gt;Operational Constraints&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SSO’s availability affects the availability of all applications that require user sign-in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, systems can achieve high availability by sacrificing consistency, but given the nature of SSO (matching username/passwords), providing an inconsistent view is not a good option, and single-copy consistency is a usability requirement.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;berkeley-db&quot;&gt;Berkeley DB&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Berkeley DB is an embedded, high-performance, scalable, transactional storage system for key-value data and provides both keyed and sequential lookup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It provides a primary copy replication model with a single writer (called master) and multiple read-only replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All writes are sent to the master, which first applies the changes and then propagates them to the replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The master and the replicas have identical logs, and in case of master failure, a new master is elected from the replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some synchronization may be needed between the replicas in case, e.g., the master dies in between a transaction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sso-architecture&quot;&gt;SSO Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SSO service maps usernames to user account data and services to service-specific data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The SSO database is partitioned into shards, where each shard is a replicated Berkeley DB (having 5 to 15 replicas).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each replica stores the data in a B+-link tree data structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consistent reads must go to the master, while non-master replicas can serve “ stale” reads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the case of larger replication groups (say 15 replicas), only a subset of replicas can become master (“electable replicas”).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, replicas are spread geographically to handle machine-failure, network-failure, and data center-failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replicas in a share are kept close to reduce the communication latency, which affects the time to commit a write operation or electing a new master.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some of the shards implement ID-map, i.e., map of username to userid and userid to shards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;database-integration&quot;&gt;Database Integration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Berkeley DB leaves decisions regarding quorums, leases, etc., up to the application.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;quorums&quot;&gt;Quorums&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SSO chooses a quorum protocol that guarantees that updates are never lost.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the write queries, the master waits for a positive acknowledgment from a majority of the replicas, including itself, before marking the query as completed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When selecting a new leader, SSO requires a majority of replicas to agree. Moreover, Berkeley DB elections always choose a replica with the latest log entry during an election, thus guaranteeing that the new master’s log will include all the previous master’s updates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;leases&quot;&gt;Leases&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The master holds a &lt;em&gt;master lease&lt;/em&gt; when responding to read queries and refreshes this lease periodically by communicating with a majority of replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The lease guarantees that the master is not returning stale data if a partition or failure causes the master to lose its mastership, i.e., holding the lease guarantees that the master is still the master.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Moreover, elections can not be completed within the lease timeout interval.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;replica-group-membership&quot;&gt;Replica Group Membership&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SSO maintains a replica configuration containing the logical (DNS) name and IP address of each replica.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of any changes to the configuration, the changes are specified in a file that the master reads periodically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the configuration changes, the master initiates a configuration change and update the database.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-master replicas can get the new configuration from the database.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new replica or a replica that lost state (say due to a failure) starts as a non-voting replica and can not participate in an election till it has caught up with the master as of the time the replica joined (again).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Searching for Build Debt - Experiences Managing Technical Debt at Google</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Searching-for-Build-Debt-Experiences-Managing-Technical-Debt-at-Google"/>
   <updated>2020-11-09T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Searching for Build Debt - Experiences Managing Technical Debt at Google</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes the efforts to control and repay the technical debt in the build system at Google (called the Build Debt).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Guiding Principles:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Automate techniques to analyze and fix issues that contribute to technical debt.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Make it easier to do the right thing as developers can incur technical debt unknowingly.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Make it hard to do the wrong thing, e.g., by building stricter checks into the build process.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that some of the metrics and design decisions may be outdated now (the paper was written in 2012). However, the core message is still relevant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37755.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;googles-build-system-debt&quot;&gt;Google’s Build System Debt&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BUILD files encapsulate the specifications for building software.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, these files are maintained manually, and the dependencies may not be up-to-date over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In extreme cases, some of the build targets are not built for months. Such targets are called zombie targets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Originally, any project could depend on any other project’s internal details, thus creating (sometimes unwanted) couplings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the lower-level project did not intend to expose some internal details, the unwanted couplings introduce technical debt and make it harder to modify the lower-level project.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One form of technical debt is the visibility debt or the cost of back-fitting visibility rules onto the existing build specifications to re-establish the appropriate encapsulations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another example of technical debt is dead code that can confuse the developers looking for useful APIs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dependency-debt&quot;&gt;Dependency Debt&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Over-declared&lt;/em&gt; or &lt;em&gt;underutilized&lt;/em&gt; dependencies can slow the build and testing of systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Under-declared&lt;/em&gt; dependencies can make the build process brittle and make it difficult to remove &lt;em&gt;over-declared&lt;/em&gt; dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Potential solutions for &lt;em&gt;over-declared&lt;/em&gt; dependencies include:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Setting aside some dedicated time for fixing build rules. But this approach is not automated, and potential breakages make it harder for developers to do the right thing.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Automatically add all the &lt;em&gt;under-declared&lt;/em&gt; dependencies to the BUILD files. The system can raise an error if a direct dependency is missing, making it harder to do the wrong thing.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Automation can be applied for finding/reporting the over-declared dependencies as well.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Potential solutions for &lt;em&gt;underutilized&lt;/em&gt; dependencies include:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;While it is challenging to automate fixing &lt;em&gt;underutilized&lt;/em&gt; dependencies, automating the discovery of such dependencies is still useful.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Highlighting dependencies with high cost and low removal effort could incentivize developers to clean up their projects.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;zombie-targets&quot;&gt;Zombie Targets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Zombie targets can be identified by query the results of build and test runs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A target is marked as “dead” if the attempts to build it have failed for at least 90 days. Until then, build errors are considered to be transient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A zombie target can be eliminated by deleting its definition from the BUILD and deleting the source files, which are reachable only via the zombie target.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;visibility-debt&quot;&gt;Visibility Debt&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Originally, the default visibility of all the targets was public, leading to unintended dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The visibility of all the existing builds was set to &lt;em&gt;legacy_public&lt;/em&gt;, and the default visibility was changed to private.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This encouraged developers to explicitly consider if they wanted other projects to depend on their project.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dead-flags&quot;&gt;Dead Flags&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Google developed its command-line parsing utilities and defined a set of recognized command-line flags for libraries and binaries.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overtime, the number of flags grew to half a million, and many of these flags are not useful anymore (i.e., dead).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These dead flags can it hard to understand and refactor code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Existing flags are analyzed to check which ones have always been set to the same value and replaced by those contents, clearing about 150 thousand flags.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing dead flags also helps to clean up dead/unreachable code.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>One Solution is Not All You Need - Few-Shot Extrapolation via Structured MaxEnt RL</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/One-Solution-is-Not-All-You-Need-Few-Shot-Extrapolation-via-Structured-MaxEnt-RL"/>
   <updated>2020-11-02T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Key idea: Practicing and remembering diverse solutions to a task can lead to robustness to that task’s variations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a framework to implement this idea - train multiple policies such that they are &lt;em&gt;collectively&lt;/em&gt; robust to a new distribution over environments while using a single training environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.14484&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the agent has access to only one MDP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the evaluation, the agent encounters a new MDP which has the same state and action space but may have a different reward and transition function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The agent is allowed some interactions (say &lt;em&gt;k&lt;/em&gt;) with the test MDP and is then evaluated on the test MDP. The setup is referred to as &lt;em&gt;few-shot robustness&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;structured-maximum-entropy-reinforcement-learning-smerl&quot;&gt;Structured Maximum Entropy Reinforcement Learning (SMERL)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Represent a set of policies using a latent variable policy (i.e., a policy conditioned on a latent variable &lt;em&gt;z&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This has two benefits: (i) Multiple policies can be represented by the same object, and (ii) diverse behaviors can be learned by encouraging the trajectories, corresponding to different &lt;em&gt;z&lt;/em&gt; to be different, while being able to solve the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A diversity-inducing objective is used to encourage the agent to learn different trajectories for different &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, the mutual information between &lt;em&gt;p(Z)&lt;/em&gt; and marginal trajectory distribution for the latent variable policy is maximized, subject to the constraint that each policy achieves close to optimal returns in the train MDP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The mutual information between &lt;em&gt;p(Z)&lt;/em&gt; and marginal trajectory distribution for the latent variable policy is lower bounded by the sum of mutual information terms over individual states (appearing in the trajectory).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An unsupervised reward function is defined using the mutual information between states and latent variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(r(s, a) = log(q_{\phi})(z\|s) - log(p(z))\) where \(q_{\phi}\) is a learned discriminator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This unsupervised reward is optimized for only when the policy achieves close to an optimal return, i.e., the environment return is close to the optimal return. Otherwise, the agent optimizes only for the environment return.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SMERL is implemented using SAC with a latent variable maximum entropy policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The set of latent variables is a fixed discrete set \(Z\) and \(p(z)\) is set to be a uniform distribution over this set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the start of an episode, a \(z\) is sampled and used throughout the episode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Discriminator \(q_{\phi}(z\|s)\) is trained to infer \(z\) from the visited states.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A baseline SAC agent is trained beforehand to evaluate if the current training policy achieves close to optimal environment return.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the evaluation, the policy corresponding to each latent variable is executed in the test MDP, and the policy with the maximum return is returned.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-analysis&quot;&gt;Theoretical Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an MDP \(M\) and \(\epsilon&amp;gt;0\), the MDP robustness set is defined as the set of all MDPs \(M&apos;\) where the optimal policy of \(M&apos;\) produces the same trajectory distribution in \(M&apos;\) as \(M\). Moreover, on the training MDP \(M\), the optimal policies (corresponding to \(M\) and \(M&apos;\)) obtain similar returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that SMERL generalizes to MDPs belong to the robustness set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also provides a simplified view of the optimization objective and shows how it naturally leads to a trajectory-centric mutual information objective.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;2D navigation environments with point mass.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Mujoco Environments: HalfCheetah-Goal, Walker2d-Velocity, Hopper-Velocity.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the 2D navigation environment, the paper shows that SMERL learns to use different trajectories to reach the goal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the Mujoco setup, the evaluation shows that SMERL generally outperforms the best-performing baseline or is close to the best-performing baseline on different tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, higher train performance does not correlate with higher test performance, and there is no single policy that performs the best across all the tasks. Thus, it should be beneficial to learn multiple diverse policies that can be selected from during testing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning Explanations That Are Hard To Vary</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-Explanations-That-Are-Hard-To-Vary"/>
   <updated>2020-10-19T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning Explanations That Are Hard To Vary</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper builds on the principle “good explanations are hard to vary” to propose that &lt;em&gt;invariant mechanisms&lt;/em&gt; can be identified by finding explanations (say model parameters) that are hard to vary across examples.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.00329&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gibipara92/learning-explanations-hard-to-vary&quot;&gt;Link to the code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Collection of &lt;em&gt;d&lt;/em&gt; different datasets (from different environments). Each dataset is a collection of input-target tuples.&lt;/li&gt;
  &lt;li&gt;Objective is to learn a function &lt;em&gt;f&lt;/em&gt; (also called &lt;em&gt;mechanism&lt;/em&gt;) to map the input to the target (for all the environments).&lt;/li&gt;
  &lt;li&gt;The standard approach is to pool the loss for examples corresponding to the different environments and perform gradient updates on this average-pooled loss.&lt;/li&gt;
  &lt;li&gt;In this standard gradient-based setup, the model may not learn invariances due to the following reasons:
    &lt;ul&gt;
      &lt;li&gt;Model learned the spurious features first, and now the training loss is too small.&lt;/li&gt;
      &lt;li&gt;The pooled loss is generally computed by summing (or averaging) the loss corresponding to individual examples. Thus the gradient for each example is calculated independently. Each sample can be thought of as a dataset of size 1, for which all the features are relevant.&lt;/li&gt;
      &lt;li&gt;Gradient descent with averaging (of gradients across the environments) greedily maximizes for the learning speed and not invariance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Performing arithmetic mean can be seen as performing an OR operation (i.e., the sum can be high if any one of the constituents is high), whereas performing geometric mean can be seen as performing an AND operation (i.e., the product can be high only if all the constituents are high).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;invariant-learning-consistencyilc&quot;&gt;Invariant Learning Consistency(ILC)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an algorithm \(A\), let \(\theta_{A}^{*}\) denote the set of convergence points of \(A\) when trained on all the environments.&lt;/li&gt;
  &lt;li&gt;Each convergence point is associated with a consistency score.&lt;/li&gt;
  &lt;li&gt;Intuitively, given a convergence point and an environment &lt;em&gt;e&lt;/em&gt;, find the set of parameters equivalent to the convergence point (in terms of loss) with respect to &lt;em&gt;e&lt;/em&gt;. Let’s call this set as &lt;em&gt;S&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Evaluate the points in this set for all the remaining environments. For the given convergence point, an environment &lt;em&gt;e’&lt;/em&gt; is consistent with &lt;em&gt;e&lt;/em&gt; if the maximum difference in the loss for two environments is small, for all points belonging to &lt;em&gt;S&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;This idea is used to define the invariant learning consistency score for algorithm \(A\), which measures the expected consistency of the converged points (on the pooled data) across all the environments.&lt;/li&gt;
  &lt;li&gt;The paper shows that the converged points’ consistency is linked to the Hessians’ geometric mean and that for the convex quadratic case, using the elementwise geometric mean of gradients improves consistency.&lt;/li&gt;
  &lt;li&gt;However, there are some practical challenges:
    &lt;ul&gt;
      &lt;li&gt;Geometric mean is defined only when all signs are consistent. This issue can potentially be handled by treating different signs as 0.&lt;/li&gt;
      &lt;li&gt;There is very little flexibility in “partial” agreement, and even a single zero gradient component can stop optimization for that component. This can probably be handled by not masking if many environments have a gradient for that component.&lt;/li&gt;
      &lt;li&gt;Geometric component needs to be computed in the log-domain (for numerical scalability), but that can be computationally more expensive.&lt;/li&gt;
      &lt;li&gt;When using adaptive optimizers like Adam, the exact magnitude of geometric mean will be ignored because of rescaling for the local curvature adaptation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some of these challenges can be handled using average gradients when the geometric mean would be 0 and masking out components based on the sign.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;and-mask&quot;&gt;AND-mask&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The ideas from the previous section can be used to develop a practical algorithm called AND-mask.&lt;/li&gt;
  &lt;li&gt;Zero-out gradients that have inconsistent signs across some threshold number (hyper-parameter) of environments.&lt;/li&gt;
  &lt;li&gt;In the presence of purely random gradient patterns, the AND-mask decreases the signals’ strength exponentially fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;synthetic-memorization-dataset&quot;&gt;Synthetic Memorization Dataset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;This is a binary classification task with two kind of features: (i) “meaningful” features that are shared across environments but harder for the model to learn and (ii) “shortcut” features that are easy to learn but not shared across environments.&lt;/li&gt;
  &lt;li&gt;While the dataset may look simple, it is difficult to find the invariant mechanism because the “shortcut” features allow for a simple, linear decision boundary, with a large margin that is fast to learn, has perfect accuracy, robust to input noise, and no iid generalization gap.&lt;/li&gt;
  &lt;li&gt;Baselines:
    &lt;ul&gt;
      &lt;li&gt;MLPs trained with regularizers like dropout, L1, L2, and batch norm.&lt;/li&gt;
      &lt;li&gt;Domain Adversarial Neural Networks (DANN)&lt;/li&gt;
      &lt;li&gt;Invariant Risk Minimization (IRM)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In terms of results, AND-mask with L1/L2 regularizers gives the best results.&lt;/li&gt;
  &lt;li&gt;Empirically, the paper shows that the signal from the “meaningful” features is present when the gradients are averaged, but their magnitude is much smaller than the signal from the “shortcut” features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments-on-cifar-10&quot;&gt;Experiments on CIFAR-10&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A ResNet model is trained on the CIFAR-10 dataset with random labels, with and without the AND-mask.&lt;/li&gt;
  &lt;li&gt;The model with the AND-mask did not memorize the data, whereas the model without the AND-mask did. As sanity, the paper ensured that both the models generalize well when trained with the original labels.&lt;/li&gt;
  &lt;li&gt;Note that for this experiment, every example was treated to have come from its own environment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;behavioral-cloning-on-coinrun&quot;&gt;Behavioral Cloning on CoinRun&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Train an expert policy using PPO for 400M steps on the full distribution of levels.&lt;/li&gt;
  &lt;li&gt;Generate a dataset of state-action pairs. Training data consists of 1000 states from each of the 64 levels, while the test data comes from 2000 levels.&lt;/li&gt;
  &lt;li&gt;A ResNet18 model is used as an imitation learning policy.&lt;/li&gt;
  &lt;li&gt;The exact implementation of the AND-mask is a little more involved, but the key takeaway is that model trained with AND-mask identifies invariant mechanisms across different levels.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Remembering for the Right Reasons - Explanations Reduce Catastrophic Forgetting</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Remembering-for-the-Right-Reasons-Explanations-Reduce-Catastrophic-Forgetting"/>
   <updated>2020-10-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Remembering for the Right Reasons - Explanations Reduce Catastrophic Forgetting</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper hypothesizes that catastrophic forgetting can happen if the model can not rely on “reasoning” used for an old datapoint. If that is the case, catastrophic forgetting may be alleviated when the model “remembers” why it made a prediction previously.&lt;/li&gt;
  &lt;li&gt;The paper presents a simple instantiation of this hypothesis, in the form of a technique called Remembering for the Right Reasons (RRR).&lt;/li&gt;
  &lt;li&gt;The idea is to store model explanations, along with previous examples in the replay buffer. During replay, an additional &lt;em&gt;explanation loss&lt;/em&gt; is used, along with the regular replay loss.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.01528&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons&quot;&gt;Link to the code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The model is trained over a sequence of data distributions in the class-incremental learning setup. A single-head architecture is used so that the task ID is not required during inference.&lt;/li&gt;
  &lt;li&gt;Along with the standard replay buffer (\(M^{rep}\)) for the raw input examples (from different tasks), another replay buffer (\(M^{RRR}\)) is maintained for storing the “explanations” (in the form of saliency maps), corresponding to examples in \(M^{rep}\).&lt;/li&gt;
  &lt;li&gt;RRR is implemented as an L1 loss on the error between the saliency map generated after training on the current task and the saliency map in \(M^{RRR}\).&lt;/li&gt;
  &lt;li&gt;Saliency maps need to be generated while the model is training. This requirement rules out black-box saliency methods, which can be used only after training.&lt;/li&gt;
  &lt;li&gt;The gradient-based white-box explainability techniques that are used include:
    &lt;ul&gt;
      &lt;li&gt;Vanilla backpropagation - Perform a forward pass through the model and take the gradient of the given output class with respect to the input.&lt;/li&gt;
      &lt;li&gt;Backpropagation with SmoothGrad - Saliency maps generated using Vanilla backpropagation can be visually noisy. These maps can be improved by adding pixel-wise Gaussian noise to &lt;em&gt;n&lt;/em&gt; copies of the image and averaging the resulting gradients. The paper used &lt;em&gt;n=40&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Gradient-weighted Class Activation Mapping (Grad-CAM) - Uses gradients to determine the importance of feature map activations on a given prediction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RRR can be easily used with memory and regularization based approaches.&lt;/li&gt;
  &lt;li&gt;The paper combined RRR with the following standard Class Incremental Learning (CIL) models:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.11652&quot;&gt;iTAML : An incremental task-agnostic meta-learning approach&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1807.09536&quot;&gt;End-to-end incremental learning (EEIL)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.13260&quot;&gt;Large scale incremental learning (BiC)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.10956&quot;&gt;TOpology-Preserving knowledge InCrementer (TOPIC)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.07725&quot;&gt;iCaRL: Incremental Classifier and Representation Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00796&quot;&gt;Elastic Weight Consolidation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.09282&quot;&gt;Learning without forgetting&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;few-shiot-class-incremental-learning&quot;&gt;Few-Shiot Class Incremental Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;C-way K-shot class incremental learning with C classes and K training samples per class and b base classes to learn as the first task.&lt;/li&gt;
  &lt;li&gt;Caltech-UCSD Birds dataset with 100 base classes and remaining 100 classes divided into ten tasks, with three samples per class. The test set is not changed.&lt;/li&gt;
  &lt;li&gt;In teems of saliency maps., Grad-CAM is better than Vanilla Backpropagation, which in turn is comparable to SmoothGrad. The same trend is seen in terms of memory overhead, with Grad-CAM having the least memory overhead.&lt;/li&gt;
  &lt;li&gt;Adding the RRR loss improves the performance of all the baselines.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;standard-class-incremental-learning&quot;&gt;Standard Class Incremental Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR100 and ImageNet100 with a memory budget of 2000 samples.&lt;/li&gt;
  &lt;li&gt;Adding the RRR loss improves all the baselines’ performance, and the gains for ImageNet100 are more significant than the gains for CIFAR100.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-often-does-the-model-remember-its-decision-for-the-right-reason&quot;&gt;How often does the model remember its decision for the right reason?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The paper uses the Pointing Game (PG) experiment, which uses the ground truth image segmentation to define the true object region.&lt;/li&gt;
  &lt;li&gt;If the maximum attention location (in the predicted saliency map) falls inside the objects, it is considered a &lt;em&gt;hit&lt;/em&gt;, else a &lt;em&gt;miss&lt;/em&gt;. A &lt;em&gt;hit&lt;/em&gt; on a previous example is considered a proxy for the model remembering its decision for the right reason.&lt;/li&gt;
  &lt;li&gt;The precision and recall are reported for the &lt;em&gt;hit&lt;/em&gt; metric. Using RRR increases both precision (i.e., less often the model makes the correct decision without looking at the right evidence) and recall (i.e., less frequently does the model makes an incorrect decision, despite looking at the proper evidence).&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Foliated View of Transfer Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Foliated-View-of-Transfer-Learning"/>
   <updated>2020-09-28T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Foliated View of Transfer Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a formalism for transfer learning, offers a definition of relatedness between tasks, and proposes foliations as a mathematical framework to represent the relationship between tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.00546&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The term &lt;em&gt;representation&lt;/em&gt; denotes a mechanism for &lt;em&gt;describing&lt;/em&gt; and &lt;em&gt;realizing&lt;/em&gt; abstract objects, thus allowing manipulation and reasoning about the objects. This description goes beyond the usual meaning (in deep learning), where &lt;em&gt;representation&lt;/em&gt; denotes some useful information about data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Relatedness&lt;/em&gt; describes &lt;em&gt;what&lt;/em&gt; changes between tasks. Consider a set of transformations (or functions) that convert one task to another. A &lt;em&gt;relationship&lt;/em&gt; between two tasks is an element of this transformation set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a transformation set, one can define a &lt;em&gt;set of related tasks&lt;/em&gt;, which is the set of all the tasks that can be transformed into each other using the functions from the given transformation set. This set of tasks is an equivalence class, and the transformation set is the equivalence relationship.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given two related tasks &lt;em&gt;t1&lt;/em&gt; and &lt;em&gt;t2&lt;/em&gt;, denote the corresponding models (trained on those tasks) as &lt;em&gt;m1&lt;/em&gt; and &lt;em&gt;m2&lt;/em&gt;. One can assume that &lt;em&gt;m1&lt;/em&gt; and &lt;em&gt;m2&lt;/em&gt; are related in the same way as &lt;em&gt;t1&lt;/em&gt; and &lt;em&gt;t2&lt;/em&gt; (equivariance).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, given a set of transformations, one can partition the space of continuous functions into non-overlapping spaces, which describe a set of related tasks. These spaces are referred to as the &lt;em&gt;parallel spaces&lt;/em&gt; or &lt;em&gt;transfer spaces&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parallel space represents a lower dimension than the original space. So knowing which parallel space a model lies on can make it easier to find it. This is the primary motivation behind transfer learning - knowing the relationship between tasks can make it easier to find a solution to new tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another way of partitioning the set of transformations is to use tessellation (e.g., Voronoi diagrams). Tasks in the same partition are similar to each other as compared to a task from another partition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two tasks are defined as &lt;em&gt;similar&lt;/em&gt; if the distance between them (under some distance metric) is small.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarity is a &lt;em&gt;geometric&lt;/em&gt; notion, while relatedness is a &lt;em&gt;transformative&lt;/em&gt; notion. Parallelized space is to relatedness what tessellation is to similarity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The distinction between similarity and relatedness is quite nuanced, and the authors provide several examples to differentiate between them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarity can only be measured in terms of a reference element (similar to what). For example, when one finetunes a pre-trained model on a new task, one assumes that the model’s pretraining task is similar to the current task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a set (say &lt;em&gt;T&lt;/em&gt;), a &lt;em&gt;quantity&lt;/em&gt; (a function that maps elemenets of &lt;em&gt;T&lt;/em&gt; to a &lt;em&gt;k&lt;/em&gt; dimensional vector) is said to be &lt;em&gt;invariant&lt;/em&gt; with respect to a transformation &lt;em&gt;p&lt;/em&gt; (defined on &lt;em&gt;T&lt;/em&gt;) if &lt;em&gt;q(f) = q(p(f))&lt;/em&gt; ie the value of &lt;em&gt;f&lt;/em&gt; (belonging to &lt;em&gt;T&lt;/em&gt;) does not change if &lt;em&gt;f&lt;/em&gt; is transformed by &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If one assumes that the set of transformations is a group, specifically a Lie group whose action on the set of tasks is locally free and regular, then one can define a parallel partitioning of the space of tasks and the space of models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One can develop a hierarchial categorization scheme for the set of all considered tasks using the invariant quantities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One can consider the space of tasks and models to be smooth manifolds as manifolds naturally give a notion of representation and transformations between them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A manifold is a topological space that can be locally mapped to a Euclidean space using coordinate charts. One can define regular foliation by choosing charts that satisfy certain conditions. In that case, the manifold has immersed, connected, non-intersecting submanifolds called leaves.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The charts (that satisfies those conditions) give a set of rectified coordinates, where the notions of “which leaf a point is on” and “where on the leaf it is” are clearly separated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thus, foliation can provide the theoretical tools to work with parallel spaces.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How can the foliations be incorporated into theory and solutions for transfer learning is left aa future work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Harvest, Yield, and Scalable Tolerant Systems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Harvest,-Yield,-and-Scalable-Tolerant-Systems"/>
   <updated>2020-09-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Harvest, Yield, and Scalable Tolerant Systems</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A classic paper that looks into strategies for scaling large systems that can tolerate graceful degradation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.5555/822076.822436&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cap-theorem&quot;&gt;CAP Theorem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CAP refers to strong &lt;strong&gt;C&lt;/strong&gt;onsistency, high &lt;strong&gt;A&lt;/strong&gt;vailability, and &lt;strong&gt;P&lt;/strong&gt;artitionability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strong consistency refers to single copy ACID consistency.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;High availability means any consumer can access the data anytime. Generally, this is achieved by adding one or more data replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Partitionability means that the system can survive a partition between the different replicas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strong CAP theorem states that any system can have only two out of three properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Weak CAP theorem says that stronger are the guarantees about any two properties, weaker are the third property’s guarantees.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;harvest-yield-and-cap-theorem&quot;&gt;Harvest, Yield, and CAP Theorem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Assume that the clients are making a request to a server.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two quantities of interest here:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Yield - the probability of completing a request.&lt;/li&gt;
      &lt;li&gt;Harvest - completeness of answer to a query.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the presence of faults, a tradeoff can is made between yield and harvest. This tradeoff applies to both read and update queries.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;two-strategies-for-scaling-systems&quot;&gt;Two strategies for scaling systems&lt;/h2&gt;

&lt;h3 id=&quot;trading-harvest-for-yield&quot;&gt;Trading Harvest for Yield&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a hundred node cluster (without replication), a single-node failure reduces harvest by 1 %, and in the case of multi-node failure, the harvest degrades linearly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of losing high-priority data can be reduced by replicating it. However, replicating all the data would not n guarantee 100% harvest and yield despite significant costs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;application-decomposition-and-orthogonal-mechanisms&quot;&gt;Application Decomposition and Orthogonal Mechanisms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Decompose a large application into subcomponents so that each component can be provisioned separately. Strong consistency can only be applied only on the components that need it, instead of the application as a whole.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, failure of one or more components need not cause the application to fail as a whole.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decomposition also provides the opportunity to use orthogonal mechanisms, i.e., mechanisms independent of other mechanisms with no runtime interface.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Composition of orthogonal subsystems improves the robustness of runtime interactions by &lt;em&gt;locally&lt;/em&gt; containing the errors. For example, the orthogonal components can be restarted /replaced independently without affecting other running components.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>MONet - Unsupervised Scene Decomposition and Representation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/MONet-Unsupervised-Scene-Decomposition-and-Representation"/>
   <updated>2020-09-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/MONet Unsupervised Scene Decomposition and Representation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces Multi-Object Network (MONet) architecture that learns a modular representation of images by spatially decomposing scenes into &lt;em&gt;objects&lt;/em&gt; and learning a representation for these &lt;em&gt;objects&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.11390&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Attention Module: generates spatial masks corresponding to the &lt;em&gt;objects&lt;/em&gt; in the scene.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;VAE: learn representation for each &lt;em&gt;object&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VAE components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Encoder: It takes as input the image and the attention mask generated by the attention module and produce the parameters for distribution over latent variable &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Decoder: It takes as input the latent variable &lt;em&gt;z&lt;/em&gt; and attempts to reproduce the image.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The decoder loss term is weighted by mask, i.e., the decoder tries to reproduce only those parts of the image that the attention mask focuses on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The attention mechanism is auto-regressive with an ongoing state (called a scope) that tracks which parts of the image are not yet attended over.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the last step, no attention mask is computed, and the previous scope is used as-is. This ensures that all the masks sum to 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The VAE also models the attention mask over the components, i.e., the probability that the pixels belong to a particular component.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A model could efficiently process compositional visual scenes if it can exploit some recurring structures in the scene.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper validates this hypothesis by showing that an autoencoder performs better if it can build up the scenes compositionally, processing one mask at a time (these masks are ground-truth spatial masks) rather than processing the scene at once.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VAE encoder parameterizes a diagonal Gaussian latent posterior with a spatial broadcast decoder that encourages the VAE to learn disentangled features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MONet with seven slots is trained on &lt;em&gt;Objects Room&lt;/em&gt; dataset with 1-3 objects.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It learns to generate different attention mask for different objects.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Combining the reconstructed components using the corresponding attention masks produces good quality reconstruction for the entire scene.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Since it is an autoregressive model, MONet can be evaluated for more slots. The model generalizes to novel scene configurations (not seen during training).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the Multi-dSprites dataset (modification of the dSprites dataset), the model (post-training) distinguishes individual sprites and background.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the CLEVER data (2-10 objects per image), the model generates good image segmentation and reconstructions and can distinguish between overlapping shapes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Revisiting Fundamentals of Experience Replay</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Revisiting-Fundamentals-of-Experience-Replay"/>
   <updated>2020-09-07T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Revisiting Fundamentals of Experience Replay</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents an extensive study of the effects of experience replay in Q-learning based methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It focuses explicitly on the replay capacity and replay ratio (ratio of learning updates to experience collected).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.06700&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Replay capacity is defined as the total number of transitions stored in the replay buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Age of a transition (stored in the replay buffer) is defined as the number of gradient steps taken by the agent since the transition was stored.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More is the replay capacity, more will be the age of the oldest transition (also referred to as the age of the oldest policy).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More is the replay capacity, more will be the degree of “off-policyness” of the transitions in the buffer (with everything else held constant).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replay ratio is the number of gradient updates per environment transition. This ratio can be used as a proxy for how often the agent uses old data (vs. collecting new data) and is related to off-policyness.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;a href=&quot;https://www.nature.com/articles/nature14236&quot;&gt;DQN paper&lt;/a&gt;, the replay ratio is set to be 0.25.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For experiments, a subset  (of 14 games) is selected from Atari ALE (Arcade Learning Environment) with sticky actions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each experiment is repeated with three seeds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rainbow is used as the base algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Total number of gradient updates and batch size (per gradient update) are fixed for all the experiments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rainbow used replay capacity of 1M and oldest policy of age 250K.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In experiments, replay capacity varies from 0.1M to 10M ( 5 values), and the age of the oldest policy varies from 25K to 25M (4 values).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;With the age of the oldest policy fixed, performance improves with higher replay capacity, probably due to increased state-action coverage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With fixed replay capacity, reducing the oldest policy’s age improves performance, probably due to the reduced off-policyness of the data in the replay buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, in some specific instances (with sparse reward, hard exploration setup), performance can drop when reducing the oldest policy’s age.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing replay capacity, while keeping the replay ratio fixed, provides varying improvements and depends on the particular values of replacy capacity and replay ratio.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports the effect of these choices for DQN as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike Rainbow, DQN does not improve with larger replay capacity, irrespective of whether the replay ratio or age of the oldest policy is kept fixed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the Rainbow agent is a DQN agent with additional components, the paper explores which of these components leads to an improvement in Rainbow’s performance as replay capacity increases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additive-experiments&quot;&gt;Additive Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Four new DQN variants are created by adding each of Rainbow’s four components to the base DQN agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DQN with n-step returns is the only variant that benefits by increased replay capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The usefulness of n-step returns is further validated by verifying that Rainbow agent without n-step returns does not benefit by increased replay capacity. While Rainbow agent without any other component benefits by the increased capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prioritized Experience Replay does not significantly affect the performance with increased replay capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The observation that n-step returns are critical for taking advantage of larger replay sizes is surprising because the uncorrected n-step returns are theoretically not suitable for off-policy learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper tests the limits of increasing replay capacity (with n-step returns) by performing experiments in the offline-RL setup, the agent collects a dataset of about 200M frames. These frames are used to train another agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even in this extreme setup, n-step returns improve the learning agent’s performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-do-n-step-returns-help&quot;&gt;Why do n-step returns help?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hypothesis 1: n-step returns help to counter the increased off-policyness produced by a larger replay buffer.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This hypothesis does not seem to hold as keeping the oldest policy fixed or using the same contrastive factor as an n-step update does not improve the 1-step update’s performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hypothesis 2: Increasing the replay buffer’s capacity may reduce the variance of the n-step returns.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;This hypothesis is evaluated by training on environments with lesser variance or by turning off the sticky actions in the atari domain.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;While the hypothesis does explain the gains by using n-step returns to some extent, n-step gains are observed even in environments with low variance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Deep Reinforcement Learning and the Deadly Triad</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Deep-Reinforcement-Learning-and-the-Deadly-Triad"/>
   <updated>2020-08-31T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Deep Reinforcement Learning and the Deadly Triad</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper investigates the practical impact of the deadly triad (function approximation, bootstrapping, and off-policy learning) in deep Q-networks (trained with experience replay).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The deadly triad is called so because when all the three components are combined, TD learning can diverge, and value estimates can become unbounded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, in practice, the component of the deadly triad has been combined successfully. An example is training DQN agents to play Atari.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.02648&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The effect of each component of the triad can be regulated with some design choices:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Bootstrapping - by controlling the number of steps before bootstrapping.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Function approximation - by controlling the size of the neural network.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Off-policy learning - by controlling how data points are sampled from the replay buffer (i.e., using different prioritization approaches)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is studied in two contexts: toy example and Atari 2600 games.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper makes several hypotheses about how different components may interact in the triad and evaluate these hypotheses by training DQN with different hyperparameters:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Number of steps before bootstrapping - 1, 3, 10&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Four levels of prioritization (for sampling data from the replay buffer)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Bootstrap target - Q-learning, target Q-learning, inverse double Q-learning, and double Q-learning&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Network sizes-small, medium, large and extra-large.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each experiment was run with three different seeds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper formulates a series of hypotheses and designs experiments to support/reject the hypotheses.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-1-combining-q-learning-with-conventional-deep-rl-function-spaces-does-not-commonly-lead-to-divergence&quot;&gt;Hypothesis 1: Combining Q learning with conventional deep RL function spaces does not commonly lead to divergence&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rewards are clipped between -1 and 1, and the discount factor is set to 0.99. Hence, the maximum absolute action value is bound to smaller than 100. This upper bound is used soft-divergence in the value estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that while soft-divergence does occur, the values do not become unbounded, thus supporting the hypothesis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-2-there-is-less-divergence-when-correcting-for-overestimation-bias-or-when-bootstrapping-on-separate-networks&quot;&gt;Hypothesis 2: There is less divergence when correcting for overestimation bias or when bootstrapping on separate networks.&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One manifestation of bootstrapping on separate networks is target-Q learning. While using separate networks helps on Atari, it does not entirely solve the problem on the toy setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One manifestation of correcting for the overestimation bias is using double Q-learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the standard form, double Q-learning benefits by bootstrapping on a separate network. To isolate the gains by using each component independently, an inverse double Q-learning update is used that does not use a separate target-network for bootstrapping.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Experimentally, Q-learning is the most unstable while target Q-learning and double Q-learning are the most stable. This observation supports the hypothesis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-3-longer-multi-step-returns-will-diverge-easily&quot;&gt;Hypothesis 3: Longer multi-step returns will diverge easily&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This hypothesis is intuitive as the dependence on bootstrapping is reduced with multi-step returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Experimental results support this hypothesis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-4-larger-more-capacity-networks-will-diverge-less-easily&quot;&gt;Hypothesis 4: Larger, more capacity networks will diverge less easily.&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This hypothesis is based on the assumption that more flexible value function approximations may behave more like the tabular case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, smaller networks show fewer instances of instability than the larger networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hypothesis is not supported by the experiments.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hypothesis-5-stronger-prioritization-of-updates-will-diverge-more-easily&quot;&gt;Hypothesis 5: Stronger prioritization of updates will diverge more easily.&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;This hypothesis is supported by the experiments for all the four updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;effect-of-the-deadly-triad-on-the-agents-performance&quot;&gt;Effect of the deadly triad on the agent’s performance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, soft-divergence correlates with poor control performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, longer multi-step returns lead to fewer instances of instabilities and better performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The trend is more interesting in terms of network capacity. Large networks tend to diverge more but also perform the best.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While action-value estimates can grow to large values, they can recover to plausible values as training progresses.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Alpha Net--Adaptation with Composition in Classifier Space</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Alpha-Net-Adaptation-with-Composition-in-Classifier-Space"/>
   <updated>2020-08-24T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Alpha Net--Adaptation with Composition in Classifier Space</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Common transfer learning method focuses on transferring knowledge in the model feature space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In contrast, the paper argues that the learned knowledge is more concisely captured in the “classifier space” as the classifier is fitted for all the samples for a given class, while the feature representation is specific to each sample.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building on this intuition, the paper proposes to combine strong classifiers (trained on large datasets) with weak classifiers (trained on smaller datasets) to improve the weak classifiers’ performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.07073&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;high-level-idea&quot;&gt;High-Level Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given $n$ classifiers, $C_1, …, C_n$, trained with a large amount of data and a weak classifier $a$ trained for a class with few samples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find the nearest neighbors of $a$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train a new classifier by linearly combining $a$ with its nearest classifiers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The coefficients (for linearly combining the classifiers) are learned using another classifier called as AlphaNet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In theory, this approach can be used with any set of classifiers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A long-tailed dataset is one where some classes (referred to as the tail classes) have very few examples—for example, ImageNet-LT and Places-LT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Split the long-tailed dataset into two splits - “base” classes with $B$ (number of) classes and “few” classes with $F$ (number of) classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Total number of classes $N = B + F$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Start with a pre-trained model, with classifiers $w_j$ and biases $b_j$ for $j \in (1, N)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For a given target class $j$, find its top $k$ nearest neighbor classifiers and concatenate their output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each “few” class, learn a feedforward network that takes the concatenated representation (of classifiers) as the input and returns a vector of $k \alpha$ values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These $\alpha$ values are interpreted as the classifier’s strength (or confidence) in its nearest neighbors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The (normalized) alpha values are used for defining the weight and bias for the classifier for the given “few” class.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The collection of all the “few” classifiers is referred to as the AlphaNet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper outlines a degenerate case, where the confidence in the prediction of all the strong classifiers goes to 0. The paper proposes to counter this case by clamping the $\alpha$ values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The entire setup is trained end-to-end using cross-entropy loss on AlphaNet.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the proposed approach’s flexibility, it is used to combine the state-of-the-art models on ImageNet-LT, namely retraining classifiers on class-balanced samples and training models with weight normalization. The combined setup outperforms the individual models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One interesting observation is that it is useful to include the weak classifiers, along with the strong classifiers, as AlphaNet adjusts the position of weak classifiers towards the appropriate strong classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the idea is described in the context of long-tail data distribution, the idea is useful in the general context of non-stationary data distribution. One instantiation could be lifelong class incremental learning where the model encounters new data classes during training. For some time duration (till sufficient data points are seen), the newly seen classes are the “few” classes. This approach can help with faster adaptation when the model is yet to see sufficient examples for the unseen classes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Outrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer"/>
   <updated>2020-08-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Outrageously Large Neural Networks--The Sparsely-Gated Mixture-of-Experts Layer</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional computation is a technique to increase a model’s capacity (without a proportional increase in computation) by activating parts of the network on a per example basis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes (and address) the computational and algorithmic challenges in conditional computation. It introduces a sparsely-gated Mixture-of-Experts layer (MoE) with 1000s of feed-forward sub-networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1701.06538&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-challenges&quot;&gt;Practical Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GPUs are fast at matrix arithmetic but slow at branching.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Large batch sizes amortizes the cost of updates. Conditional computation reduces the effective batch size for different components of the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network bandwidth can be a bottleneck with the network demand overshadowing the computational demand.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additional losses may be needed to achieve the desired level of sparsity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional computation is most useful for large datasets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;n&lt;/em&gt; Expert Networks - $E_1$, …, $E_n$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gating Network $G$ to select a sparse combination of experts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output of the MoE module is the weighted sum of predictions of experts (weighted by the output of the gate).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the gating network’s output is sparse, then some of the experts’ value does not have to be computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In theory, one could use a hierarchical mixture of experts where a mixture of experts is trained at each level.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;choices-for-the-gating-network&quot;&gt;Choices for the Gating Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Softmax Gating&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Noisy top-k gating - Add tunable Gaussian noise to the output of softmax gating and retain only the top-k values. A second trainable weight matrix controls the amount of noise per component.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;addressing-performance-challenge&quot;&gt;Addressing Performance Challenge&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Shrinking Batch Problem&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If the MoE selects &lt;em&gt;k&lt;/em&gt; out of &lt;em&gt;n&lt;/em&gt; experts, the effective batch size reduces by a factor of &lt;em&gt;k&lt;/em&gt; / &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This reduction in batch size is accounted for by combining data parallelism (for standard layers and gasting networks) and model parallelism (for experts in MoE). Thus, with &lt;em&gt;d&lt;/em&gt; devices, the batch size changes by a factor of (&lt;em&gt;k&lt;/em&gt; x &lt;em&gt;d&lt;/em&gt; ) / &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For hierarchical MoE, the primary gating network uses data parallelism while secondary MoEs use model parallelism.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper considers LSTM models where the MoE is applied once the previous layer has finished. This increases the batch size (for the current MoE layer) by a factor equal to the number of unrolling timesteps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Network Bandwith limitations can be overcome by ensuring that the ratio of computation (of each expert) to the input and output size is greater than (or equal to) the ratio of computational to network capacity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Computational efficiency can be improved by using larger hidden layers (or more hidden layers).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Balancing Expert Utilization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Importance of an expert (relative to a batch of training examples) is defined as the batchwise sum of the expert’s goal values.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;An additional loss, called importance loss, is added to encourage the experts to have equal importance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The importance loss is defined as the square of the coefficient of variation (of a set of importance values) multiplied by a (hand-tuned) scaling factor $w_{importance}$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In practice, an additional loss called $L_{load}$ might be needed to ensure that the different experts get equal load (along with equal importance).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Billon Word Language modeling Benchmark&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;100 Billion word Google News Corpus&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Machine Translation datasets&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Single Language Pairs - WMT’14 En to Fr (36M sentence pairs) and En to De (5M sentence pairs).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Multilingual Machine Translation - large combine dataset of twelve language pairs.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In all the setups, the proposed MoE models achieve significantly better results than the baseline models, at a lower computational cost.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Gradient Surgery for Multi-Task Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Gradient-Surgery-for-Multi-Task-Learning"/>
   <updated>2020-08-06T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Gradient Surgery for Multi-Task Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper hypothesizes that main optimization challenges in multi-task learning arise because of negative interference between different tasks’ gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It hypothesizes that negative interference happens when:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The gradients are conflicting (i.e., have a negative cosine similarity).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The gradients coincide with high positive curvature.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The difference in gradient magnitude is quite large.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proses to work around this problem by performing “gradient surgery.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If two gradients are conflicting, modify the gradients by projecting each onto the other’s normal plane.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This modification is equivalent to removing the conflicting component of the gradient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This approach is referred to as &lt;em&gt;projecting conflicting gradients&lt;/em&gt; (PCGrad).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.06782&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Theoretical Analysis&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper proves the local conditions under which PCGrad improves multi-task gradient descent in the two-task setup.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The conditions are:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Angle between the task gradients is not too small.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Difference in the magnitude of the gradients is sufficiently large.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Curvature of the multi-task gradient is large.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Large enough learning rate.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Experimental Setup&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Multi-task supervised learning&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;MutliMNIST, Multi-task CIFAR100, NYUv2.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For Multi-task CIFAR-100, PCGrad is used with the shared parameters of the routing networks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For NYUv2, PCGrad is combined with MTAN.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;In all the cases, using PCGrad improves the performance.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Multi-task Reinforcement Learning&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Meta-World Benchmark&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;PCGrad + SAC outperforms all other baselines.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;In the context of SAC, the paper suggests learning temperature $\alpha$ on a per-task basis.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Goal-conditioned Reinforcement Learning&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Goal-conditioned robotic pushing task with a Sawyer robot.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;PCGrad + SAC outperforms vanilla SAC.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GradNorm--Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/GradNorm-Gradient-Normalization-for-Adaptive-Loss-Balancing-in-Deep-Multitask-Networks"/>
   <updated>2020-07-30T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/GradNorm--Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes GradNorm, a gradient normalization algorithm that improves multi-task training by dynamically tuning the magnitude of gradients corresponding to different tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.02257&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During multi-task training, some tasks can dominate the training, at the expense of others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is common to define the multi-task loss as a linearly weighted combination of the individual task losses.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes two changes to this setup:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Adapt weight-coefficients, assigned to each loss term, at each training step.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Directly modify the gradient magnitudes, corresponding to different tasks, so that all the tasks are learning at similar rates.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Proposed GradNorm algorithm is similar to BatchNorm, but it performs normalization across tasks, not data batches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient norm at timestep $t$, for the $i^{th}$ task, is computed as the product between average gradient norm (across all tasks at timestep $t$) and $r_i(t) ^ {\alpha}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$r_i$ is the relative inverse training rate of task $i$. It is defined as the ratio between the loss ratio of task $i$ and the average loss ratio (across all the tasks).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\alpha$ is a hyperparameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This computed per-task gradient norm is treated as the target value for actual gradient norms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An additional $L_1$ loss is incorporated between the actual and the target gradient norms, summed over all the tasks, and optimizes the weight-coefficients only.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After every step, the weight-coefficients are renormalized to decouple the gradient normalization from the global learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that all the gradient norm computations are performed only for the layers on which GradNorm is applied. Generally, GradNorm is used with only the last shared layer of weights (to save on computational costs).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two variants of NYUv2 dataset – NYUv2+seg (small dataset) and NYUv2+kpts (big dataset).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Both regression and classification setups were used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;SegNet with a symmetric VGG16 encoder/decoder&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;FCN with modified ResNet-50 as the encoder and shallow ResNet as the decoder.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard pixel-wise losses for each task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GradNorm with $\alpha=1.5$ outperforms the equal-weight baseline and either surpasses or matches the best performance of single networks for each task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Almost any value of 0 &amp;lt; $\alpha$ &amp;lt; 3 improves the network’s performance over an equal weight baseline.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>TaskNorm--Rethinking Batch Normalization for Meta-Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/TASKNORM-Rethinking-Batch-Normalization-for-Meta-Learning"/>
   <updated>2020-07-23T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/TASKNORM--Rethinking Batch Normalization for Meta-Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Meta-learning techniques are shown to benefit from the use of deep neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BatchNorm is a commonly used component when training deep networks, especially for vision tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, BatchNorm and meta-learning make contradictory assumptions, and their combination may not work well in practice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes TaskNorm, a normalization method that is designed explicitly for meta-learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.03284&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Standard meta-learning setup with $k$ tasks, each task with its own context and target set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two sets of parameters are considered during meta-learning - (i) global parameters, and (ii) task-specific parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Meta-learning setup can be viewed as an inference task, where the task-specific parameters are inferred using a context set and some additional (trainable) parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Normalization layers are commonly used to accelerate the training of neural networks. The general approach is to use normalization moments (statistics) along with some learned parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BatchNorm is a well-known and widely used normalization approach. It relies on the implicit assumption that the dataset comprises of iid samples from some underlying distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, in meta-learning, data points are assumed to be iid only within a specific task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This leaves open the question of what moments to use during meta-train and meta-test time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variants-of-batchnorm&quot;&gt;Variants of BatchNorm&lt;/h2&gt;

&lt;h3 id=&quot;conventional-batchnorm-cbn&quot;&gt;Conventional BatchNorm (CBN)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute moments at meta train time and use during meta test time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is equivalent to lumping the moments with the global parameters. I.e., the running moments are shared globally, while the data is iid only locally.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using CBN with MAML leads to poor results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Moreover, meta-learning setup can some times require the use of a very small batch size. (e.g., 1-shot learning) In those cases, the computed statistics are likely to be inaccurate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transductive-batchnorm-tbn&quot;&gt;Transductive BatchNorm (TBN)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use context/target set statistics at both meta-train and meta-test time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is the default BatchNorm mode used in MAML.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instance-based-normalization&quot;&gt;Instance-based normalization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Moments are computed separately for each instance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This mode corresponds to treating the statistics as local at the observation level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These methods provide only limited improvement in performance, and can sometimes have a large overhead.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task-normalization-proposed&quot;&gt;Task Normalization (Proposed)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The normalization statistics are local at the task level, and statistics for a given data point should only depend on the context set’s data point. It should not depend on the other elements of the target set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Meta-Batch Normalisation (METABN) is a precursor to TaskNorm where the context set alone is used to compute the normalization statistics for both the context and the target set (during both meta-test and meta-train time).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;METABN does not perform well when used with small context sets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TaskNorm overcomes this limitation by using a set of non-transductive, secondary moments (computed from the input being normalized).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When the context is small, using additional moments will help to improve the moment estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the general case, a trainable blending factor, $\alpha$, is used to combine the two sets of moments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the computational cost of TaskNorm is slightly more than CBN, it converges faster than CBN in practice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Normalization mechanism in Reptile can be interpreted as a particular case of TaskNorm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Small scale few-shot classification experiments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Omniglot and imin ImageNet dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;First order MAML, with different kinds of normalization schemes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Transductive BatchNorm performs the best.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Among non-transductive approaches, TaskNorm using Instance Normalisation augmentation performs the best.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Similar trend holds for the speed of convergence as well.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Large scale few-shot classification experiments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;MetaDataset dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;CNAPs model&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The context set’s size varies across tasks in this setup and can be as small as 5.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;TaskNorm with Instance Normalisation ranks first in 10 (out of 13) datasets and is also the fastest to train.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;While Instance-based methods (Instance Normalisation and Layer Normalisation) are the slowest to converge, they still outperform the running average based methods (conventional BatchNorm).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The results demonstrate that designing meta-learning specific normalization methods can significantly improve performance and that Transductive BatchNorm may not always be the optimal choice.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Averaging Weights leads to Wider Optima and Better Generalization</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Averaging-Weights-leads-to-Wider-Optima-and-Better-Generalization"/>
   <updated>2020-07-16T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Averaging Weights leads to Wider Optima and Better Generalization</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes Stochastic Weight Averaging (SWA) procedure for improving the generalization performance of models trained with SGD (with cyclic or constant learning rate).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, the model is checkpointed at several points along the training trajectory, and these checkpoints are averaged (in the parameter space) to obtain a single model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Stochastic” in the name refers to the idea that with cyclical or constant learning rate, SGD proposals are approximately sampled from a neural network’s loss surface and are hence stochastic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SWA uses a learning rate schedule that allows exploration in the weight space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SGD with cyclical and constant learning rates explore points (model instances) at the periphery of high-performing networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With different initializations, SGD will find different points (of low training loss) on this boundary, but will not move inside it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Averaging the points provide a mechanism to move inside this periphery.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The train and the test error surfaces, while being similar, are not perfectly aligned. Hence, averaging several models (along the optimization trajectory) could lead to a more robust model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a model $w$ and some training budget $B$, train the model in the conventional way for approx 75% of the budget.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Starting from that point, continue training with the remaining budget, with a constant or cyclical learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For fixed learning rate, checkpoint models at each epoch. For cyclical learning rate, checkpoint the model at the lowest learning rate in the cycle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average all the models to get the SWA model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the model has Batch Normalization layers, run an additional pass to compute the SWA model’s running mean and standard deviation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The computational and space complexity of computing the SWA model is relatively low.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper highlights the ensembling like the effect of SWA by showing that if the model checkpoints ($w_i$) are generated by training with Fast Geometric Ensembling (FGE), the difference between averaging the weights and averaging the predictions is of the order $O(\Delta)$ where $\Delta = max ||w_i - w_{SA}||$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that SWA does not have the overhead of an extra-forward pass during inference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets: CIFAR10, CIFAR100, ImageNet&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models: VGG16, WideResNet, 164-layer preactivation ResNet, ShakeShake, Pyramid Net.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines: Conventional SGD, Exponentially decaying average with SGD and FGE.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In all the CIFAR experiments, SWA consistently outperforms SGD in one budget and consistently improves with training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SWA also achieves performance comparable to FGE, despite FGE being an ensemble method.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On ImageNet, SWA is run on a pre-trained model, and it improves performance in all the cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An ablation experiment (on CIFAR-100) shows that it is possible to train a network (with SWA) using a fixed learning rate. In that setup, using SWA improves performance by 16%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Decentralized Reinforcement Learning -- Global Decision-Making via Local Economic Transactions</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Decentralized-Reinforcement-Learning-Global-Decision-Making-via-Local-Economic-Transactions"/>
   <updated>2020-07-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Decentralized Reinforcement Learning -- Global Decision-Making via Local Economic Transactions</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores the connections between the concepts of a single agent vs. society of agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A society of agents can be modeled as a single agent while a single agent can be modeled as a society of components (or sub-agents).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on mechanisms for training a society of self-interested agents to solve a given task – as if the system was a single task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.02382&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contributions&quot;&gt;Contributions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Societal-decision making&lt;/strong&gt; framework relates the local optimization problem of a single agent with the global optimization problem of a society of agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cloned Vickrey Society&lt;/strong&gt; is proposed as a mechanism to guarantee that an agent’s dominant strategy equilibrium coincides with the group’s optimal policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A class of &lt;strong&gt;decentralized RL algorithms&lt;/strong&gt; that optimize the MDP object of the society as a whole, as a consequence of individual agents optimizing their objectives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Empirical evaluation of Cloned Vickrey Society using any implementation called &lt;strong&gt;Credit Conserving Vickery&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Environment&lt;/em&gt; - a tuple that specifies an input space, an output space, and parameters for determining an objective.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;A standard RL setup can be mapped to &lt;em&gt;environment&lt;/em&gt; by mapping state space to input space, action space to output space and reward function, transition function, and discount factors to the parameters specifying the objective.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Agent&lt;/em&gt; - a function that maps input space to output space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Objective&lt;/em&gt; - a functional that maps an agent to a real number.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;em&gt;auction environments&lt;/em&gt;, the input space is a single auction item (say &lt;em&gt;s&lt;/em&gt;), and the output space is bidding space &lt;em&gt;B&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are &lt;em&gt;N&lt;/em&gt; agents who compete by bidding for an item &lt;em&gt;s&lt;/em&gt; using their bidding policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$b$ is a vector of bids produced by the agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$v_s$ is a vector of agent’s valuations of item &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The $i^{th}$ agent’s utility is given as $v_s^i \times X^i(b) - P^i(b)$. Here, $X^i(b)$ is the portion of $s$ allocated to $i^{th}$ agent and $P^i(b)$ is the price that $i^{th}$ agent is willing to pay.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-choices&quot;&gt;Design Choices&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each agent is independently maximizing its utility.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In certain conditions (i.e., if the auction is dominant strategy incentive compatible), it is optimal for each agent to bid its valuation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These conditions are satisfied by the Vickery auction where $P^i(b)$ is set to be the second-highest bid and $X^i(b) = 1$ if the $i^{th}$ agent wins (and 0 otherwise).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;society&lt;/em&gt; is a set of agents where each agent is a tuple of bidding policy $\psi$ and a transformation function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The environment is modeled at two levels - (i) global environment (referred to as the global MDP) and local environment (referred to as local auction).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each state $s$ in the global MDP is an auction item in a different auction. The winner (of local auction at $s$) transforms $s$ into some other state $s’$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If these transformations are modeled as actions, then the proposed framework can be interpreted as a decentralized reinforcement learning framework.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motivated by the design of market economy (where economic transactions determine wealth distribution), the paper proposes that, for an agent, the valuation of winning an auction is the revenue it can receive in the auction at the next timestep by selling the transformed state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A global MDP that adhere to this design is referred to as the Market MDP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is a catch in the design of the market MDP - the winning agent, at time $t-1$, gets the amount that the highest bidder is willing to pay at time $t+1$. But the winner at time $t+1$ only paid the second-highest bid. Hence, the credit is not conserved.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This inconsistency can be fixed by introducing “duplicate” (or cloned) agents, and the society is called the Cloned Vickery Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Cloned Vickrey Auction mechanism is compared against alternate bidding mechanisms like &lt;em&gt;first price auction&lt;/em&gt; (where winner pays the bid they proposed), solitary version of Vickrey auction (no cloning), and &lt;em&gt;Environment Reward&lt;/em&gt; where only environment reward is used, and there is no price term.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is empirically shown that Cloned Vickrey Auction learns bids that are most close to their actual valuations. Moreover, solitary version leads bids which are more spread out than the ones learned by cloned version. This highlights the importance of competitive pressure to learn bid values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Three different implementations of Cloned Vickrey Auction are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Bucket Brigade (BB) - winner at timestep $t$ receives the highest bid at time step $t+1$, and the subsequent winner pays the highest bid. This case satisfies Credit Conservation and Bellman Optimality.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Vickrey (V) - winner at timestep $t$ receives the highest bid at time step $t+1$, and the subsequent winner pays the second-highest bid. This case satisfies Truthful Dominant Strategy and Bellman Optimality.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Credit Conserving Vickrey (CCV) - winner at timestep $t$ receives the second-highest bid at time step $t+1$, and the subsequent winner pays the second-highest bid. This case satisfies Truthful Dominant Strategy and Credit Conservation.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CCV implementation provides bid values closest to the optimal Q-values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In one experiment, the paper explores the use of the proposed approach for selecting between sub-policies. It shows that CVV is more sample efficient for pretraining sub-policies and adapting them to transfer tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another experiment, the task is to transform MNIST images by composing two out of 6 affine transformations. The transformed images are fed to a pretrained classifier that predicts a label. The agent gets a reward of 1 if the classifier makes correct prediction and 0 otherwise. CCV implementation obtains a mean reward of 0.933, thus highlighting the effectiveness of the CCV model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>When to use parametric models in reinforcement learning?</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/When-to-use-parametric-models-in-reinforcement-learning"/>
   <updated>2020-07-02T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/When to use parametric models in reinforcement learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper compares replay-based approaches with model-based approaches in Reinforcement Learning (RL).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It hypothesizes that if the parametric model is only used for generation transitions for the update rule, then under certain conditions, replay-based approaches will be as good as model-based approaches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.05243&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Planning: Any algorithm that uses additional computations (but not additional experience) to improve its performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning: Any algorithm that uses additional experience to improve its performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In some cases, a replay buffer can be seen as a model. For example, querying using state-action pair (from the replay buffer) is similar to querying the (expected) next-state and reward from a model. In general, the model will be more flexible as any arbitrary state-action pair can be used for querying.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;computation-properties&quot;&gt;Computation Properties&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Parametric models require more computation than sampling from a replay buffer. In contrast, the cost of maintaining a replay buffer scales linearly with their capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Parametric models are useful for planning multiple-steps into the future while it is much harder to do so with a replay buffer (even more so with pixel observations).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An imperfect model maybe be more suitable for selecting actions (instead of updating the policy) because the chosen action, when executed in the environment, will lead to transitions that would improve the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When planning with an imperfect model, it is better to plan backward, as the update is applied on an imaginary state (which would not be encountered if the model is poor).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the model is accurate, forward and backward planning is equivalent. This distinction between forward and backward updates does not apply to replay buffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;failure-to-learn&quot;&gt;Failure to learn&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When using a replay buffer and (i) uniformly replaying transitions, (ii) from a buffer containing only full episodes, and (iii) using TD updates, then the algorithm is stable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When using a replay buffer and (i) uniformly replaying transitions, (ii) generating transitions using a model, and (iii) using TD updates, then the algorithm can diverge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This case can be fixed by:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Repeatedly interating over the model and sampling transitions &lt;em&gt;to&lt;/em&gt; and &lt;em&gt;from&lt;/em&gt; the state model generates (not a satisfactory solution).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using multiple-step returns (this can increase the variance).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Use algorithms specifically for stable off-policy learning (not a definitive solution).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-based-algorithms-at-scale&quot;&gt;Model-based algorithms at scale&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper compares against SimPLe (model-based) with Rainbow DQN (replay-based).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that when using a similar number of real interactions, Rainbow DQN needs fewer replay samples than model samples in SimPLe, making it more efficient (computation-wise).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Changes to Rainbow DQN:
    &lt;ul&gt;
      &lt;li&gt;Increase number of steps, for bootstrapping, from 3 to 20.&lt;/li&gt;
      &lt;li&gt;Reduce the number of steps, before sampling starts from the replay buffer, from 20K to 1600.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;With these changes, Rainbow DQN outperforms SimPLe in 17 out of 26 games.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When using a parametric model in a replay-like setting (sampling observed states from the past), model-based learning can be unstable (in theory). Using a replay buffer is likely a better strategy under the state sampling distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Parametric models are likely more useful when:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;planning backward for credit assignment - even if the model is in-accurate, backward planning will only update fictional states.&lt;/li&gt;
      &lt;li&gt;planning forward for behavior - the resulting plan is only used to collect real &lt;em&gt;experience&lt;/em&gt; in the environment (and not directly update the policy).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Network Randomization - A Simple Technique for Generalization in Deep Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Network-Randomization-A-Simple-Technique-for-Generalization-in-Deep-Reinforcement-Learning"/>
   <updated>2020-06-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Network Randomization-A Simple Technique for Generalization in Deep Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed a Technique for improving the generalization ability of RL agents when evaluated on an unseen environment (which is similar to the training environment).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=HJgcvJBFvB&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pokaxpoka/netrand&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is to learn features that are invariant across environments by using a randomized CNN (&lt;em&gt;f&lt;/em&gt;) that randomly perturbs the inputs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The policy is trained using the randomized observations obtained using &lt;em&gt;f&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invariant features are learned using a feature matching (FM) loss that matches the feature representation of the original and randomized observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The random network’s parameters are initialized as $\alpha I + (1 - \alpha) N(0, \sqrt\frac{2}{n_{in} + n_{out}})$ where $\alpha \in [0, 1]$, $N$ denotes the Gaussian Distribution and $n_{in}, n_{out}$ denote the number of input and output channels respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Xavier Normal distribution is used for randomization to maintain the variance between the input and the randomized input.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;f&lt;/em&gt; is randomized per iteration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During inference, the expected action is computed by approximating over &lt;em&gt;M&lt;/em&gt; samples (i.e., randomizing the input &lt;em&gt;M&lt;/em&gt; times).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environments&quot;&gt;Environments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D CoinRun, 3D DeepMind Lab, 3D Robotics Control Task&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The evaluation environments consist of different styles of backgrounds, objects, and floors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baselines&quot;&gt;Baselines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Regularization methods: Dropout, L2 regularization, Batch Normalization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset Augmentation methods: Cutout, Gray out, Inversion, Color Jitter&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;On CoinRun, the proposed approaches significantly outperforms the other baselines during evaluation. The performance improvement saturates around 10 &lt;em&gt;M&lt;/em&gt; samples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cycle consistency is used to measure the similarity between two trajectories. The proposed method improves the cycle consistency as compared to the vanilla PPO baseline. It also produces sharper activation maps in the evaluation environments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the large-scale experiments, when evaluated on 500 levels of CoinRun, the proposed method improves the success rates from 39.8% to 58.7%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On DeepMind Lab and Surreal robotics control tasks, the proposed method leads to agents that generalize better on the unseen environments (during evaluation).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>On the Difficulty of Warm-Starting Neural Network Training</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/On-the-Difficulty-of-Warm-Starting-Neural-Network-Training"/>
   <updated>2020-06-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/On the Difficulty of Warm-Starting Neural Network Training</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers learning scenarios where the training data is available incrementally (and not at once).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, in some applications, new data is available periodically (e.g., latest news articles come out every day).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper highlights that, in such scenarios, the conventional wisdom of “warm start” does not apply.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When new data is available, it is better to train a new model from scratch than to update the model trained on previously available data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the two setups lead to similar training performance, the randomly initialized model has a much better generalization performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.08475&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;basic-batch-updating&quot;&gt;Basic Batch Updating&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create two random, equally-sized partitions of the training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the model till convergence on the first half of the data. Then train the model on the entire dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models: ResNet18, MLPs, Logisitic Regression (LR)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset: CIFAR10, CIFAR100, SVHN&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimizers: Adam, SGD&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warm starting hurts generalization in all the cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The effect is more pronounced in the case of ResNets and MLPs (compared to LR) and harder CIFAR 10 dataset (as compared to SVHN dataset).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;online-learning&quot;&gt;Online Learning&lt;/h2&gt;

&lt;h3 id=&quot;passive-online-learning&quot;&gt;Passive Online Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is given access to k new learning examples at each iteration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A warm started model reuses the previously initialized model and trains (till convergence) on the new batch of k items.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “randomly initialized” model is trained on all the examples (seen so far) from scratch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset: CIFAR10&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model: ResNet18&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As more training data becomes available, the generalization gap between the two setups increases, and warmup starts hurting generalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;active-online-learning&quot;&gt;Active Online Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In this setup, the learner is trained to sample k new examples to add to the training dataset (using margin-based sampling).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Like the previous setup, warmup strategy still hurts generalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train a Resnet18 model on the CIFAR10 dataset and use this model to warm start training on the SVHN dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a small percentage of the SVHN dataset is used, the setup resembles pretraining / transfer learning and performs better than training from scratch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the percentage of the SVHN dataset increases, the warmup approach starts underperforming.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overcoming-warm-start-problem&quot;&gt;Overcoming warm start problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ResNet18 model on CIFAR10 dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When performing a hyper-parameter sweep over the learning rate and batch size, it is possible to train warm start models to reach the same generalization performance as training from scratch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though, in that case, there are no computational savings as the warm-started models take about the same time (to converge) as the randomly initialized model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The increased training time indicates that the warm started model probably needs to forget the knowledge from previous training rounds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warm start Resnet models, that generalize well, have a low correlation to their initialization stage (measured via Pearson correlation coefficient between the model weights).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization is damaged even when using a model trained on incomplete data for only a few epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For warm start models, the gradient (corresponding to the “new” data) is higher than that for randomly initialized models. This hints that regularisation may help to close the generalization gap. But in practice, regularization helps both the warmup and randomly initialized model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warm starting only a few layers also does not close the gap.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adding some noise to the warm started model (with the motivation of having a partially random initialization) does help somewhat but also increases the training time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motivating the problem as an instance of catastrophic forgetting, the authors use the EWC algorithm but report that using EWC hurts model performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper does not propose a solution to the problem but provides a thorough analysis of the problem setup, which is quite useful for understanding the phenomenon itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Supervised Contrastive Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Supervised-Contrastive-Learning"/>
   <updated>2020-04-30T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Supervised Contrastive Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper builds on the prior work on self-supervised contrastive learning and extends it for the supervised learning case where many positive examples are available for each anchor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.11362&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The representation learning framework has the following components:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-augmentation-module&quot;&gt;Data Augmentation Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This module transforms the input example. The paper considers the following strategies:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Random crop, followed by resizing&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.09501&quot;&gt;Auto Augment&lt;/a&gt; - A method to search for data augmentation strategies.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.13719&quot;&gt;Rand Augment&lt;/a&gt; - Randomly sampling a sequence of data augmentations, with repetition&lt;/li&gt;
      &lt;li&gt;SimAugment - Sequentially apply random color distortion and Gaussian blurring, followed by probabilistic sparse image wrap.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;encoder-network&quot;&gt;Encoder Network&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* This module maps the input to a latent representation.

* The same network is used to encode both the anchor and the sample.

* The representation vector is normalized to lie on the unit hypersphere.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;projection-network&quot;&gt;Projection Network&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* This module maps the normalized representation to another representation, on which the contrastive loss is computed.

* This network is only used for training the supervised contrastive loss.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss function&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* The paper extends the standard contrastive loss formulation to handle multiple positive examples.

* The main effect is that the modified loss accounts for all the same-class pairs (from within the sampled batch as well as the augmented batch).

* The paper shows that the gradient (corresponding to the modified loss) causes the learning to focus more on hard examples. &quot;Hard&quot; cases are the ones where contrasting the anchor benefits the encoder more.

* The proposed loss can also be seen as a generalization of the triplet loss.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset - ImageNet&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models - ResNet50, ResNet200&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The network is “pretrained” using supervised contrastive loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After pre-training, the projection network is removed, and a linear classifier is added.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This classifier is trained with the CE loss while the rest of the network is kept fixed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using supervised contrastive loss improves over all the baseline models and data augmentation approaches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting classifier is more robust to image corruptions, as shown by the mean Corruption Error (mCE) metric on the ImageNet-C dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is more stable to the choice oh hyperparameter values (like optimizers, data augmentation, and learning rates).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-details&quot;&gt;Training Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised Contrastive loss is trained for 700 epochs during pre-training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each step is about 50% more expensive than performing CE.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The dense classifier layer can be trained in as few as ten epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The temperature value is set to 0.07. Using a lower temperature is better than using a higher temperature.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>CURL - Contrastive Unsupervised Representations for Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/CURL-Contrastive-Unsupervised-Representations-for-Reinforcement-Learning"/>
   <updated>2020-04-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/CURL Contrastive Unsupervised Representations for Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a contrastive learning approach, called CURL, for performing off-policy control from raw pixel observations (by transforming them into high dimensional features).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is motivated by the application of contrastive losses in computer vision. But there are additional challenges:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The learning agent has to perform both unsupervised and reinforcement learning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The “dataset” for unsupervised learning is not fixed and keeps changing with the policy of the agent.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike prior work, CURL introduces fewer changes in the underlying RL pipeline and provides more significant sample efficiency gains. For example, CURL (trained on pixels) nearly matches the performance of SAC policy (trained on state-based features).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/MishaLaskin/curl&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CURL uses instance discrimination. Deep RL algorithms commonly use a stack of temporally consecutive frames as input to the policy. In such cases, instance discrimination is applied to all the images in the stack.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For generating the positive and negative samples, random crop data augmentation is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bilinear inner product is used as the similarity metric as it outperforms the commonly used normalized dot product.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For encoding the anchors and the samples, InfoNCE is used. It learns two encoders $f_q$ and $f_k$ that transform the query (base input) and the key (positive/negative samples) into latent representations. The similarity loss is applied to these latents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Momentum contrast is used to update the parameters ($\theta_k$) of the $f_k$ network. ie $\theta_k = m \theta_k + (1-m) \theta_q$. $\theta_q$ are the parameters of the $f_q$ network and are updated in the usual way, using both the contrastive loss and the RL loss.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DMControl100K and Atart100K refer to the setups where the agent is trained for 100K steps on DMControl and Atari, respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sample Efficiency - How many steps does the baseline need to match CURL’s performance after 100K steps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Performance - Ratio of episodic returns by CURL vs. the baseline after 100K steps.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;DMControl&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.01741&quot;&gt;SAC-AE&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.00953&quot;&gt;SLAC&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://planetrl.github.io/&quot;&gt;PlaNet&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=S1lOTC4tDS&quot;&gt;Dreamer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.05905&quot;&gt;Pixel SAC&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;SAC trained on state-space observations&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Atari&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.00374&quot;&gt;SimPLe&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.02298&quot;&gt;RainbowDQN&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=Bke9u1HFwB&quot;&gt;OTRainbow (Over Trained Rainbow)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.05243&quot;&gt;Efficient Rainbow&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;Random Agent&lt;/li&gt;
          &lt;li&gt;Human Performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;DM Control&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;CURL outperforms all pixel-based RL algorithms by a significant margin for all environments on DMControl and most environments on Atari.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;On DMControl, it closely matches the performance of the SAC agent trained on state-space observations.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;On Atari, it achieves better median human normalizes score (HNS) than the other baselines and close to human efficiency in three environments.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Competitive Training of Mixtures of Independent Deep Generative Models</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Competitive-Training-of-Mixtures-of-Independent-Deep-Generative-Models"/>
   <updated>2020-03-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Competitive Training of Mixtures of Independent Deep Generative Models</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a Competitive training mechanism to train a mixture of independent generative models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is that this mixture of different models would divide the data distribution amongst themselves and specialize to their respective splits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The training procedure is related to clustering-based methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.11130&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In causal modeling, a common assumption is that the data is generated by a set of independent mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is not known which mechanism generates which datapoint and recovering the underlying mechanisms can be modeled as learning a structural causal generative model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper assumes that the support of the different generators do not overlap, i.e., the underlying data distribution is factorized into non-overlapping regions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This data factorization is learned using a set of discriminators.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there are $k$ generators, $k$ binary partition functions $c_i, … c_k$ are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For a given datapoint $x$, if $c_i(x) = 1$ then $c_j(x) = 0$ for all other $j$ and $x$ is assigned to $i^{th}$ generator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For a fixed partition function $c_j^t$ ($t$ denotes the partition function at time $t$), minimize the sum of f-divergence between the model and the data distribution (that is assigned to it). The loss formulation is an upper bound on the f-divergence of the mixture model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the next step, the data points are re-assigned to the generative models, based on the likelihood of each data point for each model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The likelihood is estimated by training a discriminator that can distinguish the generated samples from the real samples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;independence-as-an-inductive-bias&quot;&gt;Independence as an inductive bias&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The independence assumption may be too restrictive because the low-level features will be common across the distribution splits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This “violation” can be avoided by pretraining the model using a uniform random split of the dataset. In that case, the independence assumption will hold approximately after pretraining.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another approach could be to share some parameters across the models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “load balancing” approach is also used where each model always keeps training on the data points assigned to it if not enough data points are assigned to it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;comparison-to-vaes-and-gans&quot;&gt;Comparison to VAEs and GANs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VAEs tend to be “overly inclusive” of the training distribution, i.e., they try to cover the entire support of the distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GANs are prone to mode collapse where the model focuses only on one part of the distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed method provides a middle ground where the different generative models can focus on different parts of the distribution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experiments seem to be limited. The paper shows that their proposed setup improves over the VAE and GAN baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For datasets, the paper uses two-dimensional synthetic data, MNIST and CelebA&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>What Does Classifying More Than 10,000 Image Categories Tell Us?</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/What-Does-Classifying-More-Than-10,000-Image-Categories-Tell-Us"/>
   <updated>2020-03-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/What Does Classifying More Than 10,000 Image Categories Tell Us</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper is among the first to study image classification at a large scale (10000 classes and 9 million examples).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is a relatively old paper (2010). Some of the findings may not be relevant anymore. For instance, specific scaling challenges have been significantly overcome. Moreover, the paper uses approaches like SVM and KNN (popular at that time) and not use CNNs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other observations of the paper are still very relevant, and it is an educating paper. For example, since ImagetNet classes are based on WordNet, the paper looks at the effect of semantic relations (tree) of categories on the performance of the training models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2015/papers/Jain_What_do_15000_2015_CVPR_paper.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers three variants of the ImageNet dataset - ImageNet 10K (10184 classes), ImageNet 7K (7404 classes) and ImageNet 1K (1000 classes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;They also consider smaller variants with randomly sampled classes or cases where the examples are sampled from one high-level category like vehicles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SVM and KNN models are used with features like Bag of Words, GIST descriptors, and spatial pyramid of histograms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A model that performs well on the smaller dataset (with fewer classes) may not perform well on the larger dataset (with more classes).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;There seems to be an approximate correlation between the structure of the semantic hierarchy of the labels (obtained via WordNet) and visual confusion between the categories.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For example, consider two high-level concepts - says artifacts and animals. The model is less likely to confuse between the classes across the high-level concepts but more likely to confuse between the classes in the respective concepts.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For dense categories (categories where the classes are semantically more closely related to each other), the model tends to make more mistakes (even if the number of classes is fewer).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Accounting for the label hierarchy (in the loss function) improves the classification performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>mixup - Beyond Empirical Risk Minimization</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/mixup-Beyond-Empirical-Risk-Minimization"/>
   <updated>2020-02-27T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/mixup Beyond Empirical Risk Minimization</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a simple and dataset-agnostic data augmentation mechanism called &lt;em&gt;mixup&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider two training examples, $(x_1, y_1)$ and $(y_1, y_2)$, where $x_1$ and $x_2$ are the datapoints and $y_1$ and $y_2$ are the labels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;New training examples of the form $(\lambda \times x_1 + (1-\lambda) \times x_2, \lambda \times y_1 + (1-\lambda) \times y_2)$ are constructured by considering the linear interpolation of the datapoints and the labels. Here $\lambda \in [0, 1]$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\lambda$ is sampled from a Beta distribution $Beta(\alpha, \alpha)$ where $\alpha \in (0, \infty)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Setting $\lambda$ to 0 or 1 eliminates the effect of &lt;em&gt;mixup&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mixup encourages the neural network to favor linear behavior between the training examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ImageNet for ResNet-50, ResNet-101 and ResNext-101.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;CIFAR10/CIFAR100 for PreAct ResNet-18, WideResNet-28-10 and DenseNet.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Google command dataset for LeNet and VGG.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In all these setups, adding &lt;em&gt;mixup&lt;/em&gt; improves the performance of the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Mixup&lt;/em&gt; makes the model more robust to noisy labels. Moreover, &lt;em&gt;mixup&lt;/em&gt; + dropout improves over &lt;em&gt;mixup&lt;/em&gt; alone. This hints that &lt;em&gt;mixup&lt;/em&gt;’s benefits are complementary to those of dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Mixup&lt;/em&gt; makes the network more robust to adversarial examples in both white-box and black-box settings (ImageNet + Resnet101).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Mixup&lt;/em&gt; also stabilizes the training of GANs by acting as a regularizer for the gradient of the discriminator.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convex combination of three or more examples (with weights sampled from a Dirichlet distribution) does not provide gains over the case of two examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the authors’ implementation, &lt;em&gt;mixup&lt;/em&gt; is applied between images of the same batch (after shuffling).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interpolating only between inputs, with the same labels, did not lead to the same kind of gains as &lt;em&gt;mixup&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/ELECTRA-Pre-training-Text-Encoders-as-Discriminators-Rather-Than-Generators"/>
   <updated>2020-02-20T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Masked Language Modeling (MLM) is a common technique for pre-training language-based models. The idea is to “corrupt” some tokens in the input text (around 15%) by replacing them with the [MASK] token and then training the network to reconstruct (or predict) the corrupted tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the network learns from only about 15% of the tokens, the computational cost of training using MLM can be quite high.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to use a “replaced token detection” task where some tokens in the input text are replaced by other plausible tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each token in the modified text, the network has to predict if the token has been replaced or not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternative token is generated using a small generator network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike the previous MLM setup, the proposed task is defined for all the input tokens, thus utilizing the training data more efficiently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach is called ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two neural networks - Generator (G) and Discriminator (D) are trained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each network has a Transformer-based text encoder that maps a sequence of words into a sequence of vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input sequence x (of length N), k indices are chosen for replacing the tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each index, the generator produces a distribution over tokens. A token is sampled to replace in the original sequence. The resulting sequence is referred to as the corrupted sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the corrupted sequence, the Discriminator predicts which token comes from the data distribution and which comes from the generator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The generator is trained using the MLM setup, and the Discriminator is trained using the discriminative loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After pre-training, only the Discriminator is finetuned on the downstream tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;GLUE Benchmark&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Stanford QA dataset&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Architecture Choices&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sharing word embeddings between generator and Discriminator helps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Tying all the encoder weights leads to marginal improvement but forces the generator and the Discriminator to be of the same size. Hence only embeddings are shared.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generator model is kept smaller than the discriminator model as a strong generator can make the training difficult for the Discriminator.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A two-stage training procedure was explored where only the generator is trained for n steps. Then the weights of the generator are used to initialize the Discriminator. The Discriminator is then trained for n steps while keeping the generator fixed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This two-stage setup provides a nice curriculum for the Discriminator but does not outperform the joint training based setup.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;An adversarial loss based setup is also explored but it does not work well probably because of the following reasons:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Adverserially trained generator is not as good as the MLM generator.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Adverserially trained generator produces a low entropy output distribution.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Both small and large ELECTRA models outperform baselines models like &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;RoBERTa&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;ELMo&lt;/a&gt; and &lt;a href=&quot;https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf&quot;&gt;GPT&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ELECTRA-15 is a variant of ELECTRA where the Discriminator is trained on only 15% of the tokens (similar to the MLM setup). This reduces performance significantly.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Replace MLM setup&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Perform MLM training, but instead of using [MASK], use a toke sampled from the generator.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This improves the performance marginally.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;All-token MLM&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;In the MLM setup, replace the [MASK] token by the sampled tokens and train the MLM model to generate all the words.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;In practice, the MLM model can either generate a word or copy the existing word.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This approach closes much of the gap between BERT and ELECTRA.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, ELECTRA outperforms All-token MLM BERT suggesting the ELECTRA may be benefitting from parameter efficiency since it does not have to learn a distribution over all the words.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Gradient based sample selection for online continual learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Gradient-based-sample-selection-for-online-continual-learning"/>
   <updated>2020-02-13T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Gradient based sample selection for online continual learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use of replay buffer (and rehearsal) is a common technique for mitigating catastrophic forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper builds on this idea but focuses on the sample selection aspect ie, which data points to store in the replay buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It formulates sample selection as a constraint minimization problem and shows that the proposed formulation is equivalent to maximizing the diversity of the samples with respect to parameter gradient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.08671&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised learning tasks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Online stream of data (i.e., one or few datapoints accessed at a time).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When considering the $t^{th}$ task, the objective is: minimize the loss on the current task without increasing the loss on any of the previous tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above constraint can be rephrased as $dot(g_t, g_i) \gt 0 \forall i \in [0, t-1]$ where $g_t$ is the gradient for the $t^{th}$ task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is equivalent to saying that the current task gradient should not interfere negatively with the previous task gradient.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, the gradient constraint is enforced only over the examples in the minibatch (and not the full dataset).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper interprets the constraint satisfaction problem as approximating an optimal feasible region (in the gradient space) where current task performance can be improved without hurting the performance on the previous tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approximate region (of the shape of a polyhedral convex cone) is determined using only the examples from the replay buffer. Hence, the optimal region (defined for the entire dataset) would be contained within the approximate region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The size of the approximate region can be measured in terms of the solid angle defined by the intersection between the approximate region and a unit sphere.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper argues that the approximate region can be made smaller by reducing the angle between each pair of gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The set of points, satisfying the constraint, can be computed using the Integer Quadratic Programming (IQP).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the problem setup is online learning, using IDP for every new data point is not feasible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An in-exact, greedy alternative is suggested where a score is maintained for each example in the buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a new datapoint comes in, the score is computed and used to decide if the existing datapoint in the buffer should be replaced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The score is the maximal cosine similarity of the current example with a random sample in the buffer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Benchmarks&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Disjoint MNIST&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Permuted MNIST&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Disjoint CIFAR10&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared head setup&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines for sample selection&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Randomly select examples to keep in the buffer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Perform clustering - either in the feature space or in the gradient space.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Use IQP to select the examples. This approach is not used for CIFAR10, as it is computationally costly.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;It would be interesting if the paper had considered baselines like selecting samples which had the largest loss.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed greedy approach outperforms the other methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In an ablation experiment, the paper shows that the proposed approach works better than reservoir sampling (when the underlying data distribution is imbalanced).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another experiment compares the proposed approach with &lt;a href=&quot;https://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf&quot;&gt;Gradient Episodic Memory&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1611.07725&quot;&gt;iCaRL&lt;/a&gt;. For Permuted and Disjoint MNIST, the different methods perform quite similar though the proposed approach performs better on Disjoint CIFAR10.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Your-Classifier-is-Secretly-an-Energy-Based-Model,-and-You-Should-Treat-it-Like-One"/>
   <updated>2020-02-06T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Your Classifier is Secretly an Energy-Based Model, and You Should Treat it Like One</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed a framework for joint modeling of labels and data by interpreting a discriminative classifier &lt;em&gt;p(y|x)&lt;/em&gt; as an energy-based model &lt;em&gt;p(x, y)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Joint modeling provides benefits like improved calibration (i.e., the predictive confidence should align with the miss classification rate), robustness, and out of order distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.03263&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a standard classifier $f_{\theta}(x)$ which produces a k-dimensional vector of logits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$p_{\theta}(y | x) = softmax(f_{\theta}(x)[y])$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Uisng concepts from energy based models, we write $p_{\theta}(x, y) = \frac{exp(-E_{\theta}(x, y))}{Z_{\theta}}$ where $E_{\theta}(x, y) = -f_{\theta}(x)[y]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$p_{\theta}(x) = \sum_{y}{ \frac{exp(-E_{\theta}(x, y))}{Z_{\theta}}}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$E_{\theta}(x) = -LogSumExp_y(f_{\theta}(x)[y])$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that in the standard discriminative setup, shiting the logits $f_{\theta}(x)$ does not affect the model but it affects $p_{\theta}(x)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Computing $p_{\theta}(y | x)$ using $p_{\theta}(x, y)$ and $p_{\theta}(x)$ gives back the same softmax parameterization as before.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This reinterpreted classifier is referred to as a Joint Energy-based Model (JEM).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The log-liklihood of the data can be factoized as $log p_{\theta}(x, y) = log p_{\theta}(x) + log p_{\theta}(y | x)$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second factor can be trained using the standard CE loss. In contrast, the first factor can be trained using a sampler based on Stochastic Gradient Langevin Dynamics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;hybrid-modelling&quot;&gt;Hybrid Modelling&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets: CIFAR10, CIFAR100, SVHN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics: Inception Score, Frechet Inception Distance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;JEM outperforms generative, discriminative, and hybrid models on both generative and discriminative tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;calibration&quot;&gt;Calibration&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A calibrated classifier is the one where the predictive confidence aligns with the misclassification rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset: CIFAR100&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;JEM improves calibration while retaining high accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;out-of-distribution-ood-detection&quot;&gt;Out of Distribution (OOD) Detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One way to detect OOD samples is to learn a density model that assigns a higher likelihood to in-distribution examples and lower likelihood to out of distribution examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;JEM consistently assigns a higher likelihood to in-distribution examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also proposes an alternate metric called &lt;em&gt;approximate mass&lt;/em&gt; to detect OOD examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The intuition is that a point could have likelihood but be impossible to sample because its surroundings have a very low density.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the other hand, the in-distribution data points would lie in a region of high probability mass.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hence the norm of the gradient of log density could provide a useful signal to detect OOD examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;robustness&quot;&gt;Robustness&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;JEM is more robust to adversarial attacks as compared to discriminative classifiers.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Massively-Multilingual-Neural-Machine-Translation-in-the-Wild-Findings-and-Challenges"/>
   <updated>2020-01-30T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Massively Multilingual Neural Machine Translation in the Wild-Findings and Challenges</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to build a universal neural machine translation system that can translate between any pair of languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a concrete instance, the paper prototypes a system that handles 103 languages (25 Billion translation pairs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.05019&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-universal-machine-translation&quot;&gt;Why universal Machine Translation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hypothesis: &lt;em&gt;The learning signal from one language should benefit the quality of other languages&lt;/em&gt;&lt;a href=&quot;https://link.springer.com/article/10.1023/A:1007379606734&quot;&gt;1&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This positive transfer is evident for low resource languages but tends to hurt the performance for high resource languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, adding new languages reduces the effective per-task capacity of the model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desiderata-for-multilingual-translation-model&quot;&gt;Desiderata for Multilingual Translation Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Maximize the number of languages within one model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximize the positive transfer to low resource languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minimize the negative interference to high resource languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform well ion the realistic, multi-domain settings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In-house corpus generated by crawling and extracting parallel sentences from the web.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;102 languages, with 25 billion sentence pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compared with the existing datasets, this dataset is much larger, spans more domains, has a good variation in the amount of data available for different language pairs, and is noisier. These factors bring additional challenges to the universal NMT setup.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baselines&quot;&gt;Baselines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dedicated Bilingual models (variants of Transformers).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most bilingual experiments used Transformer big and a shared source-target sentence-piece model (SPE).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For medium and low resource languages, the Transformer Base was also considered.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch size of 1 M tokes per-batch. Increasing the batch size improves model quality and speeds up convergence.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;effect-of-transfer-and-interference&quot;&gt;Effect of Transfer and Interference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper compares the following two setups with the baseline:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Combine all the datasets and train over them as if it is a single dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Combine all the datasets but upsample low resource languages so all that all the languages are equally likely to appear in the combined dataset.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A target “index” is prepended with every input sentence to indicate which language it should be translated into.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared encoder and decoder are used across all the language pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The two setups use a batch size of 4M tokens.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When all the languages are equally sampled, the performance on the low resource languages increases, at the cost of performance on high resource languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training over all the data at once reverse this trend.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;countering-interference&quot;&gt;Countering Interference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Temperature based sampling strategy is used to control the ratio of samples from different language pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A balanced sampling strategy improves the performance for the high resource languages (though not as good as the multilingual baselines) while retaining the high transfer performance on the low resource languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another reason behind the lagging performance (as compared to bilingual baselines) is the capacity of the multilingual models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some open problems to consider:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Task Scheduling - How to decide the order in which different language pairs should be trained.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Optimization for multitask learning - How to design optimizer, loss functions, etc. that can exploit task similarity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Understanding Transfer:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;For the low resource languages, translating multiple languages to English leads to improved performance than translating English to multiple languages.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This can be explained as follows: In the first case (many-to-one), the setup is that of a multi-domain model (each source language is a domain). In the second case (one-to-many), the setup is that of multitasking.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;NMT models seem to be more amenable to transfer across multiple domains than transfer across tasks (since the decoder distribution does not change much).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;In terms of zero-shot performance, the performance for most language pairs increases as the number of languages change from 10 to 102.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;effect-of-preprocessing-and-vocabulary&quot;&gt;Effect of preprocessing and vocabulary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sentence Piece Model (SPM) is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Temperature sampling is used to sample vocabulary from different languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using smaller vocabulary (and hence smaller sub-word tokens) perform better for low resource languages, probably due to improved generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low and medium resource languages tend to perform better with higher temperatures.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;effect-of-capacity&quot;&gt;Effect of Capacity&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Using deeper models improves performance (as compared to the wider models with the same number of parameters) on most language pairs.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Observational Overfitting in Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Observational-Overfitting-in-Reinforcement-Learning"/>
   <updated>2020-01-23T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Observational Overfitting in Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies &lt;em&gt;observational overfitting&lt;/em&gt;: The phenomenon where an agent overfits to different observation spaces even though the underlying MDP remains fixed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike other works, the “background information” (in the pixel space) is correlated with the progress of the agent (and is not just noise).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02975&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Base MDP $M = (S, A, R, T)$ where $S$ is the state space, $A$ is the action space, $R$ is the reward function, and $T$ is the transition dynamics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$M$ is parameterized using $\theta$. In practice, it means introducing an observation function $\phi_{\theta}$ ie $M_{\theta} = (M, \phi_{\theta})$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A distribution over $\theta$ defines a distribution over the MDPs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has access to the pixel space observations and not the state space observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization gap is defined as $J_{\theta}(\pi) - J_{\theta^{train}}(\pi)$ where $\pi$ is the learning agent, $\theta$ is the distribution over all the observation functions, $\theta^{train}$ is the distribution over the observation functions corresponding to the training environments. $J_{\theta}(\pi)$ is the average reward that the agent obtains over environments sampled from $M_{\theta}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\phi_{\theta}$ considers two featurs - generalizable (invariant across $\theta$) and non-generalizable (depends on $\theta$) ie $\phi_{\theta}(s) = concat(f(s), g_{\theta}(s))$ where $f$ is the invariant function and $g$ is the non-generalizable function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is set up such that “explicit regularization” can easily solve it. The focus is on understanding the effect of “implicit regularization”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;overparameterized-lqr&quot;&gt;Overparameterized LQR&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LQR is used as a proxy for deep RL architectures given its advantages like enabling exact gradient descent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The functions are parameterized as follows:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;$f(s) = W_c(s)$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$g_{\theta}(s) = W_{\theta}(s)$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observation at time $t$ , $o_t$, is given as $[W_c W_{\theta}]^{-1} s_t$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Action at time $t$ is given as $a_t = K o_{t}$ where $K$ is the policy matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dimensionality:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;state $s$: $d_{state}$ 100&lt;/li&gt;
      &lt;li&gt;$f(s)$: $d_{state}$ 100&lt;/li&gt;
      &lt;li&gt;$g_{\theta}(s)$: $d_{noise}$ 100&lt;/li&gt;
      &lt;li&gt;observation $o$: $d_{state}$ + $d_{noise}$ 1100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of training on just one environment, multiple solutions exist, and overfitting happens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing $d_{noise}$ increases the generalization gap.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overparameterizing the network decreases the generalization gap and also reduces the norm of the policy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projected-gym-environments&quot;&gt;Projected Gym Environments&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The base MDP is the Gym Environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$M_{\theta}$ is generated as before.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing both width and depth for basic MLPs improves generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization also depends on the choice of activation function, residual layers, etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deconvolutional-projections&quot;&gt;Deconvolutional Projections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the Gym environment, the actual state is projected to a larger vector and reshaped into an 84x84 tensor (image).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The image from $f$ is concatenated with the image from $g$. This setup is referred to as the Gym-Deconv.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The relative order of performance between NatureCNN, IMPALA, and IMPALA-Large (on both CoinRun and Gym-Deconv) is the same as the order of the number of parameters they contain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In an ablation, the policy is given access to only $g_{\theta}(s)$, which makes it impossible for the model to generalize. In this test of memorization capacity, implicit regularization seems to reduce the memorization effect.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overparameterization-in-coinrun&quot;&gt;Overparameterization in CoinRun&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The pixel space observation in CoinRun is downsized from 64x64 to 32x32 and flattened into a vector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In CoinRun, the dynamics change per level, and the noisy “irrelevant” features change location across the 1D input, making this setup more challenging than the previous ones.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overparameterization improves generalization in this scenario as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Rapid-Learning-or-Feature-Reuse-Towards-Understanding-the-Effectiveness-of-MAML"/>
   <updated>2020-01-16T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper investigated two possible reasons behind the usefulness of MAML algorithm:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Rapid Learning&lt;/strong&gt; - Does MAML learn features that are amenable for rapid learning?&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Feature Reuse&lt;/strong&gt; - Does the MAML initialization provide high-quality features that are useful for the unseen tasks.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This leads to a follow-up question: how much task-specific inner loop adaptation is needed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.09157&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a standard few-shot learning setup, the different datasets have different classes. Hence, the top-most layer (or the head) of the learning model should be different for different tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The subsequent discussion only applies to the body of the network (ie, network minus the head).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Freezing Layer Representations&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setup, a subset (or all) of parameters are frozen (after MAML training) and are not adapted during the representation.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even when the entire network is frozen, the performance drops only marginally.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This indicates that the representation learned by the meta-initialization is good enough to be useful on the test tasks (without requiring any adaptation step).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Note that the head of the network is still adapted during testing.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Representational Similarity&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setup, the paper reports the change in the latent representation (learned by the network) during the inner loop update with a fully trained model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Canonical Correlation Analysis (CCA) and Central Kernel Alignment (CKA) metrics are used to measure the similarity between the representations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The main finding is that the representations in the body of the network are very similar before and after the inner loop updates while the representations in the head of the network are very different.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The above two observations indicate that feature reuse is the primary driving factor for the success of MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;When does feature reuse happen&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper considers the model at different stages of training and compares the similarity in the representation (before and after the inner loop update).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even early in training, the CCA similarity between the representations (before and after the inner loop update) is quite high. Similarly, freezing the layers (for the test time update), early in training, does not degrade the test time performance much. This hints that the feature reuse happens early in the learning process.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-anil-almost-no-inner-loop-algorithm&quot;&gt;The ANIL (Almost No Inner Loop) Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The empirical evidence suggests that the success of MAML lies in the feature reuse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors build on this observation and propose a simplification of the MAML algorithm: ANIL or Almost No Inner Loop Algorithm&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In this algorithm, the inner loop updates are applied only to the head of the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Despite being much more straightforward, the performance of ANIL is close to the performance of MAML for both few-shot image classification and RL tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing most of the inner loop parameters speed up the computation by a factor of 1.7 (during training) and 4.1 (during inference).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;removing-the-inner-loop-update&quot;&gt;Removing the Inner Loop Update&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given that it is possible to remove most of the parameters from the inner loop update (without affecting the performance), the next step is to check if the inner loop update can be removed entirely.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This leads to the NIL (No Inner Loop) algorithm, which does not involve any inner loop adaptation steps.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A few-shot learning model is trained - either with MAML or ANIL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, the head is removed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each task, the K training examples are fed to the body to obtain class representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For a given test data point, the representation of the data point is compared with the different class representations to obtain the target class.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The NIL algorithm performs similar to the MAML and the ANIL algorithms for the few-shot image classification task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that it is still important to use MAML/ANIL during training, even though the learned head is not used during evaluation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper discusses the different classes of meta-learning approaches. It concludes with the observation that feature reuse (and not rapid adaptation) seems to be the common model of operation for both optimization-based meta-learning (e.g., MAML) and model-based meta-learning.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Accurate-Large-Minibatch-SGD-Training-ImageNet-in-1-Hour"/>
   <updated>2020-01-09T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Accurate Large Minibatch SGD - Training ImageNet in 1 Hour</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training models with large minibatches (using distributed synchronous SGD) can lead to optimization issues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents techniques for training models with large batch size while matching the accuracy of small minibatch setups.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on the ImageNet dataset, but many of the proposed ideas are applicable broadly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.02677&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-scaling-rule&quot;&gt;Linear Scaling Rule&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When the minibatch size increases by a factor of &lt;em&gt;k&lt;/em&gt;, the learning rate should also be increased by a factor of &lt;em&gt;k&lt;/em&gt; (while keeping all other hyperparameters like weight decay fixed).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that this is an empirical rule and is not expected to hold under all conditions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One such condition is when the model is changing rapidly during the first few epochs. In this case, a warmup phase is introduced to stabilize the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper verifies that the scaling rule is applicable to batch sizes as large as 8K.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;warmup&quot;&gt;Warmup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The learning rate should be gradually ramped up from a small value to a large value to allow convergence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization uses batch statistics to normalize the data. Hence, the loss corresponding to each data point (in the batch) is not independent. Thus, changing the batch size could change the underlying function being optimized.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the distributed SGD setup, the per-GPU (or per-worker) batch size should be kept constant, and only one worker should compute the batch norm statistics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pitfalls-when-using-distributed-sgd&quot;&gt;Pitfalls when using distributed SGD&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When using weight decay, scaling the cross-entropy loss is not the same as scaling the learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When using momentum, changing the learning rate could require “momentum correction.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensure that the per-worker loss is normalized by the size of the total minibatch and not just by the size of minibatch that each worker sees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each epoch, uses a single random shuffling of the training data (before dividing between the workers).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;communication&quot;&gt;Communication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes various techniques to speed up the training pipeline by reducing the communication overhead between nodes. (Each node can have one or more GPUs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First, a node sums the gradient from all the GPUs it has.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The gradients are shared and summed across all the nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each node broadcasts the resulting gradient to all the GPUs it has.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient Aggregation is performed in parallel with the backpropagation operator. While aggregating the gradient for one layer, the system starts computing the gradient of the next layer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using these approaches, a Resnet50 model can be trained on the ImageNet dataset in an hour (using 256 workers).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When an appropriate warmup strategy is used, the training and the validation curves (for the large batch size setup) matches the corresponding curves for the small batch size setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best performing warmup strategy is the one where training starts at a learning rate of 0.1 and linearly increases to 3.2 over five epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that the results are not specific to the Resnet50 model (experiments with Resnet101 model) or the use case (experiments with object detection and instance segmentation using Mask R-CNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Along with providing the empirical validation of the proposed ideas, the paper describes all the hyperparameters. It also includes the training and validation curves with the different configurations which enable others to replicate and build on this work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Superposition of many models into one</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Superposition-of-many-models-into-one"/>
   <updated>2020-01-02T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Superposition of many models into one</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a technique (called Parameter Superposition or PSP) for training and storing multiple models within a single set (or instance) of parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The different models exist in “superposition” and can be retrieved dynamically given task-specific context information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.05522&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameter-substitution&quot;&gt;Parameter Substitution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a task with input \(x \in R^N\) and parameter \(W$ \in R^{M \times N}\) where the output (target or features) are given as \(y=Wx\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now consider \(K\) such tasks with parameters \(W_1, W_2, \cdots W_K\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If each \(W_k\) requires only a small subspace in \(R^N\), then a linear transformation \(C_k^{-1}\) can be used such that each \(W_kC_k^{-1}\) occupies a mutually orthogonal subspace in \(R^N\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The set of parameters \(W_1, \cdots W_K\) can be represented by a single \(W^{M \times N}\) by adding \(W_kC_k^{-1}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter corresponding to the \(k^{th}\) task can be retrived (with some noise) using the context \(C_k\) as \(W^{~}_k = WC_k\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even though the retrieval is noisy, the effect of noise is limited for the context vectors used in the paper.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, \(\widetilde(y) = \widetilde(W)_{k}x = (WC_{k})x = W(C_{k}x)\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of learning \(K\) separate models, only \(K\) context vectors (along with 1 superimposed model) needs to be learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key assumption is that \(N\) (in \(x \in R^N)\) is large enough such that each \(W_k\) requires only a small subspace of \(R^N\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since images and speech signals tend to occupy a low dimensional manifold, this requirement can be satisfied by over-parameterizing x.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;choice-of-context-c&quot;&gt;Choice of Context C&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rotational Superposition (pspRotation)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Sample rotations uniformly from the orthogonal group \(O(M)\).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Downside is that if \(M \sim N\), it requires storing as many parameters as learning \(K\) individual models (since \(C\) is of the size of ##M \times M$$).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Complex Superposition (pspComplex)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The design of rotational superposition can be improved by choosing \(C_k\) to be a diagonal matrix ie \(C_k = diag(c_k)\) where \(c_k\) is a vector of size \(M\).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Choosing \(c_k\) to be a vector of complex numbers (of the form \(c_{k}^{j} = e^{i\phi_{j}(k)}\) where \(\phi_{j}(k)\) or the phase is sampled uniformly from \([-\pi, \pi]\)) leads to \(C_k\) being a digonal orthogonal matrix.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Powers of a single context&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The memory footprint can be further reduced by choosing the context vectors to be integral powers of the first context vector.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Binary Superposition (pspBinary)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This is a special case of complex superposition where the context vectors are binary.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-network-superposition&quot;&gt;Neural Network Superposition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The parameter superposition principle can be applied to all the linear layers of a network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the convolutional layers, it makes more sense to apply superposition to the convolutional kernel and not to the input image (as the dimensionality of convolutional parameters is smaller than that of inputs).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For all the experiments, the baseline is a standard supervised learning setup, unless mentioned otherwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The metric is the performance on the previous tasks when the model has been trained on the newer tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Input Interference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The input distribution changes over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Permuted MNIST dataset is used where each permutation of the pixels corresponds to a new task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A new task is sampled every 1000 mini-batches.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;As the network size increases, the performance of Parameter Superposition (psp) outperforms the baseline significantly.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;pspRotation &amp;gt; pspComplex &amp;gt; pspBinary in terms of both performance and the number of additional parameters required for each new task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Given that pspBinary is the easiest to implement while being comparable to more sophisticated baselines like Elastic Weight Consolidation (EWC) and Synaptic Intelligence, the paper presents most of the results with the pspBinary model.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continous Domain Shift&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Rotating-MNIST and Rotating-FashionMNIST tasks are proposed to simulate continuous domain shift.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In these tasks, the input images are rotated in-plane by a small angle such that the rotation is complete after 1000 steps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;A new context is assigned after 100 steps as per step changes in the angle would be very small.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The 10 context vectors used in the first 1000 steps are reused for the subsequent steps.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly changing the context vector&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The paper considers an ablation where the context vector is randomly changed at every step (of the 1000 step cycle). This required the superposition model to store 1000 models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This approach is better than the supervised learning baseline but not as good as the proposed psp* models.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output Interference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;This is the setup where the model transitions from one classification task to another.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Incremental CIFAR dataset is used with Resnet18 as the base model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Baseline is a standard supervised learning model where a new classification head is used for each task (since the classes have a different meaning in each dataset). The model component before the classification layer is shared across the tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Even though the labels are different across the datasets, the pspBinary model, trained with a single output layer, outperforms the multi-headed baseline.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Towards a Unified Theory of State Abstraction for MDPs</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Towards-a-Unified-Theory-of-State-Abstraction-for-MDPs"/>
   <updated>2019-12-26T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Towards a Unified Theory of State Abstraction for MDPs</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies five different techniques for stat abstraction in MDPs (Markov Decision Processes) and evaluates their usefulness for planning and learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The general idea behind abstraction is to map the actual (or observed) state to an abstract state that should be more amenable for learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It can be thought of as a mapping from one representation to another representation while preserving some useful properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/ca9a/2d326b9de48c095a6cb5912e1990d2c5ab46.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;general-definition&quot;&gt;General Definition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a MDP \(M = &amp;lt;S, A, P, R, \gamma&amp;gt;\) where \(S\) is the finite set of states, \(A\) is finite set of actions, \(P\) is the transition function, \(R\) is the bounded reward function and \(\gamma\) is the discount factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The abstract version of the MDP is \(\widetilde{M} = &amp;lt;\widetilde{S}, A, \widetilde{P}, \widetilde{R}, \gamma&amp;gt;\) where \(\widetilde{S}\) is the finite set if abstract states, \(\widetilde{P}\) is the transition function in the abstract state space and \(\widetilde{R}\) is the bounded reward function in the abstract reward space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abstraction function \(\phi\) is a function that maps a given state \(s\) to its abstract counterpart \(\widetilde{s}\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The inverse image \(\phi^{-1}(\widetilde{s})\) is the set of ground states that map to the \(\widetilde{s}\) under the abstraction function \(\phi\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A wieghing functioon \(w(s)\) is used to measure how much does a state \(s\) contribute to the abstract state \(\phi(s)\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topology-of-abstraction-space&quot;&gt;Topology of Abstraction Space&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given two abstraction functions \(\phi_{1}\) and \(\phi_{2}\), \(\phi_{1}\) is said to be &lt;em&gt;finer&lt;/em&gt; than \(\phi_{2}\) iff for any states \(s_{1}, s_{2}\) if \(\phi_{1}(s_{1}) = \phi_{1}(s_{2})\) then \(\phi_{2}(s_{1}) = \phi_{2}(s_{2})\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This &lt;em&gt;finer&lt;/em&gt; relation is reflex, antisymmetric, transitive and partially ordered.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;five-types-of-abstraction&quot;&gt;Five Types of Abstraction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While many abstractions are possible, not all abstractions are equally important.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model-irrelevance abstraction \(\phi_{model}\):&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If two states $s_{1}$ and $s_{2}$ have the same abstracted state, then their one-step model is preserved.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Consider any action \(a\) and any abstract state \(\widetilde{s}\), if \(\phi_{model}(s_{1} = \phi_{model}(s_{2})\) then \(R(s_1, a) = R(s_2, a)\) and \(\sum_{s&apos; \in \phi_{model}^{-1}\widetilde(s)}P_{s_1, s&apos;}^{a} = \sum_{s&apos; \in \phi_{model}^{-1}\widetilde(s)}P_{s_2, s&apos;}^{a}\).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(Q^{\pi}\)-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It preserves the state-action value finction for all the states.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;\(\phi_{Q^{\pi}}(s_1) = \phi_{Q^{\pi}}(s_2)\) implies \(Q^{\pi}(s_1, a) = Q^{\pi}(s_1, a)\).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(Q^{*}\)-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal state-action value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(a^{*}\)-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal action and its value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\phi_{\pi^{*}}\)-irrelevance abstraction:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It preserves the optimal action.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of &lt;em&gt;fineness&lt;/em&gt;, \(\phi_0 \geq \phi_{model} \geq \phi_{Q^{\pi}} \geq \phi_{Q^*} \geq \phi_{a^*} \geq \phi_{\pi^*}\). Here \(\phi_0\) is the identity mapping ie \(\phi_0(s) = s\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If a property applies to any abstraction, it also applies to all the finer abstractions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-theorems&quot;&gt;Key Theorems&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As we go from finer to coarser abstractions, the information loss increases (ie fewer components can be recovered) while the state-space reduces (ie the efficiency of solving the problem increases). This leads to a tradeoff when selecting abstractions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, with abstractions \(\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}, \phi_{a^*}\), the optimal abstract policy \(\widetilde(\pi)^*\) is optimal in the ground MDP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, if each state-action pair is visited infinitely often and the step-size decays properly, Q-learning with \(\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}\) converges to the optimal state-action value functions in the MDP. More conditions are needed for convergence in the case of the remaining two abstractions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For \(\phi_{model}, \phi_{Q^{\pi}}, \phi_{Q^*}, \phi_{a^*}\), the model built with the experience converges to the true abstract model with infinite experience if the weighing function \(w(s)\) is fixed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>ALBERT - A Lite BERT for Self-supervised Learning of Language Representations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"/>
   <updated>2019-12-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/ALBERT - A Lite BERT for Self-supervised Learning of Language Representations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes parameter-reduction techniques to lower the memory consumption (and improve training speed) of BERT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also proposes to use a self-supervised loss (based on inter-sentence coherence) and argues that this loss is better than the NSP loss used by BERT.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.11942&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT architecture is similar to that of BERT with three major differences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Factorized Embedding Parameterization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In BERT and followup works, the embedding size was tied to the size of the context vector.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Since context vector is expected to encoder the entire context, it needs to have a large dimensionality.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;One consequence of this choice is that even the embedding layer (which encodes the representation for each token) has a large size. This increases the overall memory footprint of the model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper proposed to factorize the embedding parameters into two smaller matrics.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The embedding layer learns a low dimensional representation of the tokens and this representation is projected into a high dimensional space.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cross-layer parameter sharing&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;ALBERT shares all the parameters across the layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inter-sentence coherence loss&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;BERT uses two losses - Masked Language Modeling loss (MLM) and Next Sentence Prediction (NSP).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In the NSP task, the model is provided a pair of sentences and it has to predict if the two sentences appear consecutively in the same document or not. Negative samples are created by sampling sentences from different documents.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The paper argues that NSP is not effective as a loss function as it merges topic prediction and coherence prediction into one task (as the two sentences come from different documents). The topic prediction is an easier task as compared to coherence prediction.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Hence the paper proposes to use the Sentence Order Prediction task where the model has to predict which of the two sentences comes first in a document. The negative samples are created by simply swapping the order in the positive samples. Hence both the sentences come from the same document and topic prediction alone can not be used to solve the task.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Different variants (in terms of size) of ALBERT and BERT models are compared (eg ALBERT, ALBERT-x, BERT-x, etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, ALBERT models have many-times fewer parameters as compared to the BERT models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets - BookCorpus, English Wikipedia.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT-xxlarge significantly outperforms the BERT-large model even though it has around 70% parameters as the BERT-large model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BERT-xlarge performs worse than BERT-base hinting that it is difficult to train such large models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT models also have better data throughput as compared to BERT models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the ALBERT models, an embedding size of 128 performs the best.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the hidden dimension is increased, the model obtains better performance, but with diminishing returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Very wide ALBERT models (say with a context size of 1024) do not benefit much from depth.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using additional training data boosts the performance for most of the downstream tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper empirically shows that using dropout could hurt the performance of the ALBERT models. This observation may not hold for BERT as it does not share parameters across layers and hence may need regularization via dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ALBERT also improves the state of the art performance on GLUE, SQuAD and RACE benchmarks, for both single-model and ensemble setup.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Everything Happens for a Reason - Discovering the Purpose of Actions in Procedural Text</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Everything-Happens-for-a-Reason-Discovering-the-Purpose-of-Actions-in-Procedural-Text"/>
   <updated>2019-12-12T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Everything Happens for a Reason - Discovering the Purpose of Actions in Procedural Text</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Procedural text comprehension tasks focus on modeling the effect of actions and predicting what happens next.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But they do not consider &lt;em&gt;why&lt;/em&gt; some actions need to happen before other actions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new model called XPAD (eXPlainable Action Dependency) that considers the &lt;em&gt;purpose&lt;/em&gt; of actions while predicting their effect.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model favors &lt;em&gt;effects&lt;/em&gt; that:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;explain more of actions in the text.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;are more plausible given the context.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An existing procedural text benchmark dataset (Propara) is expanded by adding the task of explaining actions by predicting their dependencies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.04745&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://data.allenai.org/propara/&quot;&gt;Link to the dataset&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Input&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Procedural (chronologically ordered text) sequence of &lt;em&gt;T&lt;/em&gt; sentences.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;List of &lt;em&gt;N&lt;/em&gt; participant entities, whose state changes at some step.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State change matrix $\pi(T \times N)$ with four possible states - move, create destroy, none.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This matrix tracks how property changes after each step.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dependency Explanation Graph&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Identify what steps are necessary to execute a given step (say &lt;em&gt;s&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;) and represent this dependency in the form of a dependency explanation graph &lt;em&gt;G = &amp;lt;S, E&amp;gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this graph, each node is a step and the direction of edge describes the order of dependency.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dependency-graph-dataset&quot;&gt;Dependency Graph Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06975&quot;&gt;Propara dataset&lt;/a&gt; is expanded to extract the dependency graph using both heuristic and automated methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The automated method is based on the coherence assumption that if step &lt;em&gt;s&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; changes state of entity &lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; then &lt;em&gt;s&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; is a precondition for the first subsequent step that changes the state of &lt;em&gt;e&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;xpad-model&quot;&gt;XPAD Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is based on the ProStruct system and uses an encoder-decoder based architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Input: Sentence &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; and entity &lt;em&gt;e&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sentence is encoded using the GloVe vectors and a BiLSTM model and the entity is encoded as an indicator variable.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The combined representation is denoted as &lt;em&gt;c&lt;sub&gt;tj&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This representation is passed through an MLP to generate &lt;em&gt;k&lt;/em&gt; logits that encode the probability of each entity &lt;em&gt;j&lt;/em&gt; undergoing a state change at step &lt;em&gt;t&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Beam search is performed to decode the encoder representation into the state change matrix and dependency graph using a score function that ensures global consistency.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Score function has two components:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;State change score - depends on the likelihood that the selected state changes at step &lt;em&gt;t&lt;/em&gt; given the text and state change history from steps &lt;em&gt;s&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; to &lt;em&gt;s&lt;sub&gt;t-1&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Dependency graph score&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;This is based on the connectivity and likelihood of the resulting dependency explanation graph.&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;This score is used to bias the graph search towards:&lt;/p&gt;

                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;p&gt;predictions that have an identifiable purpose ie checking if a particular state change prediction leads to a connection in the dependency explanation graph.&lt;/p&gt;
                  &lt;/li&gt;
                  &lt;li&gt;
                    &lt;p&gt;graphs that are more likely according to the background knowledge to distinguish likely dependency links from the unlikely ones.&lt;/p&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, XPAD has access to the correct path (in the search space) and learns to minimize the joint loss corresponding to predicting the state change and the dependency explanation graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, XPAD performs beam search to predict the most likely state change and dependency explanation graph.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tasks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;State change prediction&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Dependency explanation prediction&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.03969&quot;&gt;Recurrent Entity Networks&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.04582&quot;&gt;Query-Reduction Networks&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06975&quot;&gt;ProLocal and ProGlobal&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.10012&quot;&gt;ProStruct&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;XPAD significantly outperforms all the baseline models on the dependency explanation task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvements on the state change prediction task are less significant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing dependency graph scores from XPAD leads to a drop in the F1 score.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides an elaborate discussion on the different types of errors that the XPAD system makes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Mastering-Atari,-Go,-Chess-and-Shogi-by-Planning-with-a-Learned-Model"/>
   <updated>2019-12-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents the MuZero algorithm that performs planning with a learned model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The algorithm achieves state of the art results on Atari suite (where generally model-free approaches perform the best) and on planning-oriented games like Chess and Go (where generall planning approaches perform the best).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.08265&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-to-standard-model-based-approaches&quot;&gt;Relation to standard Model-Based Approaches&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model-based approaches generally focus on reconstructing the true environment state or the sequence of full observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero focuses on predicting only those aspects that are most relevant for planning - policy, value functions, and rewards.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model consists of three components: (representation) encoder, dynamics function, and the prediction network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has two kinds of interactions - real interactions (ie the actions that are actually executed in the real environment) and hypothetical or imaginary actions (ie the actions that are executed in the learned model or the dynamics function).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At any timestep &lt;em&gt;t&lt;/em&gt;, the past observations &lt;em&gt;o&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt;, … &lt;em&gt;o&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; are encoded into the state &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; using the encoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the model takes hypothetical actions for the next &lt;em&gt;K&lt;/em&gt; timesteps by unrolling the model for &lt;em&gt;K&lt;/em&gt; steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each timestep &lt;em&gt;k = 1, …, K&lt;/em&gt;, the dynamics model predicts the immediate reward &lt;em&gt;r&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; and a new hidden state &lt;em&gt;h&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; using the previous hidden state &lt;em&gt;h&lt;sub&gt;k-1&lt;/sub&gt;&lt;/em&gt; and action &lt;em&gt;a&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the same time, the policy &lt;em&gt;p&lt;sup&gt;k&lt;/sup&gt;&lt;/em&gt; and the value function &lt;em&gt;v&lt;sup&gt;k&lt;/sup&gt;&lt;/em&gt; are computed using the prediction network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The initial hidden state &lt;em&gt;h&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt; is initialized using the state &lt;em&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Any MDP Planning algorithm can be used to search for optimal policy and value function given the state transitions and the rewards induced by the dynamics function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, the MCTS (Monte Carlo Tree Search) algorithm is used and the action &lt;em&gt;a&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; (ie the action that is executed in the actual environment) is selected from the policy outputted by MCTS.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;collecting-data-for-the-replay-buffer&quot;&gt;Collecting Data for the Replay Buffer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;At each timestep &lt;em&gt;t&lt;/em&gt;, the MCTS algorithm is executed to choose the next action (which will be executed in the real environment).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting next observation &lt;em&gt;o&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; and reward &lt;em&gt;r&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; are stored and the trajectory is written to the replay buffer (at the end of the episode).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For every hypothetical step &lt;em&gt;k&lt;/em&gt;, match the predicted policy, value, and reward to the actual target values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The target policy is generated by the MCTS algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The target value function and reward are generated by actually playing the game (or the MDP).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-to-alphazero&quot;&gt;Relation to AlphaZero&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero leverages the search-based policy iteration from AlphaZero.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It extends AlphaZero to setups with a single agent (where self-play is not possible) and setups with a non-zero reward at the intermediate time steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The encoder and the predictions functions are similar to ones used by AlphZero.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;K&lt;/em&gt; is set to 5.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: 57 games in Atari along with Chess, Go and Shogi&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero achieves the same level of performance as AlphaZero for Chess and Shogi. In Go, MuZero slightly outperforms AlphaZero despite doing fewer computations per node in the search tree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Atari, MuZero achieves a new state-of-the-art compared to both model-based and model-free approaches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers a variant called MuZero Reanalyze that reanalyzes old trajectories by re-running the MCTS algorithm with the updated network parameter. The motivation is to have a better sample complexity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MuZero performs well even when using a single simulation of MCTS (during inference).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, using more simulations of MCTS helps to achieve better performance through even just 6 simulations per move is sufficient to learn a good model for Ms. Pacman.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Contrastive Learning of Structured World Models</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Contrastive-Learning-of-Structured-World-Models"/>
   <updated>2019-11-28T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Contrastive Learning of Structured World Models</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces Contrastively-trained Structured World Models (C-SWMs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These models use a contrastive approach for learning representations in environments with compositional structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.12247&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/tkipf/c-swm&quot;&gt;Link to the code&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The training data is in the form of an experience buffer \(B = \{(s_t, a_t, s_{t+1})\}_{t=1}^T\) of state transition tuples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The goal is to learn:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;an encoder \(E\) that maps the observed states $s_t$ (pixel state observations) to latent state $z_t$.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;a transition model \(T\) that predicts the dynamics in the hidden state.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model defines the enegry of a tuple \((s_t, a_t, s_{t+1})\) as \(H = d(z_t + T(z_t, a_t), z_{t+1})\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model has an inductive bias for modeling the effect of action as translation in the abstract state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An extra hinge-loss term is added: \(max(0, \gamma - d(z^{~}_{t}, z_{t+1}))\) where \(z^{~}_{t} = E(s^{~}_{t})\) is a corrputed latent state corresponding to a randomly sampled state \(s^{~}_{t}\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-oriented-state-factorization&quot;&gt;Object-Oriented State Factorization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The goal is to learn object-oriented representations where each state embedding is structured as a set of objects.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assuming the number of object slots to be \(K\), the latent space, and the action space can be factored into \(K\) independent latent spaces (\(Z_1 \times ... \times Z_K\)) and action spaces (\(A_1 \times ... \times A_k\)) respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are &lt;em&gt;K&lt;/em&gt; CNN-based object extractors and an MLP-based object encoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The actions are represented as one-hot vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A fully connected graph is induced over &lt;em&gt;K&lt;/em&gt; objects (representations) and the transition function is modeled as a Graph Neural Network (GNN) over this graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The transition function produces the change in the latent state representation of each object.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The factorization can be taken into account in the loss function by summing over the loss corresponding to each object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environments&quot;&gt;Environments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Grid World Environments - 2D shapes, 3D blocks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Atari games - Pong and Space Invaders&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3-body physics simulation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Random policy is used to collect the training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation is performed in the latent space (no reconstruction in the pixel space) using ranking metrics. The observations (to compare against) are randomly sampled from the buffer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines - auto-encoder based World Models and &lt;a href=&quot;https://arxiv.org/abs/1905.11169&quot;&gt;Physics as Inverse Graphics model&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the grid-world environments, C-SWM models the latent dynamics almost perfectly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Removing either the state factorization or the GNN transition model hurts the performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;C-SWM performs well on Atari as well but the results tend to have high variance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The optimal values of $K$ should be obtained by hyperparameter tuning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the 3-body physics tasks, both the baselines and proposed models work quite well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, the paper has a section on limitations:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The object extractor module can not disambiguate between multiple instances of the same object (in a scene).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The current formulation of C-SWM can only be used with deterministic environments.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Gossip based Actor-Learner Architectures for Deep RL</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Gossip-based-Actor-Learner-Architectures-for-Deep-RL"/>
   <updated>2019-09-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Gossip based Actor-Learner Architectures for Deep RL</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.04585&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers the task of training an RL system by sampling data from multiple simulators (over parallel devices).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is that of distributed RL setting with &lt;em&gt;n&lt;/em&gt; agents or actor-learners (composed of a single learner and several actors). These agents are trying to maximize a common value function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One (existing) approach is to perform on-policy updates with a shared policy. The policy could be updated in synchronous (does not scale well) or asynchronous manner (can be unstable due to stale gradients).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Off policy approaches allow for better computational efficiency but can be unstable during training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed Gossip based Actor-Learner Architecture (GALA) which uses asynchronous communication (gossip) between the &lt;em&gt;n&lt;/em&gt; agents to improve the training of Deep RL models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These agents are expected to converge to the same policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the different agents are not required to share the same policy and it is sufficient that the agent’s policies remain $\epsilon$-close to each other. This relaxation allows the policies to be trained asynchronously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA approach is combined with A2C agents resulting in GALA-A2C agents. They have better computational efficiency and scalability (as compared to A2C) and similar in performance to A3C and Impala.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training alternates between one local policy-gradient (and TD update) and asynchronous gossip between agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During the gossip step, the agents send their parameters to some of the other agents (referred to as the peers) and update their parameters based on the parameters received from the other agents (for which the given agent is a peer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA agents are implemented using non-blocking communication so that they can operate asynchronously.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper includes the proof that the policies learned by the different agents are within $\epsilon$ distance of each other (ie all the policies lie within an $\epsilon$-distance ball) thus ensuring that the policies do not diverge much from each other.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Six games from the Ataru 2600 games suite are used for the experiments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines: A2C, A3C, Impala&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GALA agents are configured in a directed ring graph topology.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With A2C, as the number of simulators increases, the number of convergent runs (runs with a threshold reward) decreases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using gossip algorithms increases or maintains the number of convergent runs. It also improves the performance, sample efficiency and compute efficiency of A2C across all the six games.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When compared to Impala and A3C, GALA-A2C generally outperforms (or performs as well as) those baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the learned policies remain within an $\epsilon$ ball, the agent’s gradients are less correlated as compared to the A2C agents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How to train your MAML</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/How-to-train-your-MAML"/>
   <updated>2019-09-05T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/How to train your MAML</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes MAML++ - a modification of MAML algorithm that stabilizes its training improves generalization performance and reduces the computational overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.09502&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;h3 id=&quot;unstable-training&quot;&gt;Unstable Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training the outer loop requires unfolding the inner loop multiple times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In absence of skip connections, the gradient is multiplied by the same parameter multiple times.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Large depth and absent skip connections could lead to exploding and vanishing gradients respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to stabilize the gradient propagation by minimizing the target set loss computed by the base-network after every step towards a support set task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is important to anneal the contribution of earlier steps and increase the contribution of later steps over time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;second-order-derivatives-are-expensive-to-compute&quot;&gt;Second Order derivatives are expensive to compute&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While the first-order MAML is faster, the resulting model may not have as good a generalization error as the second-order MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to use derivative order annealing where the first order gradients are used for the first 50 epochs and the network uses second-order gradients from thereon.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This derivative order annealing appears to be more stable than models that use second-order derivatives only.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In MAML, the statistics of the current batch are used for normalization instead of accumulating the running statistics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to collect the statistics per step which can increase the convergence speed, stability, and generalization performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In MAML, the batch normalization biases are not updated in the inner-loop which can adversely impact the performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a set of biases (per step) within the inner loop update.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fixed-learning-rate&quot;&gt;Fixed Learning Rate&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MAML uses a single learning rate across all the steps and all the parameters. This means there is one single learning rate that needs to be hyperparameter to work well for all the layers and steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An alternate solution would be to learn a separate learning rate per parameter but this can be impractical as it doubles the number of parameters to be learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a learning rate and direction for each layer in the network, for each step it takes in the inner loop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also proposed to anneal the learning rate of the outer loop (using cosine annealing) as it helps to achieve better generalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using these modifications helps to outperform the MAML model on both Omniglot and MiniImagenet datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The biggest benefit comes by learning the per-layer, per-step learning rates and by using the per-step batch normalization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>PHYRE - A New Benchmark for Physical Reasoning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/PHYRE-A-New-Benchmark-for-Physical-Reasoning"/>
   <updated>2019-08-29T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/PHYRE - A New Benchmark for Physical Reasoning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes the PHYRE (PHYsical REasoning) benchmark - consisting of classic mechanical puzzles in 2D physical environments - as a means to evaluate the physical reasoning ability of machine learning models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.05656&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D world that obeys Newtonian mechanics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gravitational force + Friction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-deformable objects that can be static (ie fixed) or dynamic (ie can move and are affected by collisions etc).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent starts in some initial world state (ie configuration of objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal is described in the form of (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subject&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relation&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object&lt;/code&gt;) where the agent’s task is to satisfy the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relation&lt;/code&gt; between the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subject&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Currently, only the “touch” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relation&lt;/code&gt; is supported.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The learning agent has to take a single action - placing one or more new dynamic objects in the world.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A simulator is run on the new configuration (for a fixed amount of time) to check if the goal condition is satisfied.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the end of the simulation, a binary reward and intermediate observations (collected as the simulator executes) are provided to the learning agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These observations are 256*256 grids where each grid cell can take 1 of the 7 values (denoting different types of objects).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since only one relation supported currently, the color is sufficient to encode the goal.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmark-tiers&quot;&gt;Benchmark Tiers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two benchmark tiers are provided where each tier comprises of a combination of:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;a predefined set of all the actions that the agent is allowed to perform.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;set of tasks that can be solved by at least one action from the allowed action set.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PHYRE-B&lt;/strong&gt; - The agent is allowed to place a single (ball of any radii) at any valid location.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PHYRE-2B&lt;/strong&gt; - The agent is allowed to place 2 balls at any valid pair of locations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each of the two tiers has 25 task templates where each template comprises of variants of a single task (same goal but different initial conditions).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two evaluation setups are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;within-template&lt;/strong&gt; where the agent is trained on some tasks in a template and evaluated on a set of held-out tasks from the same template.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;cross-template&lt;/strong&gt; where the agent is evaluated on tasks from a different template.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the training phase, the model has access to the simulator (but not to the correct solution). So the model could learn an action-prediction model or forward dynamics model or both.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the testing phase, the model can query the simulator only a few times. Each query provides it with the binary reward and the intermediate observations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-measure&quot;&gt;Performance Measure&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The emphasis is on solving more tasks (in few queries) during the test phase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This requirement is captured using a metric called AUCCESS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the tasks in PHYRE-2B are harder than tasks in PHYRE-B.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baseline-agents&quot;&gt;Baseline Agents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Random Agent - Randomly samples actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-parametric agent (MEM) - generates R actions at random and uses the simulator to check how many tasks can be solved using these R random actions. During testing, try the R actions in the decreasing order of the number of tasks they solve.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-parametric agent with online learning (MEM-O) - Variant of MEM where an online adaptation step is performed during test time (to update the rank of the actions).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep Q Networks with an action encoder, observation encoder and fusion model (combine action and observation representation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DQN with online learning (DQN-0): Variant of DQN with online updates (during the test phase).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Contextual bandits.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy learning approaches like PPO and A2C.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Both Contextual bandits and policy-based approaches show poor training stability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best agent, DQN-O, reaches AUCCESS of 56.2\% on PHYRE-B and 39.26\% on PHYRE-2B. In general, agents with online adaptation perform better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The tasks are designed such that 100000 attempts are sufficient to solve 100\% of tasks in PHYRE-B and 95\% of tasks in PHYRE-2B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even though only two tiers are provided right now, the benchmark is readily extensible and new tasks can be added in the future.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Large Memory Layers with Product Keys</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Large-Memory-Layers-with-Product-Keys"/>
   <updated>2019-08-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Large Memory Layers with Product Keys</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes a structured key-value memory layer that:
    &lt;ul&gt;
      &lt;li&gt;Can scale to a very large size (and capacity).&lt;/li&gt;
      &lt;li&gt;Has very low computational overhead.&lt;/li&gt;
      &lt;li&gt;Supports exact search in the keyspace.&lt;/li&gt;
      &lt;li&gt;Can be easily integrated with neural networks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.05242&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The memory layer is composed of 3 components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Query Network&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Maps input to a latent space.&lt;/li&gt;
          &lt;li&gt;Can be implemented as a feed-forward network.&lt;/li&gt;
          &lt;li&gt;Adding batch-norm on top of the query network helps to spread out keys.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Key selection module&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Lets say there are a total of &lt;em&gt;K&lt;/em&gt; keys of dimensionality &lt;em&gt;d&lt;sub&gt;q&lt;/sub&gt;&lt;/em&gt; of which we want to select top &lt;em&gt;k&lt;/em&gt; keys.&lt;/li&gt;
          &lt;li&gt;Partition the set of keys into two sets of &lt;em&gt;subkeys&lt;/em&gt; (say &lt;em&gt;Q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;Q&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;) where each subset has &lt;em&gt;K&lt;/em&gt; keys of dimensionality &lt;em&gt;d_q/2&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;The query is split into two subqueries (say &lt;em&gt;q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;q&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;).&lt;/li&gt;
          &lt;li&gt;Each of these two queries are compared with every query in their corresponding set of &lt;em&gt;subkeys&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;For example, &lt;em&gt;q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is compared with every query is &lt;em&gt;Q&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;Top &lt;em&gt;k&lt;/em&gt; ranked keys are selected from each set to create two new sets &lt;em&gt;C&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;C2&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;The keys from these two sets are combined uder the concatenation operator to obtain &lt;em&gt;k&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; vectors.&lt;/li&gt;
          &lt;li&gt;the final top &lt;em&gt;k&lt;/em&gt; (concatenated) keys are searched from these *k&lt;sup&gt;2* keys.&lt;/sup&gt;&lt;/li&gt;
          &lt;li&gt;The overall complexity is $O((\sqrt K + k^2) \times d_q)$ where &lt;em&gt;K&lt;/em&gt; is the total number of keys (whiuc)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Value lookup table&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;The values (corresponding to selected subkeys) are aggregated (using weighted sum operation) to obtain the output.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the parameters are trainable, though, in practice, only the selected &lt;em&gt;k&lt;/em&gt; memory slots are updated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Multihead attention mechanism helps to improve the performance further.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;1 or more feedforward layers in transformers are placed by the memory layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is evaluated on large scale language modeling tasks with 140 Gb of data from common crawl corpora (28n billion words).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Perplexity on the test set.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Fraction of accessed values.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;KL divergence between the (normalized) weights of key access and uniform distribution.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The last two metrics are used together to determine how well the keys are utilized.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the large size of the training dataset, adding more layers to the transformer model helps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Effect of using memory layer is more powerful than the effect of adding new layers to the transformer. For example, a 12 layer transformer + memory layer outperforms a 24 layer transformer while being almost twice as fast.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best position to place the memory is at an intermediate layer and placing the memory layer right after the input or just before the softmax layer does not work well in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Abductive Commonsense Reasoning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Abductive-Commonsense-Reasoning"/>
   <updated>2019-08-15T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Abductive Commonsense Reasoning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents the task of abductive NLP (pronounced as &lt;em&gt;alpha NLP&lt;/em&gt;) where the model needs to perform abductive reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abductive reasoning is the inference to the most plausible explanation. Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided. The dataset models the task as multiple-choice questions to make the evaluation process easy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.05739&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task-setup&quot;&gt;Task Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a pair of observations &lt;em&gt;O&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;O&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; and two hypothesis &lt;em&gt;h&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;h&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, the task is to select the most plausible hypothesis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, &lt;em&gt;P(h | O&lt;sub&gt;1&lt;/sub&gt;, O&lt;sub&gt;2&lt;/sub&gt;)&lt;/em&gt; is propotional to &lt;em&gt;P(h |O&lt;sub&gt;1&lt;/sub&gt;)P(O&lt;sub&gt;2&lt;/sub&gt;|h, O&lt;sub&gt;1&lt;/sub&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the “fully connected” assumption would jointly model both the observations and the hypothesis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observation pairs are created using the &lt;a href=&quot;https://aclweb.org/anthology/N16-1098&quot;&gt;ROCStories dataset&lt;/a&gt; which is a collection of short, manually crafted stories of 5 sentences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The average word length for both the content and the hypothesis is between 8 to 9.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To collect plausible hypothesis, the crowd workers were asked to fill in a plausible “in-between” sentence in natural language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adversarial filtering approach from &lt;a href=&quot;https://aclweb.org/anthology/D18-1009&quot;&gt;Zellers et al.&lt;/a&gt; is used with BERT as the adversary. A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-observations&quot;&gt;Key Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Human performance: 91.4%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entailment NLI baseline: 59%. This highlights the additional complexity of abductive NLI as compared to entailment NLI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BERT: 68.9%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GPT: 63.1%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Numerical and spatial knowledge-based data points are particularly hard.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Deep-Reinforcement-Learning-in-a-Handful-of-Trials-using-Probabilistic-Dynamics-Models"/>
   <updated>2019-08-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new algorithm called as Probabilistic Ensemble with Trajectory sampling (PETS) that combines uncertainty aware deep learning models (ensemble of deep learning models that encode uncertainty) with sampling-based uncertainty propagation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PETS improves over other probabilistic MBRL approaches by isolating epistemic uncertainty (due to limited training data) and aleatoric uncertainty (inherent in the system).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;uncertainty-aware-neural-network-dynamics-model&quot;&gt;Uncertainty-Aware Neural Network Dynamics Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Aleatoric uncertainty can be accounted for by learning a parameterized distribution (probabilistic neural network) trained with negative log-likelihood.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Epistemic uncertainty can be accounted for by either having an infinite amount of data or by using ensembles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses a neural network to predict the mean and standard deviation of a gaussian distribution which defines the predictive model. This setup is referred to as the “probabilistic” model and denoted by &lt;strong&gt;P&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternate setup of the deterministic model is where a neural network is used to make a point prediction (and is denoted by &lt;strong&gt;D&lt;/strong&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble of probabilistic models is denoted as &lt;strong&gt;PE&lt;/strong&gt; while that of deterministic models is denoted as &lt;strong&gt;DE&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;planning-and-control-with-learned-dynamics&quot;&gt;Planning and Control with learned Dynamics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model Predictive Control (MPC) is used for planning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a start state and an action sequence, the probabilistic dynamics model induces a distribution over the trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first action, among the sequence of optimized actions, is executed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of random shooting, &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/B9780444538598000035&quot;&gt;Cross Entropy Method (CEM)&lt;/a&gt; is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let us say there are B bootstrap models in the ensemble. Given the current state, P particles are created and each particle is propagated using one of the bootstrap models. Two variants are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;TS1 - At each timestep, each particle samples a bootstrap. In this case, particle separation can not be attributed to ti the compounding effects of the bootstraps.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;TS$\infty$ - The bootstrapped model (per particle) is sampled just once and is not changed after that. This setup separates aleatoric and epistemic uncertainty. Aleatoric state variance is the average variance of the particles of the same bootstrap while epistemic state variance is the variance of the average of particles of same bootstrap indexes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach reaches the asymptotic performance of state-of-the-art model-free algorithms in much fewer samples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The general performance trend is probabilistic emsemble &amp;gt; probabilisitc model &amp;gt; deterministc ensemble &amp;gt; determinisitc model./.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Initial experiments for learning policy by propagating gradients through the ensemble of models did not work and has been left as future work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Assessing Generalization in Deep Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Assessing-Generalization-in-Deep-Reinforcement-Learning"/>
   <updated>2019-08-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Assessing Generalization in Deep Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Several RL algorithms are evaluated and the key takeaway is that the “vanilla” RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.12282&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two generalization regimes are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Interpolation - parameters of the test environment are similar to the parameters of the training environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation - parameters of the test environment are different from the parameters of the training environment.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Following algorithms are considered as part of the benchmark:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;“Vanilla” RL algorithms - A2C, PPO&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RL algorithms that are designed to generalize:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;RL&lt;sup&gt;2&lt;/sup&gt; - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The models are either composed of feedforward networks completely or feedforward + recurrent networks.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environments&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HalfCheetah and Hopper from OpenAI Roboschool.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Three versions of each environment are considered:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Deterministic: Environment parameters are fixed. This case corresponds to the standard environment setup in classical RL.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Random: Environment parameters are sampled randomly. This case corresponds to sampling from a distribution of environments.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Extreme: Environment parameters are sampled from their extreme values. This case corresponds to the edge-case environments which would not be encountered during training generally.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Performance Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Average total reward per episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Success percentage: Percentage of episodes where a certain goal (or reward) is obtained.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluation Metrics/Setups&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Default: success percentage when training and evaluating the deterministic version of the environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Interpolation: success percentage when training and evaluating on the random version of the environment.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation: the geometric mean of the success percentage of following three versions:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Train on deterministic and evaluate on the random version.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Train on deterministic and evaluate on extreme version.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Train on random and evaluate on the extreme version.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Observations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Extrapolation is harder than interpolation.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EPOpt improves generalization only for continuous control environments and only with PPO.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RL&lt;sup&gt;2&lt;/sup&gt; is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Quantifying Generalization in Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Quantifying-Generalization-in-Reinforcement-Learning"/>
   <updated>2019-07-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Quantifying Generalization in Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.02341&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;coinrun-environment&quot;&gt;CoinRun Environment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CoinRun is made of multiple levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are many obstacles in between and colliding with an obstacle leads to agent’s death.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each episode extends for a maximum for 1000 steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalization&quot;&gt;Generalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;9 agents are trained to play CoinRun, on different training sets (each with a different number of levels).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-architectures&quot;&gt;Evaluating Architectures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two convolutional architectures (of different sizes) are compared:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Nature-CNN: The CNN architecture used in the &lt;a href=&quot;https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf&quot;&gt;Deep Q Network&lt;/a&gt;. This is the smaller network among the two models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;IMPALA-CNN: The CNN architecture used in the &lt;a href=&quot;https://arxiv.org/abs/1802.01561&quot;&gt;Imapla architecture&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;IMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization. But increasing the network size beyond a limit gives diminishing returns.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluating-regularization&quot;&gt;Evaluating Regularization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color. This approach seems to improve performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch Normalization helps to improve performance as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus. Both these forms of stochasticity boost performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach. This suggests that these different approaches induce similar generalization properties.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additional-environments&quot;&gt;Additional Environments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CoinRun-Platforms:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Unlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Levels are larger as well so the agent might need to backtrack their steps.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;RandomMazes:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Partially observed environment with square mazes of dimensions 3x3 to 25x25.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Timelimit of 500 steps&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overfitting is observed for both these environments as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Set Transformer - A Framework for Attention-based Permutation-Invariant Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Set-Transformer-A-Framework-for-Attention-based-Permutation-Invariant-Neural-Networks"/>
   <updated>2019-07-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Set Transformer A Framework for Attention-based Permutation-Invariant Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider problems where the input to the model is a set. In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In “set pooling” methods (&lt;a href=&quot;https://arxiv.org/abs/1606.02185&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1703.06114&quot;&gt;2&lt;/a&gt;), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the “sum” operator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This approach can be shown to be bot permutation-invariant and a universal function approximator.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.00825&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An attention function &lt;em&gt;Attn(Q, K, V) = (QK&lt;sup&gt;T&lt;/sup&gt;)V&lt;/em&gt; is used to map queries &lt;em&gt;Q&lt;/em&gt; to output using key-value pairs &lt;em&gt;K, V&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of multi-head attention, the key, query, and value are projected into &lt;em&gt;h&lt;/em&gt; different vectors and attention is applied on all these vectors. The output is a linear transformation of the concatenation of all the vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;set-transformer&quot;&gt;Set Transformer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;3 modules are introduced: MAB, SAB and ISAB.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Set Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie &lt;em&gt;SAB(X) = MAB(X, X)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The time complexity of the SAB operation is &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt; where &lt;em&gt;n&lt;/em&gt; is the number of elements in the set. It can be reduced to &lt;em&gt;O(m*n)&lt;/em&gt; by using Induced Set Attention Blocks (ISAB) with &lt;em&gt;m&lt;/em&gt; induced point vectors (denoted as I).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;ISAB&lt;sub&gt;m&lt;/sub&gt; = MAB(X, MAB(I, X))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ISAB can be seen as performing a low-rank projection of inputs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These modules can be used to model the interactions between data points in any given set.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pooling-by-multihead-attention-pma&quot;&gt;Pooling by Multihead Attention (PMA)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Aggregation is performed by applying multi-head attention on a set of &lt;em&gt;k&lt;/em&gt; seed vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The interaction between the &lt;em&gt;k&lt;/em&gt; outputs (from PMA) can be modeled by applying another SAB.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thus the entire network is a stack of SABs and ISABs. Both the modules are permutation invariant and so is any network obtained by stacking them.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets include:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Predicting the maximum value from a set.&lt;/li&gt;
      &lt;li&gt;Counting unique (Omniglot) characters from an image.&lt;/li&gt;
      &lt;li&gt;Clustering with a mixture of Gaussians (synthetic points and CIFAR 100).&lt;/li&gt;
      &lt;li&gt;Set Anomaly detection (celebA).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, increasing &lt;em&gt;m&lt;/em&gt; (the number of inducing datapoints) improve performance, to some extent. This is somewhat expected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The work has two main benefits over prior work:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Reducing the &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt; complexity to &lt;em&gt;O(m*n)&lt;/em&gt; complexity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using self-attention mechanism for both encodings the inputs and for aggregating the encoded representations.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Measuring abstract reasoning in neural networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Measuring-Abstract-Reasoning-in-Neural-Networks"/>
   <updated>2019-06-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Measuring Abstract Reasoning in Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Visual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let’s say that we want to test if the model understands the abstract notion of “increasing”. We could train the model on data that captures the notion of “increasing”, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;h3 id=&quot;ravens-progressive-matrics-rpms&quot;&gt;Raven’s Progressive Matrics (RPMs):&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;procedurally-generated-matrices-pgms&quot;&gt;Procedurally Generated Matrices (PGMs)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generating RPM like matrices procedurally by building an abstract structure for matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;The abstract structure &lt;em&gt;S&lt;/em&gt; consists of 3 components: (i) Relation types (&lt;em&gt;R&lt;/em&gt;), (ii) Object types (&lt;em&gt;O&lt;/em&gt;) and (iii) Attribute types (&lt;em&gt;A&lt;/em&gt;). ie *S = {(r, o, a)&lt;/td&gt;
          &lt;td&gt;r in R, o in O and a in A}*.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be read as: “Structure &lt;em&gt;S&lt;/em&gt; is instantiated on attribute &lt;em&gt;a&lt;/em&gt; of object &lt;em&gt;o&lt;/em&gt; and exhibits the relation &lt;em&gt;r&lt;/em&gt;”. For example, &lt;em&gt;S&lt;/em&gt; is instantiated on “color” of object “shape” and exhibits the relation “increasing”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the structure could be made of more than one such tuple and more are the tuples, harder is the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the structure, sample values &lt;em&gt;v&lt;/em&gt; for each attribute &lt;em&gt;a&lt;/em&gt; while conforming with the relation &lt;em&gt;r&lt;/em&gt;. For example, if the attribute is “color” and the relation is “increasing”, the intensity of color must increase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The resulting structure is rendered as pixels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;test-for-generalization&quot;&gt;Test for Generalization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper tests for the following generalization scenarios:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neutral: The structure of the training and test data can contain any tuple.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: 8 context panels (from the 3x3) matrix where the last panel needs to be filled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN-MLP - 4 layer CNN with batchnorm and ReLU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Context-blind ResNet - ResNet network without the context (or the 8 input panels).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WReN model outperforms the other models on the Neutral setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models have a harder time differentiating between size than quantity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WRen is the best performing models in all the setups and rest of the discussion only applies to that model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auxiliary-training&quot;&gt;Auxiliary Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The auxiliary training helps in all the cases. Further, the model’s accuracy on the main task is where in the cases where it solves the auxiliary tasks well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-takeaway&quot;&gt;Key Takeaway&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using auxiliary loss that encourages the model to “explain” its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hamiltonian Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hamiltonian-Neural-Networks"/>
   <updated>2019-06-20T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hamiltonian Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a very cool idea at the intersection of deep learning and physics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01563&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/greydanus/hamiltonian-nn&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://greydanus.github.io/2019/05/15/hamiltonian-nns/&quot;&gt;Link to author’s blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hamiltonian-mechanics&quot;&gt;Hamiltonian Mechanics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is a branch of physics that can describe systems which follow some conservation laws and invariants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a set of &lt;em&gt;N&lt;/em&gt; pair of coordinates [(q&lt;sub&gt;1&lt;/sub&gt;, p&lt;sub&gt;1&lt;/sub&gt;), …, (q&lt;sub&gt;N&lt;/sub&gt;, p&lt;sub&gt;N&lt;/sub&gt;)] where &lt;strong&gt;q&lt;/strong&gt; = [q&lt;sub&gt;1&lt;/sub&gt;, …, q&lt;sub&gt;N&lt;/sub&gt;] dnotes the position of the set of objects while &lt;strong&gt;p&lt;/strong&gt; = [p&lt;sub&gt;1&lt;/sub&gt;, …, p&lt;sub&gt;N&lt;/sub&gt;] denotes the momentum of the set of variables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Together these &lt;em&gt;N&lt;/em&gt; pairs completely describe the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A scalar function &lt;em&gt;H(&lt;strong&gt;q&lt;/strong&gt;, &lt;strong&gt;p&lt;/strong&gt;)&lt;/em&gt;, called as the Hamiltonian is defined such that the partial derivative of &lt;em&gt;H&lt;/em&gt; with respect to &lt;strong&gt;p&lt;/strong&gt; is equal to derivative of &lt;strong&gt;q&lt;/strong&gt; with respect to time &lt;em&gt;t&lt;/em&gt; and the negative of partial derivative of &lt;em&gt;H&lt;/em&gt; with respect to &lt;strong&gt;q&lt;/strong&gt; is equal to derivative of &lt;strong&gt;p&lt;/strong&gt; with respect to time &lt;em&gt;t&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be expressed in the form of the equation as follows:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation1.png&quot; alt=&quot;equation1&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hamiltonian-neural-network-hnn&quot;&gt;Hamiltonian Neural Network (HNN)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Hamiltonian &lt;em&gt;H&lt;/em&gt; can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The loss function looks as follows:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/HNN/equation2.png&quot; alt=&quot;equation2&quot; width=&quot;400&quot; height=&quot;50&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The partial derivatives can be obtained by computing the &lt;em&gt;in-graph&lt;/em&gt; gradient of the output variables with respect to the input variables.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the final experiment, the model is trained on pixel observations and not state observations. In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations. Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Extrapolating-Beyond-Suboptimal-Demonstrations-via-Inverse-Reinforcement-Learning-from-Observations"/>
   <updated>2019-06-13T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard IRL approaches aim to learn a reward function that “justifies” the demonstration policy and hence those approaches cannot outperform the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In contrast, T-REX aims to learn a reward function that “explains” the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06387&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The input is a sequence of trajectories &lt;em&gt;T&lt;sub&gt;1&lt;/sub&gt;, … T&lt;sub&gt;m&lt;/sub&gt;&lt;/em&gt; which are ranked in the order of preference. That is, given any pair of trajectories, we know which of the two trajectories is better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reward Inference&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;A parameterized reward function &lt;em&gt;r&lt;sub&gt;θ&lt;/sub&gt;&lt;/em&gt; is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Given a trajectory, the reward function predicts the reward for each state. The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy Optimization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Once a reward function has been learned, standard RL approaches can be used to train a new policy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Demonstrations generated using PPO (checkpointed at different stages of training).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ensemble of networks used to learn the reward functions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach outperforms the baselines &lt;a href=&quot;https://arxiv.org/abs/1805.01954&quot;&gt;Behaviour Cloning from Observations&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1606.03476&quot;&gt;Generative Adversarial Imitation Learning&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Meta-Reinforcement Learning of Structured Exploration Strategies</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Meta-Reinforcement-Learning-of-Structured-Exploration-Strategies"/>
   <updated>2019-06-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Meta-Reinforcement Learning of Structured Exploration Strategies</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper looks at the problem of learning structured exploration policies for training RL agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Link to the &lt;a href=&quot;https://arxiv.org/abs/1802.07245&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;structured-exploration&quot;&gt;Structured Exploration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a stochastic, parameterized policy π&lt;sub&gt;θ&lt;/sub&gt;(a|s) where θ represents the &lt;em&gt;policy-parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a distibution over the tasks p(T). At the start of any episode of the i&lt;sup&gt;th&lt;/sup&gt; task, a latent variable z&lt;sub&gt;i&lt;/sub&gt; is sampled from the distribution &lt;em&gt;N(μ&lt;sub&gt;i&lt;/sub&gt;, σ&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; where μ&lt;sub&gt;i&lt;/sub&gt; and σ&lt;sub&gt;i&lt;/sub&gt; are the learned parameters of the distribution and are referred to as the &lt;em&gt;variation parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once sampled, the same &lt;em&gt;z&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution π&lt;sub&gt;θ&lt;/sub&gt;(a|s, z&lt;sub&gt;i&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The intuition is that the latent variable z&lt;sub&gt;i&lt;/sub&gt; would encode the notion of a task or goal that does not change arbitrarily during the episode.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-agnostic-exploration-with-structured-noise&quot;&gt;Model Agnostic Exploration with Structured Noise&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the &lt;em&gt;policy-parameters&lt;/em&gt; and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General meta-RL approaches have two limitations when it comes to “learning to explore”:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively.&lt;/li&gt;
      &lt;li&gt;Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Idea behind MAESN is to meta-train &lt;em&gt;policy-parameters&lt;/em&gt; so that they learn to use the task-specific &lt;em&gt;latent variables&lt;/em&gt; for exploration and can quickly adapt to a new task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For every iteration of meta-training, an “inner” gradient update is performed on the variational parameters and the &lt;em&gt;post-inner-update&lt;/em&gt; parameters are used to perform the meta-update.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors report that performing the “inner” gradient update on the &lt;em&gt;policy-parameters&lt;/em&gt; does not help the overall learning objective and that the step size for each parameter had to be meta-learned.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After training, the &lt;em&gt;variational parameters&lt;/em&gt; for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the &lt;em&gt;variational parameters&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training. To be fair to the baselines, reward shaping is used while training baselines as well. Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion. Each task distribution has 100 meta-training tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions. In the Locomotion task distributions, the different tasks correspond to the different goal positions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;• In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Relational Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Relational-Reinforcement-Learning"/>
   <updated>2019-06-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Relational Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.01830&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterative computations can be used to capture higher-order interactions between entities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entity extraction is based on the assumption that entities are things located at a particular point in space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A CNN is used to parse the pixel space observation into &lt;em&gt;k&lt;/em&gt; feature maps of size &lt;em&gt;nxn&lt;/em&gt;. The &lt;em&gt;(x, y)&lt;/em&gt; coordinates are concatenated to each &lt;em&gt;k-&lt;/em&gt;dimensional pixel feature-vector to indicate the pixel’s position in the map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting &lt;em&gt;n&lt;sup&gt;2&lt;/sup&gt; x k&lt;/em&gt; matrix acts as the entity matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actor-critic architecture (using distributed agent IMPALA) is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;h3 id=&quot;box-world&quot;&gt;Box-World&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;12 x 12-pixel room with keys and boxes placed randomly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Agent can move in 4 directions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal. (ii) The number of distractor branches, (iii)  Length of distractor branches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;starcraft-ii-minigames&quot;&gt;StarCraft II minigames&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;9 mini games designed as specific scenarios in the Starcraft game are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;box-world-1&quot;&gt;Box-World&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Visualising the attention scores indicate that:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;keys attend to locks they can unlock.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;all objects attend to agent’s location.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;agent and gem attend to each other (and themselves).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization capacity is tested in two ways:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Performance on levels that require opening a larger sequence of boxes than it is trained on.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Performance on levels that require key-lock combinations not seen during training.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In both the scenarios, the RRL agent significantly outperforms the RL agent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;starcraft&quot;&gt;StarCraft&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RLL agent achieves better or equal results that the RL agent in all but one game.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines. These results are not conclusive given the high variability.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Good-Enough Compositional Data Augmentation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Good-Enough-Compositional-Data-Augmentation"/>
   <updated>2019-05-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Good-Enough Compositional Data Augmentation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Synthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments. This operation is referred to as GECA (Good Enough Compositional Augmentation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.09545&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Discover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same. A lexical environment is the k-word window around each span of the fragment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands. Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of λ calculus or SQL and low resource language modeling. While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given that the proposed approach is relatively simple and straightforward, it appears to be quite promising.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Model-Based Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Multiple-Model-Based-Reinforcement-Learning"/>
   <updated>2019-05-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Multiple Model-Based Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents some general ideas and mechanisms for multiple model-based RL. Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful. As such, I am focusing only on high-level ideas and not the implementation details themselves.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.mitpressjournals.org/doi/abs/10.1162/089976602753712972&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The prediction error from each of the state prediction model defines the “responsibility signal” for each module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This responsibility signal is used to:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Weigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Weigh the parameter update of the environment models as well as the RL controllers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Weighing the action output - ie predicted action is a weighted sum of individual actions.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is amenable for incorporating prior knowledge about which module should be selected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Temporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exploration can be ensured by using a stochastic version of greedy action selection.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One failure mode for such modular architectures is when a single module tries to perform well across all the tasks. The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Towards a natural benchmark for continual learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Towards-a-natural-benchmark-for-continual-learning"/>
   <updated>2019-04-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Towards a natural benchmark for continual learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Continual Learning paradigm focuses on learning from a non-stationary stream of data with additional desiderata - transferring knowledge from previously seen task to unseen tasks and being resilient to catastrophic forgetting - all with a fixed memory and computational budget.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is in contrast to the IID (independent and identically distributed) assumption in statistical learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One common example of the non-iid data is setups involving sequential decision making - eg Reinforcement learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://marcpickett.com/cl2018/CL-2018_paper_48.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Many existing benchmarks use MNIST as the underlying dataset (eg Permuted MNIST, Split MNIST, etc). These benchmarks lack complexity and make it hard to observe positive and negative backward transfer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most works focus only on the catastrophic forgetting challenge and ignore the other issues (like computation and memory footprint, the capacity of the network, etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a new benchmark based on Starcraft II video game to understand the different approaches for lifelong learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequence of tasks is designed to be a curriculum - the learning agent stats with learning simple skills and later move to more complex tasks. These complex tasks require remembering and composing skills learned in the earlier levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To evaluate for catastrophic forgetting, the tasks are designed such that not all the skills are needed for solving each task. Hence the learning agent needs to remember skills even though they are not needed at the current level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each level comes with a fixed computational budget of episodes and each episode has a fixed time limit. Once the budget is consumed the agent has to proceed to the next level. Hence agents with better sample efficiency would benefit.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The benchmark supports both RL and supervised learning version. In the supervised version, expert agents (pretrained on each level) are also provided.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines are provided for distillation (using experts): sequential training (fine tuning), Dropout and SER. None of the baseline methods achieve positive or negative backward transfer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When modeled as a pure RL task, the benchmark is extremely difficult to solve.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper suggests using a metric to record the amount of learning/data required to recover performance on the previous task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Meta-Learning Update Rules for Unsupervised Representation Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Meta-Learning-Update-Rules-for-Unsupervised-Representation-Learning"/>
   <updated>2019-04-02T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Meta-Learning Update Rules for Unsupervised Representation Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Standard unsupervised learning aims to learn transferable features. The paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.00222&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider training the model with supervised learning - &lt;em&gt;φ&lt;sub&gt;t+1&lt;/sub&gt; = SupervisedUpdate(φ&lt;sub&gt;t&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;, y&lt;sub&gt;t&lt;/sub&gt;, θ)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Here &lt;em&gt;t&lt;/em&gt; denotes the step, &lt;em&gt;(x, y)&lt;/em&gt; denotes the data points, &lt;em&gt;θ&lt;/em&gt; denotes the hyperparameters of the optimizer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extending this formulation for meta-learning, one could say that &lt;em&gt;t&lt;/em&gt; is the step of the inner loop, &lt;em&gt;θ&lt;/em&gt; are the parameters of the meta learning model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, the paper proposes to use &lt;em&gt;φ&lt;sub&gt;t+1&lt;/sub&gt; = UnsupervisedUpdate(φ&lt;sub&gt;t&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;, θ)&lt;/em&gt; ie &lt;em&gt;y&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; is not used (or even assumed to be available as this is unsupervised learning).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The meta update rule is used to learn the weights of a meta-model by performing SGD on the sum of &lt;em&gt;MetaObjective&lt;/em&gt; over the distribution of tasks (over the course of inner loop training).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Base model: MLP with parameters &lt;em&gt;φ&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To ensure that it generalizes across architectures, the update rule is designed to be neural-local ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each neuron &lt;em&gt;i&lt;/em&gt; in every layer &lt;em&gt;l&lt;/em&gt; (in the base model) has an update network (MLP) which takes as input the feedforward activations, feedback weights and error signals. ie &lt;em&gt;h&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i) = MLP(x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i), z&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i), v&lt;sup&gt;l+1&lt;/sup&gt;,
δ&lt;sup&gt;l&lt;/sup&gt;(i), θ)&lt;/em&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;b&lt;/em&gt; - index of the minibatch&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;x&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - pre non-linearity activations&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;z&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - post non-linearity activations&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;v&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - feedback weights&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;δ&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; - error signal&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the update networks share the meta parameters &lt;em&gt;θ&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is run in a standard feed-forward manner and the update network (corresponding to each unit) is used to generate the error signal &lt;em&gt;δ&lt;sup&gt;l&lt;/sup&gt;&lt;sub&gt;b&lt;/sub&gt;(i) = lin(h&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;l&lt;/sup&gt;(i))&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This loss is backpropogated using the set of learned backward weights &lt;em&gt;v&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt; instead of the forward weights &lt;em&gt;w&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The weight update &lt;em&gt;Δw&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt; is also generated using a per-neuron update network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;meta-objective&quot;&gt;Meta Objective&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;MetaObjective&lt;/em&gt; is based on fitting a linear regression model to labeled examples with a small number of data points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;MetaObjective&lt;/em&gt; is to reduce the cosine distance between &lt;em&gt;y&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sup&gt;T&lt;/sup&gt;x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;L&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;y&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; - Actual lables on the evaluation batch&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;x&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;L&lt;/sup&gt;&lt;/em&gt; - Features of the evaluation batch (using the base model)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;v&lt;/em&gt; - parameters of the linear regression model (learned on train batch)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practical-considerations&quot;&gt;Practical Considerations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Meta gradients are approximated using truncated backdrop through time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing variation in the training dataset helps the meta optimization process. Data is augmented with shifts, rotations, and noise. Predicting these coefficients is an auxiliary (regression) task for training the meta-objective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training the system requires a lot of resources - 8 days with 512 workers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;With standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving. This suggests that the objective function for the two tasks starts to mismatch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; leads to a better generalization as compared to both VAE and supervised learning (followed by transfer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the meta-objective does not overfit).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;UnsupervisedUpdate&lt;/em&gt; also generalizes to larger model architectures and different activation functions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GNN Explainer - A Tool for Post-hoc Explanation of Graph Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/GNN-Explainer-A-Tool-for-Post-hoc-Explanation-of-Graph-Neural-Networks"/>
   <updated>2019-03-26T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/GNN Explainer - A Tool for Post-hoc Explanation of Graph Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Neural Network (GNN) is a family of powerful machine learning (ML) models for graphs that can combine node information with the structural information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside of GNNs is that their predictions are hard to interpret.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes GNN Explainer model for solving the problem of interpretability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.03894&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desiderata-for-gnn-explanations&quot;&gt;Desiderata for GNN explanations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Local edge fidelity&lt;/strong&gt; - identify the subgraph structure (ideally the smallest) that significantly affected the predictions of the GNN. ie identify the important edges in the graph (for a given prediction).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Local node fidelity&lt;/strong&gt; - identify the import node features and correlations in the features of the neighboring nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Single instance and multi-instance explanations&lt;/strong&gt; - Support both single instance prediction tasks and multi-instance prediction tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt; - Support a large family of models (ideally all)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task Agnostic&lt;/strong&gt; - Support a large family of tasks (ideally all)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;I first describe the single instance prediction case and use that as the base to describe the multiple instance prediction cases. All the discussion in this section assumes a single instance prediction task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Trained GNN, a single instance whose prediction is to be explained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Identify the small subgraph and the small subset of features that explain the prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;: Maximize the mutual information (MI) between the GNN and the explanation by learning a &lt;em&gt;graph mask&lt;/em&gt; which can be used for selecting the relevant subgraph (from the GNN’s computational graph) and features (from all layers of the GNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Computational graph of GNN (corresponding to a node) refers to the approx L-hop neighborhood of the node in the graph ie the subgraph formed by nodes and edges whose representation affected the representation of the given node.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-instance-explanations&quot;&gt;Single-Instance Explanations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For a node &lt;em&gt;v&lt;/em&gt;, the information used to predict its label &lt;em&gt;y&lt;/em&gt; is completely described by its computation graph &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and the associated feature set &lt;em&gt;X&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;. The feature set includes the features of all the nodes in the computation graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When constructing the explaination, only &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and &lt;em&gt;X&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task can be reformulated as identifying a subgraph &lt;em&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; (subset of &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;) with associated features &lt;em&gt;X&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; which are important when predicting the label &lt;em&gt;y&lt;/em&gt; for node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Importance” is measured in terms of MI&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;MI(Y, (G&lt;sub&gt;S&lt;/sub&gt;, X&lt;sub&gt;S&lt;/sub&gt;)) = H(Y) - H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt; where &lt;em&gt;H&lt;/em&gt; is the entropy and &lt;em&gt;Y&lt;/em&gt; is a random variable representing the prediction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A further constraint, &lt;em&gt;| G&lt;sub&gt;S&lt;/sub&gt;| &amp;lt; k&lt;/em&gt; is imposed to obtain consise explaintations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since &lt;em&gt;H(Y)&lt;/em&gt; is fixed (recall that the network has already been trained and is now being used in the inference mode), maximizing MI is equivalent to minimizing the conditional entropy &lt;em&gt;H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is equivalent to selecting the subgraph that minimizes the uncertainty in the prediction of &lt;em&gt;y&lt;/em&gt; when the computational graph is &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optimiation-process&quot;&gt;Optimiation Process&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the exponentially large number of possible subgraphs, we can not directly optimize the given equation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A “relaxed”-adjacency matrix (whose values are real numbers in the range 0 to 1) is introduced where each element of this fractional adjacency matrix is smaller than the corresponding element of the original adjacency matrix. Gradient descent can be performed on this adjacency matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “relaxed” &lt;em&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; can be interpreted as a variational approximation of the subgraph distributions of &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt; and the objective can be written as &lt;em&gt;min E&lt;sub&gt;G&lt;sub&gt;S&lt;/sub&gt;&lt;/sub&gt;H(Y | G = G&lt;sub&gt;S&lt;/sub&gt;, X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the paper makes a big approximation that the GNN is convex so as to leverage the Jensen inequality and push the expectation inside the entropy term to get an upper bound and then minimize that ie &lt;em&gt;min H(Y | G = E&lt;sub&gt;s&lt;/sub&gt;[G&lt;sub&gt;S&lt;/sub&gt;], X = X&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that the convexity approximation (along with discreteness constraint) works in practice.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, mean field approximation is used to decompose &lt;em&gt;P(G&lt;sub&gt;S&lt;/sub&gt;)&lt;/em&gt; as a multivariate Bernoulli distrbitution ie product of &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;(i, j)&lt;/em&gt; for all &lt;em&gt;(i, j)&lt;/em&gt; belonging to &lt;em&gt;G&lt;sub&gt;c&lt;/sub&gt;(v)&lt;/em&gt;. &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; can be optimized directly and its values represent the expectation of the Bernoulli distrbitution on wether the edge &lt;em&gt;e&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; exists.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the constraints on &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt;, it is easier to learn a mask matrix &lt;em&gt;M&lt;/em&gt; and optimize that such that &lt;em&gt;A&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; = M * A&lt;sub&gt;c&lt;/sub&gt;* Additionally, the sigmod operator can be applied on &lt;em&gt;M&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once &lt;em&gt;M&lt;/em&gt; is learned, only the top &lt;em&gt;k&lt;/em&gt; values are retained.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;including-node-features-in-the-explanation&quot;&gt;Including Node Features in the Explanation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Similar to the previous approach, another feature mask is learned (either one for entire GNN or one per node of the GNN) and is used as a feature selector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The mask could either be learned such that same set of node features (in terms of dimensions) are selected or a different set of features are selected per node. The paper uses the former as it is more straightforward.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Just like before, a “relaxed” mask &lt;em&gt;M&lt;sub&gt;T&lt;/sub&gt;&lt;/em&gt; is trained to select features as &lt;em&gt;M&lt;sub&gt;T&lt;/sub&gt; * X&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One tricky case is where one feature is important but its value is set to 0. In the case, the value will be masked even though it should not be&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The workaround is to use Monte Carlo (MC) estimates of marginals of the missing features. This gives a way to assign importance scores to each feature dimension and a form of reparameterization trick is used to perform end-to-end learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Masks are encouraged to be discrete by regularizing their element-wise entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Resulting computation graph is valid as in it allows message passing towards the central node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-instance-explanations&quot;&gt;Multi-Instance Explanations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a set of nodes (having the label say &lt;em&gt;y&lt;/em&gt;),  the task is to obtain a global explanation of the predictions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the given class, a prototypical reference node is chosen by computing the mean of embeddings of all the nodes in the class and then selecting the node which is closest to the mean.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, compute the important computational graph corresponding to this node and align the computational subgraphs of all the other nodes (in the given class) to reference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let &lt;em&gt;A*&lt;/em&gt; be the adjacency matrix and &lt;em&gt;X*&lt;/em&gt; be the feature matrix for the explanation corresponding to the reference node. Let &lt;em&gt;A&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;X&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; be the adjacency matrix and feature matrix of the to-ber-aligned computational graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A relaed alignment matrix &lt;em&gt;P&lt;/em&gt; is optimized to align the nodes and features in the two graphs ie we minimize &lt;em&gt;|P&lt;sup&gt;T&lt;/sup&gt;A&lt;sub&gt;v&lt;/sub&gt;P - A*| + *|P&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;v&lt;/sub&gt;P - X*|&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choosing concise explanations helps in efficient graph matching.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For GNNs that compute attention over the entire graph, edges with low attention weight can be pruned to increase efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Node classification: BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Graph classification: MUTAG, Reddit-Binary&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;GRAD - Compute the gradient of the model loss with respect to the adjacency matrix and the node features to be classified and fix the edges with the highest absolute gradient.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;GAT - Graph Attention Network&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed model seems to outperform the baselines both qualitatively and quantitatively. But the results should be taken with a grain of salt as only 2 baselines are considered.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/To-Tune-or-Not-to-Tune-Adapting-Pretrained-Representations-to-Diverse-Tasks"/>
   <updated>2019-03-16T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.05987&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides useful empirical advice for adapting pretrained language models for a given target task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pre-trained models considered&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ELMo&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;BERT&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tasks considered&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Named Entity Recognition (NER) - CoNLL 2003 dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Natural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Paraphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Semantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The last 3 tasks (NLI, PD, STS) are defined for sentence pairs.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adaptation Strategies&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Feature Extraction&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The pretrained model is only used for extracting features and its weights are kept fixed.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For both ELMo and BERT, the contextual representation of the words from all the layers are extracted.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;A weighted combination of these layers is used as an input to the task-specific model.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - BiLSTM with CRF layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA - bi-attentive classification network&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;NLI, PD, STS - &lt;a href=&quot;https://arxiv.org/abs/1609.06038&quot;&gt;Enhanced Sequential Inference Model (ESIM)&lt;/a&gt;&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Fine-tuning&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The pretrained model is finetuned on the target task.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models for ELMO&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - CRF on top of LSTM states&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA - Max-pool over the language model states followed by a softmax layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;NLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer.&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Task-specific models for BERT&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;
                &lt;p&gt;NER - Extract representation of the first-word piece of each token followed by the softmax layer&lt;/p&gt;
              &lt;/li&gt;
              &lt;li&gt;
                &lt;p&gt;SA, NLI, PD, STS - standard BERT training&lt;/p&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main observations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Feature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS). The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Adding extra parameters is helpful for feature extraction but not fine-tuning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Training a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Knowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Model Primitive Hierarchical Lifelong Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Model-Primitive-Hierarchical-Lifelong-Reinforcement-Learning"/>
   <updated>2019-03-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Model Primitive Hierarchical Lifelong Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.01567&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of learning a single transition model of the environment (aka &lt;em&gt;world model&lt;/em&gt;) that can model the transitions very well, it is sufficient to learn several (say &lt;em&gt;k&lt;/em&gt;) suboptimal models (aka &lt;em&gt;model primitives&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each &lt;em&gt;model primitive&lt;/em&gt; will be good in only a small part of the state space (aka &lt;em&gt;region of specialization&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These &lt;em&gt;model primitives&lt;/em&gt; can then be used to train a gating mechanism for selecting sub-policies to solve a given task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since these &lt;em&gt;model primitives&lt;/em&gt; are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;single-task-learning&quot;&gt;Single Task Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A gating controller is trained to choose the sub-policy whose &lt;em&gt;model primitive&lt;/em&gt; makes the best prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This requires modeling &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt; | s&lt;sub&gt;t&lt;/sub&gt;, a&lt;sub&gt;t&lt;/sub&gt;, s&lt;sub&gt;t+1&lt;/sub&gt;)&lt;/em&gt; where &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt;)&lt;/em&gt; denotes the probability of selecting the &lt;em&gt;k&lt;sup&gt;th&lt;/sup&gt; model primitive&lt;/em&gt;. This is hard to compute as the system does not have access to &lt;em&gt;s&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt;  and &lt;em&gt;a&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; before it has choosen the sub-policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Properly marginalizing &lt;em&gt;s&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;a&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; would require expensive MC sampling. Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce &lt;em&gt;p(M&lt;sub&gt;k&lt;/sub&gt; | s&lt;sub&gt;t&lt;/sub&gt;)&lt;/em&gt;. This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper notes that technique is biased but reports that it still works for the downstream tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The gating controller composes the sub-policies as a mixture of Gaussians.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For learning, PPO algorithm is used with each &lt;em&gt;model primitives&lt;/em&gt; gradient weighted by the probability from the gating controller.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lifelong-learning&quot;&gt;Lifelong Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Different tasks could share common subtasks but may require a different composition of subtasks. Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Domains:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Mujoco ant navigating different mazes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Stacker arm picking up and placing different boxes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation Details:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Gaussian subpolicies&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;PPO as the baseline&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Model primitives are hand-crafted using the true next state provided by the environment simulator.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Single Task&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Only maze task is considered with the start position (of the ant) and the goal position is fixed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Observation includes distance from the goal.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lifelong Learning&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Maze&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;10 random Mujoco ant mazes used as the task distribution.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pick and Place&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;8 Pick and Place tasks are created with max 3 goal locations.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Observation includes the position of the goal.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablations&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Overlapping &lt;em&gt;model primitives&lt;/em&gt; can degrade the performance (to some extent). Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The order of tasks (in the 10-Mazz task) does not degrage the performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Transfering the gating controller leads to negative transfer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Notes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!) through the experimental results are not conclusive given the limited number of domains on which the approach is tested.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>TuckER - Tensor Factorization for Knowledge Graph Completion</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/TuckER-Tensor-Factorization-for-Knowledge-Graph-Completion"/>
   <updated>2019-02-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/TuckER-Tensor Factorization for Knowledge Graph Completion</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.09590&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ibalazevic/TuckER&quot;&gt;Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;knowledge-graph-as-a-tensor&quot;&gt;Knowledge Graph as a Tensor&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (e&lt;sub&gt;s&lt;/sub&gt;, r, e&lt;sub&gt;o&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element’s value corresponds to ether that element is present in the KG or not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple. The score indicates whether the triple is actually present in the KG or not.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tucker-decomposition&quot;&gt;TuckER Decomposition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor &lt;strong&gt;X&lt;/strong&gt; (of shape &lt;em&gt;IxJxK&lt;/em&gt;) can be factorized into a core tensor &lt;strong&gt;W&lt;/strong&gt; (of shape &lt;em&gt;PxQxR&lt;/em&gt;) and 3 factor matrics - &lt;strong&gt;A&lt;/strong&gt; (of shape &lt;em&gt;IxP&lt;/em&gt;), &lt;strong&gt;B&lt;/strong&gt; (of shape &lt;em&gt;JxQ&lt;/em&gt;) and &lt;strong&gt;C&lt;/strong&gt; (of shape &lt;em&gt;KxR&lt;/em&gt;) such that &lt;strong&gt;X&lt;/strong&gt; is approximately &lt;strong&gt;W&lt;/strong&gt; x&lt;sub&gt;1&lt;/sub&gt; &lt;strong&gt;A&lt;/strong&gt; x&lt;sub&gt;2&lt;/sub&gt; &lt;strong&gt;B&lt;/strong&gt; x&lt;sub&gt;3&lt;/sub&gt; &lt;strong&gt;C&lt;/strong&gt;, where X&lt;sub&gt;n&lt;/sub&gt; denotes the tensor product along the nth mode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally, &lt;em&gt;P, Q, R&lt;/em&gt; are smaller than &lt;em&gt;I, J K&lt;/em&gt; (respectively) and &lt;strong&gt;W&lt;/strong&gt; can be seen as a compressed version of &lt;strong&gt;X&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tucker-decomposition-for-link-prediction&quot;&gt;TuckER Decomposition for Link Prediction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two embedding matrics are used for embedding the entities and the relations respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entity embedding matrix &lt;strong&gt;E&lt;/strong&gt; is shared for both subject and the object ie &lt;strong&gt;E&lt;/strong&gt; = &lt;strong&gt;A&lt;/strong&gt; = &lt;strong&gt;B&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The scoring function is gives as &lt;strong&gt;W&lt;/strong&gt; x&lt;sub&gt;1&lt;/sub&gt; &lt;strong&gt;e&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt; x&lt;sub&gt;2&lt;/sub&gt; &lt;strong&gt;w&lt;sub&gt;r&lt;/sub&gt;&lt;/strong&gt; x&lt;sub&gt;3&lt;/sub&gt; &lt;strong&gt;e&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt; where &lt;strong&gt;e&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt;, &lt;strong&gt;w&lt;sub&gt;r&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;e&lt;sub&gt;o&lt;/sub&gt;&lt;/strong&gt; are the embedding vectors corresonding to e&lt;sub&gt;s&lt;/sub&gt;, e&lt;sub&gt;r&lt;/sub&gt; and e&lt;sub&gt;o&lt;/sub&gt; respectively. Note that both the core tensor and the factor matrices are to be learnt.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To speed up training and increase accuracy, 1-N scoring is used. A given (e&lt;sub&gt;s&lt;/sub&gt;, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-analysis&quot;&gt;Theoretical Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For full expressiveness, dimensionality of entity (relation) is n&lt;sub&gt;E&lt;/sub&gt; (n&lt;sub&gt;R&lt;/sub&gt;) where n&lt;sub&gt;E&lt;/sub&gt; (n&lt;sub&gt;R&lt;/sub&gt;) are the number of entities (relations). In comparsion, the required dimensionality for ComplEx is n&lt;sub&gt;E&lt;/sub&gt; * n&lt;sub&gt;R&lt;/sub&gt; (for both entity and relations) and for SimplE, it is min(&lt;sub&gt;E&lt;/sub&gt; * n&lt;sub&gt;R&lt;/sub&gt;, number of facts + 1) (for both entity and relations).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;FB15k, FB15k-237, WN18, WN18RR&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The max number of entities is around 41K and max number of relations is around 1.3K&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;BatchNorm, Dropout and Learning rate decay are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall n&lt;sub&gt;e&lt;/sub&gt; generated triples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the n&lt;sub&gt;e&lt;/sub&gt; generated triples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Higher is better for both the metrics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER outperforms all the baseline models on all but one task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TuckER improves performance more significantly when the number of relations is large.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Even with lower embedding dimensions, TuckER’s performance does not deteriorate as much as other models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Linguistic Knowledge as Memory for Recurrent Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Linguistic-Knowledge-as-Memory-for-Recurrent-Neural-Networks"/>
   <updated>2019-02-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Linguistic Knowledge as Memory for Recurrent Neural Networks</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.02620&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture. Its GRU version is referred to as MAGE-GRU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input sequence of tokens [x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, …, x&lt;sub&gt;T&lt;/sub&gt;] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the graph &lt;em&gt;G&lt;/em&gt;, two DFS orderings are computed - forward DFS and backward DFS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAGE-RNN uses separate networks for accessing the forward and backward DFS orders.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A separate hidden state is maintained for each entity type to separate memory content from addressing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For any DFS order (forward or backward), the representation at time &lt;em&gt;t&lt;/em&gt; is given as the concatenation of representation of different edge types at that time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at x&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The representation of the DFS order is given as the sequence of all the previous representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In some cases, elements across multiple sequences could be related to each other. In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all the cases, the model with MAGE-GRU works the best.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Diversity is All You Need - Learning Skills without a Reward Function</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Diversity-is-All-You-Need-Learning-Skills-without-a-Reward-Function"/>
   <updated>2019-01-29T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Diversity is All You Need - Learning Skills without a Reward Function</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.06070&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ben-eysenbach/sac&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Unsupervised “exploration” stage followed by supervised stage.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desirable-qualities-of-skills&quot;&gt;Desirable Qualities of Skills&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Skills should dictate the states that the agent visits. Different skills should visit different states to be distinguishable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skills are encouraged to be diverse and “exploratory” by learning skills that act randomly (have high entropy).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss-formulation&quot;&gt;Loss Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(S, A) - state and action&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;z ~ p(z) - latent variable to condition the policy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Skill - policy conditioned on a fixed z.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximize the entropy of the mixture of policies (p(z) and all the skills).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Policy π(a | s, z)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task reward replaced by the pseduoreward logq&lt;sub&gt;φ&lt;/sub&gt;(z | s) - log(p(z)).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During unsupervised training, z is sampled at the start of the episode and then not changed during the episode.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;h3 id=&quot;analysis-of-learned-skills&quot;&gt;Analysis of Learned Skills&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for inverted pendulum and mountain car, the skills become increasingly diverse throughout training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies. This provides a mechanism for incorporating “supervision” in the learning setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “discovered” primitives can also be used for imitation learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Modular meta-learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Modular-meta-learning"/>
   <updated>2019-01-22T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Modular meta-learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed model is called as BOUNCEGRAD.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.10166&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/FerranAlet/modular-metalearning&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Focuses on supervised learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task distribution &lt;em&gt;p(T)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each task is a joint distribution &lt;em&gt;p&lt;sub&gt;T&lt;/sub&gt;(x, y)&lt;/em&gt; over &lt;em&gt;(x, y)&lt;/em&gt; data pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given data from &lt;em&gt;m&lt;/em&gt; meta-training tasks, and a meta-test task, find a hypothesis &lt;em&gt;h&lt;/em&gt; which performs well on the unseen data drawn from the meta-test task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;structured-hypothesis&quot;&gt;Structured Hypothesis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a compositional scheme &lt;em&gt;C&lt;/em&gt;, a set of modules &lt;em&gt;F&lt;sub&gt;1&lt;/sub&gt;, …, F&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt; (represented as a whole by &lt;em&gt;F&lt;/em&gt;) and the set of their respective parameters θ&lt;sub&gt;1&lt;/sub&gt;, …, θ&lt;sub&gt;k&lt;/sub&gt; (represented as a whole by θ), &lt;em&gt;(C, F, θ)&lt;/em&gt; represents the set of possible functional input-output mappings. These mappings form the hypothesis space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A structured hypothesis model is specified by what modules to use and their parametric forms (but not the values).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;examples-of-compositional-schemes&quot;&gt;Examples of compositional schemes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Choosing a single module for the task at hand.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fixed compositional structure but different modules selected every time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Weight ensemble (maybe using attention mechanism)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;General function composition tree&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phases&quot;&gt;Phases&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Offline Meta Learning Phase:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Take training and validation dataset for the first &lt;em&gt;k&lt;/em&gt; tasks and generate a parameterization for each module &lt;em&gt;θ&lt;sub&gt;1&lt;/sub&gt;, …, θ&lt;sub&gt;k&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The hypothesis (or composition) to use comes from the online meta-test learning phase.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this stage, find the best θ given a structure.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Online Meta-test Learning Phase&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given a hypothesis space and θ, the output is a compositional form (or hypothesis) that specifies how to compose the models.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In this stage, find the best structure, given a hypothesis space and θ.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-algorithm&quot;&gt;Learning Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature &lt;em&gt;T&lt;/em&gt; decreased over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once a structure has been picked, any gradient descent based approach can be used to optimize the modules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically the state of optimization process comprises of the parameters and the temperature. Together, they are used to induce a distribution over the structures. Given a structure, θ is optimized and &lt;em&gt;T&lt;/em&gt; is annealed over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well. the resulting approach is referred to as MOMA - MOdular MAml.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;approaches&quot;&gt;Approaches&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pooled - Single network using combined data of all the tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAML - Single network using MAML&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BOUNCEGRAD - Modular Network without MAML adaptation in online learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MOMA - BOUNCEGRAD with MAML adaptation in online learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;domains&quot;&gt;Domains&lt;/h3&gt;

&lt;h4 id=&quot;simple-functional-relationships&quot;&gt;Simple Functional Relationships&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sine-function prediction problem&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, MOMA outperforms other models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;predicting-next-frame-of-a-kinematic-skeleton-motion-capture-data&quot;&gt;Predicting next frame of a kinematic skeleton (motion capture data)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;11 different objects (with different shapes) on 4 surfaces with different friction properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2 meta-learning scenarios are considered. In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For unseen combinations, all the 3 are equally good.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compositional scheme is the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;predicting-next-frame-of-a-kinematic-selection-using-motion-capture-data&quot;&gt;Predicting next frame of a kinematic selection (using motion capture data)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Composition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Again 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;While the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hierarchical-RL-Using-an-Ensemble-of-Proprioceptive-Periodic-Policies"/>
   <updated>2019-01-15T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=SJz1x20cFQ&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The state can be divided into two components: proprioceptive states s&lt;sup&gt;p&lt;/sup&gt; (measurement of agent’s own body that can be directly controlled by the agent) and the external states s&lt;sup&gt;e&lt;/sup&gt;/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;low-level-policy-training&quot;&gt;Low-Level Policy Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low-level policies should be:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Diverse: should cover all the skills that the agent might have to perform.&lt;/li&gt;
      &lt;li&gt;Effective: can make significant changes to the environment.&lt;/li&gt;
      &lt;li&gt;Controllable: easy for high-level policies to use and control&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the low-level policy, the per-time step reward is directly proportional to change in the external state. The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;phase-conditioned-policies&quot;&gt;Phase conditioned policies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Good movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Phase conditioned policy (=f(s&lt;sup&gt;p&lt;/sup&gt;, φ)) where φ = {0, 1, …, k-1} is the phase index.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At each timestep &lt;em&gt;t&lt;/em&gt;, the model receives observation s&lt;sup&gt;p&lt;/sup&gt; and phase index φ = t%k. The phase index is represented by a vector b&lt;sub&gt;φ&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environments: Ant and Humanoid from Mujoco.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low-level control:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Using phase-conditioning is helpful when training low-level primitives.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;High-level control:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Cross Maze Environment with fixed goals&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;3 goals along 3 paths&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Proposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds).&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Random Goal Maze&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The goal is randomly drawn from a set of goals.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;“Cross” (shaped) maze and “skull” (shaped) mazes are considered.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Even with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Efficient Lifelong Learning with A-GEM</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Efficient-Lifelong-Learning-with-A-GEM"/>
   <updated>2019-01-08T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Efficient Lifelong Learning with A-GEM</id>
   <content type="html">&lt;h2 id=&quot;contributions&quot;&gt;Contributions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modification of &lt;a href=&quot;https://arxiv.org/abs/1706.08840&quot;&gt;Gradient Episodic Memory ie GEM&lt;/a&gt; which reduces the computational overhead of GEM without compromising on the results.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.00420&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/agem/&quot;&gt;Link to the code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-protocol&quot;&gt;Learning Protocol&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two group of datasets - one for training and evaluation (D&lt;sup&gt;EV&lt;/sup&gt;) and other for cross validation (D&lt;sup&gt;CV&lt;/sup&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data can be sampled multiple times for cross-validation dataset but only once from the training dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each group of dataset (say D&lt;sup&gt;EV&lt;/sup&gt; or D&lt;sup&gt;CV&lt;/sup&gt;) is a list of task-specific datasets D&lt;sub&gt;k&lt;/sub&gt; (k is the task index).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each sample in D&lt;sub&gt;k&lt;/sub&gt; is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D&lt;sub&gt;k&lt;/sub&gt; contains B&lt;sup&gt;k&lt;/sup&gt; minibatches of data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metrics&quot;&gt;Metrics&lt;/h2&gt;

&lt;h3 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a&lt;sub&gt;k,i,j&lt;/sub&gt; = accuracy on test task j after training on ith minibatch of training task k.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A&lt;sub&gt;k&lt;/sub&gt; = mean over all j = 1 to k (a&lt;sub&gt;k, B&lt;sub&gt;k&lt;/sub&gt;, j&lt;/sub&gt;) ie train the model on data for task k and then test it on all the tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forgetting-measure&quot;&gt;Forgetting Measure&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt; = forgetting on task j after training on all minibatches upto task k.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt; = max over all l = 1 to k-1 (a&lt;sub&gt;l, B&lt;sub&gt;l&lt;/sub&gt;j&lt;/sub&gt; - a&lt;sub&gt;k, B&lt;sub&gt;k&lt;/sub&gt;j&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forgetting = F&lt;sub&gt;k&lt;/sub&gt; = mean over all j = 1 to k-1 (f&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt;)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lca---learning-curve-area&quot;&gt;LCA - Learning Curve Area&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Z&lt;sub&gt;b&lt;/sub&gt; = average b shot performance where b is the minibatch number.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z&lt;sub&gt;b&lt;/sub&gt; = mean over all k = 0 to T (a&lt;sub&gt;k, b, k&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LCA&lt;sub&gt;β&lt;/sub&gt; = mean over all b = 0 to β (Z&lt;sub&gt;b&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One special case is LCA&lt;sub&gt;0&lt;/sub&gt; which is the forward transfer performance or performance on the unseen task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In experiments, β is kept small as we want the model to learn from few examples.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;joint-embedding-model-using-compositional-task-descriptors&quot;&gt;Joint Embedding Model Using Compositional Task Descriptors&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compositional Task Descriptors are used to speed training on the subsequent tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A matrix specifying the attribute value of objects (to be recognized in the task) are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A joint-embedding space between image features and attribute embeddings is learned.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00796&quot;&gt;Permuted MNIST&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04200&quot;&gt;Split CIFAR&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.vision.caltech.edu/visipedia/CUB-200-2011.html&quot;&gt;Split CUB&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cvml.ist.ac.at/papers/lampert-cvpr2009.pdf&quot;&gt;Split AWA&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines include &lt;a href=&quot;https://arxiv.org/abs/1706.08840&quot;&gt;GEM&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1611.07725&quot;&gt;iCaRL&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1612.00796.pdf&quot;&gt;Elastic Weight Consolidation&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1606.04671&quot;&gt;Progressive Neural Networks&lt;/a&gt; etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead. One reason could be that MNIST has a large number of training examples per task. But Progressive Neural Networks lead to bad utilization of capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use of task descriptors improves the accuracy for all the models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Pre-training Graph Neural Networks with Kernels</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Pre-training-Graph-Neural-Networks-with-Kernels"/>
   <updated>2019-01-02T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Pre-training Graph Neural Networks with Kernels</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a pretraining technique that can be used with the &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry&quot;&gt;GNN&lt;/a&gt; architecture for learning graph representation as induced by powerful graph kernels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.06930&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a dataset of graphs &lt;em&gt;g&lt;sub&gt;1&lt;/sub&gt;, g&lt;sub&gt;2&lt;/sub&gt;, …, g&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;, use a relevant kernel function to compute &lt;em&gt;k(g&lt;sub&gt;i&lt;/sub&gt;, g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; for all pairs of graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A siamese network is used to encode the pair of graphs into representations &lt;em&gt;f(g&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; and &lt;em&gt;f(g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; such that &lt;em&gt;dot(f(g&lt;sub&gt;i&lt;/sub&gt;), f(g&lt;sub&gt;j&lt;/sub&gt;))&lt;/em&gt; equals &lt;em&gt;k(g&lt;sub&gt;i&lt;/sub&gt;, g&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The function &lt;em&gt;f&lt;/em&gt; is trained to learn the compressed representation of kernel’s feature space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf&quot;&gt;DGCNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graphlet Kernel (GK)&lt;/li&gt;
  &lt;li&gt;Random Walk Kernel&lt;/li&gt;
  &lt;li&gt;Propogation Kernel&lt;/li&gt;
  &lt;li&gt;Weisfeiler-Lehman subtree kernel (WL)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pretraining uses the WL kernel&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The idea is straightforward and intuitive. In general, this kind of pretraining should help the downstream model. It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Smooth Loss Functions for Deep Top-k Classification</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Smooth-Loss-Functions-for-Deep-Top-k-Classification"/>
   <updated>2018-12-25T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Smooth Loss Functions for Deep Top-k Classification</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.07595&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/oval-group/smooth-topk&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Inspired by the multi-loss SVMs, a surrogate loss (l&lt;sub&gt;k&lt;/sub&gt;) is introduced that creates a margin between the ground truth and the kth largest score.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq1.png&quot; alt=&quot;Equation 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Here &lt;strong&gt;s&lt;/strong&gt; denotes the output of the classifier model to be learnt, &lt;em&gt;y&lt;/em&gt; is the ground truth label, &lt;em&gt;s[p]&lt;/em&gt; denotes the kth largest element of &lt;strong&gt;s&lt;/strong&gt; and &lt;strong&gt;s\p&lt;/strong&gt; denotes the vector &lt;strong&gt;s&lt;/strong&gt; without &lt;em&gt;p&lt;/em&gt;th element.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This l&lt;sub&gt;k&lt;/sub&gt; loss has two limitations:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;It is continous but not differentiable in &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Its weak derivatives have at most 2-nonzero elements.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The loss can be reformulated by adding and subtracting the k-1 largest scores of &lt;strong&gt;s\y&lt;/strong&gt; and &lt;em&gt;s&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt; and by introducing a temperature parameter τ.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/shagunsodhani/papers-I-read/raw/master/assets/topk/eq2.png&quot; alt=&quot;Equation 2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;properties-of-lkτ&quot;&gt;Properties of L&lt;sub&gt;kτ&lt;/sub&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For any τ &amp;gt; 0, L&lt;sub&gt;kτ&lt;/sub&gt; is infinite-differentiable and has non-sparse gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Under mild conditions, L&lt;sub&gt;kτ&lt;/sub&gt; apporachs l&lt;sub&gt;k&lt;/sub&gt; (in a pointwise sense) as τ approaches to 0+&lt;sup&gt;+&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is an upper bound on the actual loss (up to a constant factor).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is a generalization of the cross-entropy loss for different values of k, and τ and higher margins.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;computational-challenges&quot;&gt;Computational Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;nCk&lt;/em&gt; number of terms needs to be evaluated for computing the loss for one sample (n is number of classes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss L&lt;sub&gt;kτ&lt;/sub&gt; can be expressed in terms of elementary symmetric polynomials σ&lt;sub&gt;i&lt;/sub&gt;(&lt;strong&gt;e&lt;/strong&gt;) (sum of all products of i distinct elements of vector e). Thus the challenge is to compute σ&lt;sub&gt;k&lt;/sub&gt; efficiently.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forward-computation&quot;&gt;Forward Computation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute σ&lt;sub&gt;k&lt;/sub&gt;(&lt;strong&gt;e&lt;/strong&gt;) where &lt;strong&gt;e&lt;/strong&gt; is a n-dimensional vector and k« n and e[i]!=0 for all i.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;σ&lt;sub&gt;i&lt;/sub&gt;(&lt;em&gt;e&lt;/em&gt;) can be computed using the coefficients of the polynomial (X+e&lt;sub&gt;1&lt;/sub&gt;)(X+e&lt;sub&gt;2&lt;/sub&gt;)…(X+e&lt;sub&gt;n&lt;/sub&gt;) by divide and conquer approach with polynomial multiplication.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backward-computation&quot;&gt;Backward computation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The backward pass uses optimizations like computing derivative of σ&lt;sub&gt;j&lt;/sub&gt; with respect to e&lt;sub&gt;i&lt;/sub&gt; in a recursive manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Appendix of the paper describes these techniques in detail.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed on CIFAR-100 (with noise) and Imagenet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hindsight Experience Replay</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hindsight-Experience-Replay"/>
   <updated>2018-12-18T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hindsight Experience Replay</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.01495&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Assume a footballer misses the goal narrowly. Even though the player does not get any “reward”(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The same intuition is applied for the RL agent - let us say that the true goal state was &lt;em&gt;g&lt;/em&gt; while the agent ends up in the state &lt;em&gt;s&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the action sequence is not useful for reaching the goal state &lt;em&gt;g&lt;/em&gt;, it is indeed useful for reaching state &lt;em&gt;s&lt;/em&gt;. Hence the trajectory could be replayed with the goal as &lt;em&gt;s&lt;/em&gt;(and not &lt;em&gt;g&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;technical-details&quot;&gt;Technical Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-goal policy trained using Universal Value Function Approximation (UVFA).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every episode starts by sampling a start state and a goal state. Each goal has a different reward function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy uses both the current state and the current goal state and leads to a state transition sequence &lt;em&gt;s&lt;sub&gt;1&lt;/sub&gt;, s&lt;sub&gt;2&lt;/sub&gt;,…, s&lt;sub&gt;n&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each of these transitions &lt;em&gt;s&lt;sub&gt;i&lt;/sub&gt; -&amp;gt; s&lt;sub&gt;i+1&lt;/sub&gt;&lt;/em&gt; are stored in a buffer with both the original goal and a subset of the other goals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the goal selection, following strategies are tried:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Future&lt;/em&gt; - goal state is the state &lt;em&gt;k&lt;/em&gt; steps after observing the state transition.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Final&lt;/em&gt; - goal state is the final state of the current episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Episode&lt;/em&gt; - &lt;em&gt;k&lt;/em&gt; random states are selected from the current episode.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Randon&lt;/em&gt; - &lt;em&gt;k&lt;/em&gt; states are selected randomly.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Any off-policy algorithm can be used. Specifically, DDPG is used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Robotic arm simulated using MuJoCo for &lt;em&gt;push&lt;/em&gt;, &lt;em&gt;slide&lt;/em&gt; and &lt;em&gt;pick and place&lt;/em&gt; tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DDPG with and without HER evaluated on the 3 tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DDPG with the HER variant significantly outperforms the baseline in all the cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Representation Tradeoffs for Hyperbolic Embeddings</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Representation-Tradeoffs-for-Hyperbolic-Embeddings"/>
   <updated>2018-12-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Representation Tradeoffs for Hyperbolic Embeddings</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.03329&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic Spaces&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Have the “tree” like property ie the shortest path between a pair of points is almost the same as the path through the origin.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fidelity Measures&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Mean Average Precision - MAP&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;A local metric that ranks between distances of the immediate neighbors.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Distortion&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;A global metric that depends on the underlying distances and not just the local relationship between distances.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;combinatorial-construction-for-embedding-hierarchies-into-hyperbolic-spaces&quot;&gt;Combinatorial Construction for embedding hierarchies into Hyperbolic spaces&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Embed the given graph &lt;em&gt;G = (V, E)&lt;/em&gt; into a tree &lt;em&gt;T&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Embed the tree &lt;em&gt;T&lt;/em&gt; into the poincare ball &lt;em&gt;H&lt;sub&gt;d&lt;/sub&gt;&lt;/em&gt; of dimensionality &lt;em&gt;d&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sarkars-construction-to-embed-points-in-a-2-d-poincare-ball&quot;&gt;Sarkar’s construction to embed points in a 2-d Poincare ball&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider two points &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; (from the tree) where &lt;em&gt;b&lt;/em&gt; is the parent of &lt;em&gt;a&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assume that &lt;em&gt;a&lt;/em&gt; is embedded as &lt;em&gt;f(a)&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; is embedded as &lt;em&gt;f(b)&lt;/em&gt; and the children of &lt;em&gt;a&lt;/em&gt; needs to be embedded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reflect &lt;em&gt;f(a)&lt;/em&gt; and &lt;em&gt;f(b)&lt;/em&gt; across a geodesic such that &lt;em&gt;f(a)&lt;/em&gt; is mapped to 0 (origin) while &lt;em&gt;f(b)&lt;/em&gt; is mapped to some new point &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Children of &lt;em&gt;a&lt;/em&gt; are placed at points &lt;em&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; which are equally placed around a circle of radius &lt;em&gt;(e&lt;sup&gt;r&lt;/sup&gt; - 1) / (e&lt;sup&gt;r&lt;/sup&gt; + 1)&lt;/em&gt; and maximally seperated from &lt;em&gt;z&lt;/em&gt;, where &lt;em&gt;r&lt;/em&gt; is the scaling factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then all the points are reflected back across the geodesic so that all children are at a distance &lt;em&gt;r&lt;/em&gt; from &lt;em&gt;f(a)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;d-dimensional-hyperbolic-spaces&quot;&gt;&lt;em&gt;d&lt;/em&gt;-dimensional hyperbolic spaces&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the &lt;em&gt;d&lt;/em&gt;-dimensional space, the points are embedded into hyperspheres (instead of circles).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The number of children node that can be placed for a particular angle grows with the dimension.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing dimension helps with bushy trees (with high node degree).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperbolic-multidimensional-scaling-h-mds&quot;&gt;Hyperbolic multidimensional scaling (h-MDS)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the pairwise distance from a set of points in the hyperbolic space, how to recover the points?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The corresponding problem in the Euclidean space is solved using MDS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A variant of MDS called as h-MDS is proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MDS makes a centering assumption that points have 0 mean. In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pseudo-euclidean-mean&quot;&gt;pseudo-Euclidean Mean&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recovery-via-matrix-factorization&quot;&gt;Recovery via matrix factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the pairwise distances, a new matrix &lt;em&gt;Y&lt;/em&gt; is constructed by applying &lt;em&gt;cosh&lt;/em&gt; on the pairwise distances.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Running PCA on &lt;em&gt;-Y&lt;/em&gt; recovers X up to rotation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dimensionality-reduction-with-pga-principal-geodesic-analysis&quot;&gt;Dimensionality Reduction with PGA (Principal Geodesic Analysis)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PGA is the counterpart of PCA in the hyperbolic spaces.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First the &lt;em&gt;Karcher&lt;/em&gt; mean of the given points is computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All points &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; are reflected so that their mean is 0 in the Poincare disk model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Combining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Trees: fully balanced and phylogenic trees expressing genetic heritage.&lt;/li&gt;
      &lt;li&gt;Tree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships.&lt;/li&gt;
      &lt;li&gt;No-tree like disease relationships, proteins interactions etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Combinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion.&lt;/li&gt;
      &lt;li&gt;eg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Learned Optimizers that Scale and Generalize</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learned-Optimizers-that-Scale-and-Generalize"/>
   <updated>2018-11-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learned Optimizers that Scale and Generalize</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04813&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-advantage&quot;&gt;Key Advantage&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed. The learnt optimizer generalizes to much more complex tasks and setups.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A hierarchical RNN is designed to act as a learned optimizer. This RNN is the meta-learner and its parameters are shared across different tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the lowest level of hierarchical, a small “parameter RNN” ingests the gradient (and related metadata).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One level up, an intermediate “Tensor RNN” incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, the hidden states are fixed at 10, 30 and 20 respectively.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;features-inspired-from-existing-optimizers&quot;&gt;Features inspired from existing optimizers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Attention and Nesterov’s momentum&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Attention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;To incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The average gradients are rescaled (as in RMSProp and Adam)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Relative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>One-shot Learning with Memory-Augmented Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/One-shot-Learning-with-Memory-Augmented-Neural-Networks"/>
   <updated>2018-10-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/One-shot Learning with Memory-Augmented Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Here, MANN refers to neural networks that have an external memory. This includes Neural Turning Machines (NTMs) and excludes LSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.06065&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;meta-learning&quot;&gt;Meta-Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In meta-learning, a learner is learning at two levels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learner is shown a sequence of tasks D&lt;sub&gt;1&lt;/sub&gt;, D&lt;sub&gt;2&lt;/sub&gt;, …, D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When it is training on one of the datasets (say D&lt;sub&gt;T&lt;/sub&gt;), it learns to solve the current dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mann--meta-learning&quot;&gt;MANN + Meta Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Following are the desirable characteristics for a scalable, combined architecture:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Memory representation should be both stable and element-wise accessible.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Number of model parameters should not be tied to the size of the memory.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task-setup&quot;&gt;Task Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each dataset is presented to the model in the form (x&lt;sub&gt;1&lt;/sub&gt;, null), (x&lt;sub&gt;1&lt;/sub&gt;, y&lt;sub&gt;0&lt;/sub&gt;), …, (x&lt;sub&gt;t+1&lt;/sub&gt;, y&lt;sub&gt;t&lt;/sub&gt;) where y&lt;sub&gt;t&lt;/sub&gt; is the correct label (or value) corresponding to the inpuit x&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, the data labels are shuffled from dataset to dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses NTM as the MANN with one modification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the original formulation, the memories were addressed by both context and location. Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new access module - LRUA - Least Recent Used Access - is used to write to memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information). This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Omniglot (classification)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sampled functions from Gaussian Processes&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As baselines, following models were considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Regular NTM&lt;/li&gt;
      &lt;li&gt;LSTM&lt;/li&gt;
      &lt;li&gt;Feedforward RNN&lt;/li&gt;
      &lt;li&gt;Nearest Neighbour Classifier&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>BabyAI - First Steps Towards Grounded Language Learning With a Human In the Loop</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/BabyAI-First-Steps-Towards-Grounded-Language-Learning-With-a-Human-In-the-Loop"/>
   <updated>2018-10-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/BabyAI-First Steps Towards Grounded Language Learning With a Human In the Loop</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.08272&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Curriculum learning is incorporated by having a curriculum of levels of increasing difficulty.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interactive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside of human-in-the-loop is the poor sample complexity of the learning agent. The heuristic agent can be used to estimate the sample  efficiency.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contribution&quot;&gt;Contribution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;BabyAI research platform for grounded language learning with a simulated human-in-the-loop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baseline results for performance and sample efficiency for the different tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;babyai-platform&quot;&gt;BabyAI Platform&lt;/h2&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MiniGrid - A partially observable 2D grid-world environment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entities - Agent, ball, box, door, keys&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actions - pick, drop or move objects, unlock doors etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baby-language&quot;&gt;Baby Language&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Synthetic Language (a proper subset of English) - Used to give instructions to the agent&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Support for verifying if the task (and the subtasks) are completed or not&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;levels&quot;&gt;Levels&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A level is an instruction-following task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Formally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motivated by curriculum learning, the authors create a series of tasks (with increasing difficulty).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A subset of skills (competencies) is required for solving each task. The platform takes into account this constraint when creating a level.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heuristic-expert&quot;&gt;Heuristic Expert&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For any level, it can suggest actions or generate demonstrations (given the state of the environment).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An imitation learning baseline is trained for each level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GRU to encode the sentence, CNN to encode the input observation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FiLM layer to combine the two representations&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM to encode the per-timestep FiLM encoding (timesteps in the environment)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two model variants are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Large Model - Bidirectional GRU + attention + large hidden state&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Small Model - Unidirectional GRU + No attention + small hidden state&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Curriculum-based pretraining and interactive learning was found to be useful in only some cases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Poincaré Embeddings for Learning Hierarchical Representations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Poincare-Embeddings-for-Learning-Hierarchical-Representations"/>
   <updated>2018-10-11T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Poincare Embeddings for Learning Hierarchical Representations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Much of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This approach is not effective when data has a (latent) hierarchical structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.08039.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperbolic-geometry&quot;&gt;Hyperbolic Geometry&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;poincare-embeddings&quot;&gt;Poincare Embeddings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Poincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Distance between 2 pints change smoothly and is symmetric. Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Eventually the norm of a point represents its hierarchy and distance between the points represents similarity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RSGD (Riemannian SGD) method is used.&lt;/li&gt;
  &lt;li&gt;Riemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor.&lt;/li&gt;
  &lt;li&gt;The embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-details&quot;&gt;Training Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Initializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps.&lt;/li&gt;
  &lt;li&gt;The model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Embedding taxonomy for wordnet task&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Setup&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Reconstruction&lt;/li&gt;
          &lt;li&gt;Link Prediction&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The input data is a collection of a pair of words (u, v) which are related to each other.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For each word pair, 10 negative samples of the form (u, v’) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Network Embedding&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Baselines&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Euclidean Embeddings&lt;/li&gt;
          &lt;li&gt;Translational Embedding where a relation vector corresponding to the edge type is also learnt.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Datasets&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ASTROPH&lt;/li&gt;
          &lt;li&gt;CONDMAT&lt;/li&gt;
          &lt;li&gt;GRQC&lt;/li&gt;
          &lt;li&gt;HEPPH&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lexical Entailment&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10].

* The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic embeddings are not suitable for all the datasets. Eg if the dataset is not tree-like or has cycles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>When Recurrent Models Don’t Need To Be Recurrent</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/When-Recurrent-Models-Don-t-Need-To-Be-Recurrent"/>
   <updated>2018-10-04T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/When Recurrent Models Don’t Need To Be Recurrent</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores “if a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Well behaved” is defined in terms of control-theoretic notion of stability. This roughly requires that the gradients do not explode over time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference. The results are empirically validated as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.10369&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider a general, non linear dynamical system given by a differential state transition map Φ&lt;sub&gt;w&lt;/sub&gt;. The hidden h&lt;sub&gt;t&lt;/sub&gt; = Φ&lt;sub&gt;w&lt;/sub&gt;(h&lt;sub&gt;t-1&lt;/sub&gt;, x&lt;sub&gt;t&lt;/sub&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Assumptions:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Φ is smooth in w and h.&lt;/li&gt;
      &lt;li&gt;h&lt;sub&gt;0&lt;/sub&gt; = 0&lt;/li&gt;
      &lt;li&gt;Φ&lt;sub&gt;w&lt;/sub&gt;(0, 0) = 0 (can be ensured by translation)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stable models are the ones where Φ is contractive ie Φ&lt;sub&gt;w&lt;/sub&gt;(h, x) - Φ&lt;sub&gt;w&lt;/sub&gt;(h’, x) is less than Λ * (h - h’)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, in RNN, stability would require that norm(w) is less than (L&lt;sub&gt;p&lt;/sub&gt;)&lt;sup&gt;-1&lt;/sup&gt; where L&lt;sub&gt;p&lt;/sub&gt; is the Lipschitz constant of the point-wise non linearity used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The feedforward approximation uses a finite context (of length k) and is a truncated model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A non-parametric function f maps the output of the recurrent model to prediction. If f is desired to be a parametric model, its parameters can be pushed to the recurrent model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theoretical-results&quot;&gt;Theoretical Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For a Λ-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space. It can be shown that this small difference does not lead to large gradient differences during subsequent update steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The stability condition is important as, without that, truncated models would be bad (even for large values of k). Further, it is difficult to show that gradient descent converges to a stationary point.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>HoME - a Household Multimodal Environment</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/HoME-a-Household-Multimodal-Environment"/>
   <updated>2018-09-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/HoME - a Household Multimodal Environment</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Environment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.11017&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Humans learn by interacting with their surroundings (environment).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Open-source and Open-AI gym compatible&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Built on top of 45000 3D house layouts from SUNCG dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Provides both 3D visual and audio recording.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic image segmentation and langauge description of objects.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;components&quot;&gt;Components&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rendering Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using Panda 3D game engine.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Renders RGB+depth scenes based on textures, multi-source lightings and shadows.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Acoustic Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using EVERT&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Supports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Semantics Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Provides a short textual description for each object, along with information like color, category, material size, location etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Physics Engine&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Implemented using Bullet3 Engine&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Supports physical interaction, external forces like gravity and position and velocity information for multiple agents.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;potential-applications&quot;&gt;Potential Applications&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Visual Question Answering&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conversational Agents&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training an agent to follow instructions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-agent communication&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Emergence of Grounded Compositional Language in Multi-Agent Populations</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Emergence-of-Grounded-Compositional-Language-in-Multi-Agent-Populations"/>
   <updated>2018-09-12T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Emergence of Grounded Compositional Language in Multi-Agent Populations</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The language is quite rudimentary and is essentially a sequence of abstract discrete symbols. But it does comprise of a defined vocabulary and syntax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04908&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cooperative, partially observable Markov game (multi-agent extension of MDP).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All agents have identical action and observation spaces, use the same policy and receive a shared reward.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;grounded-communication-environment&quot;&gt;Grounded Communication Environment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The agents and the landmarks would occupy some location and would have some attributes (colour, shape).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Within the environment, the agents can &lt;em&gt;go to&lt;/em&gt; a location, &lt;em&gt;look&lt;/em&gt; at a location or &lt;em&gt;do nothing&lt;/em&gt;. Additionally, they can utter communication symbols c (from a shared vocabulary C). Agents themselves learn to assign a meaning to the symbols.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal for agent &lt;em&gt;i&lt;/em&gt; consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A model of all agent and environment state dynamics is created over time and the return gradient is computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gumbel-Softmax distribution is used to obtain categorical word emission c.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations. These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compositionality-and-vocabulary-size&quot;&gt;Compositionality and Vocabulary Size&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words. This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Along the lines of rich gets richer dynamics, the communication symbol c’s are modelled as being generated by a Dirichlet process. The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;case-i---agents-can-not-see-each-other&quot;&gt;Case I - Agents can not see each other&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Non-verbal communication is not possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When trained with just 2 agents, symbols are assigned for each landmark and action.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As the number of agents is increased, additional symbols are used to refer to agents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks. If distractor locations are added, the agents learn to ignore them.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;non-verbal-communication&quot;&gt;Non-verbal communication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Agents are allowed to observe other agents’ position, gaze etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now the location can be pointed to using gaze.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If gaze is disabled, the agent could indicate the goal landmark by moving to it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Basically even when the communication is disabled the agents can come up with strategies to complete the task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Semantic Loss Function for Deep Learning with Symbolic Knowledge</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Semantic-Loss-Function-for-Deep-Learning-with-Symbolic-Knowledge"/>
   <updated>2018-08-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Semantic Loss Function for Deep Learning with Symbolic Knowledge</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes an approach for using symbolic knowledge in deep learning systems. These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.11157&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable. In this process, the precise meaning of symbolic knowledge is often lost.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A differentiable “semantic loss” is derived which captures the meaning of the constraint while being independent of its syntax.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A state &lt;strong&gt;x&lt;/strong&gt; (state refers to the instantiation of boolean variables) satisfies a sentence &lt;em&gt;a&lt;/em&gt; if &lt;em&gt;a&lt;/em&gt; evaluates to true when using the variables as specified by &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A sentence &lt;em&gt;a&lt;/em&gt; entails another sentence &lt;em&gt;b&lt;/em&gt; if all states that satisfy &lt;em&gt;a&lt;/em&gt; also satisfy &lt;em&gt;b&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The row output vector of the neural network is denoted as &lt;em&gt;p&lt;/em&gt; where each value in &lt;em&gt;p&lt;/em&gt; denotes the probability of an output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Three different output constraints are studied:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;Exactly-one constraint&lt;/em&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Exactly one value in &lt;em&gt;p&lt;/em&gt; should be true.&lt;/li&gt;
          &lt;li&gt;Can be expressed in boolean logic as follows: Let (x1, x2, …, xn) be variables in &lt;em&gt;p&lt;/em&gt;. Then (not xi or not xj) for all pair of variables and (x1 or x2 or … xn).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Valid Simple Path Constraint&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;Set of edges must form a valid path.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Ordering Constraint&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;Defining an ordering over the variables.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-loss&quot;&gt;Semantic Loss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p)&lt;/em&gt; is a function of a propositional logic sentence &lt;em&gt;a&lt;/em&gt; (the symbolic knowldge constraint) and &lt;em&gt;p&lt;/em&gt; (output of the neural network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;a&lt;/em&gt; is defined over variables (x1, …, xn) and &lt;em&gt;p&lt;/em&gt; is interpreted as a vector of probabilities corresponding to these variables &lt;em&gt;xi’s&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;main-axioms-and-insights&quot;&gt;Main Axioms and Insights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Monotonicity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If a sentence &lt;em&gt;a&lt;/em&gt; entails another sentence &lt;em&gt;b&lt;/em&gt; then for any given &lt;em&gt;p&lt;/em&gt;, &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p) &amp;gt; L&lt;sup&gt;s&lt;/sup&gt;(b, p)&lt;/em&gt; ie adding more constraints cannot decrease the semantic loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Semantic Equivalence&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If two sentences are logically equivalent, their semantic loss is the same.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Identity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For any given sentence &lt;em&gt;a&lt;/em&gt;, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the “one-hot” constraint as a boolean expression is equivalent to a one-hot vector.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Satisfaction&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If &lt;em&gt;p&lt;/em&gt; entails the sentence &lt;em&gt;a&lt;/em&gt; then &lt;em&gt;L&lt;sup&gt;s&lt;/sup&gt;(a, p) = 0&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Label-literal correspondence&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;When the constraint is defined in terms of a single variable, it can be interpreted as the supervised label.&lt;/li&gt;
      &lt;li&gt;Hence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Truth&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The semantic loss of a true sentence is 0&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-negativity&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Semantic loss should always be non-negative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Probabilities of variables that are not part of the constraint, do not affect the semantic loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;It can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-evaluation&quot;&gt;Experimental Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Semantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model. Further, the semantic loss does not improve the performance in case of fully supervised scenario.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further experiments are performed to evaluate the performance of the semantic loss on complex constraints. Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tractability-of-semantic-loss&quot;&gt;Tractability of Semantic Loss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The semantic loss is similar to the automated reasoning task called as weight model counting (wmc).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Circuit compiler techniques can be used to compute wmc while allowing backpropagation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good. It would be interesting to extend and scale this method for more complex constraints.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Hierarchical Graph Representation Learning with Differentiable Pooling</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling"/>
   <updated>2018-08-16T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Hierarchical Graph Representation Learning with Differentiable Pooling</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.08804&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-idea&quot;&gt;Key Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph &lt;em&gt;G(A, F)&lt;/em&gt;, where &lt;em&gt;A&lt;/em&gt; is the adjacency matrix and &lt;em&gt;F&lt;/em&gt; is the feature matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a permutation invariant GNN that follows the message passing architecture. The output of this GNN can be expressed as &lt;em&gt;Z = GNN(A, X)&lt;/em&gt; where &lt;em&gt;X&lt;/em&gt; is the current feature matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goal is to stack &lt;em&gt;L&lt;/em&gt; GNN layers on top of each other such that the &lt;em&gt;l&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer uses coarsened output from the  &lt;em&gt;(l-1)&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This coarsening operation uses a cluster assignment matrix &lt;em&gt;S&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learned cluster assignment matrix at layer &lt;em&gt;l&lt;/em&gt; is denoted at &lt;em&gt;S&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given &lt;em&gt;S&lt;sup&gt;l&lt;/sup&gt;&lt;/em&gt;, the embedding matrix for the &lt;em&gt;(l+1)&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; layer is given as &lt;em&gt;transpose(S&lt;sub&gt;l&lt;/sub&gt;)Z&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt; and adjancecy matrix is given by &lt;em&gt;transpose(S&lt;sub&gt;l&lt;/sub&gt;)A&lt;sub&gt;l&lt;/sub&gt;S&lt;sub&gt;l&lt;/sub&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A new GNN, called as GNN&lt;sub&gt;pool&lt;/sub&gt; is used to produce the assignment matrix &lt;em&gt;S&lt;/em&gt; by taking a softmax over &lt;em&gt;GNN&lt;sub&gt;pool&lt;/sub&gt;(A&lt;sup&gt;l&lt;/sup&gt;, X&lt;sup&gt;l&lt;/sup&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auxiliary-losses&quot;&gt;Auxiliary Losses&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;baselines&quot;&gt;Baselines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GNN based models
    &lt;ul&gt;
      &lt;li&gt;GraphSage
        &lt;ul&gt;
          &lt;li&gt;Mean pooling&lt;/li&gt;
          &lt;li&gt;Set2Set pooling&lt;/li&gt;
          &lt;li&gt;Sort pooling&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Structure2vec&lt;/li&gt;
      &lt;li&gt;Edge conditioned filters in CNN&lt;/li&gt;
      &lt;li&gt;PatchySan&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kernel based models
    &lt;ul&gt;
      &lt;li&gt;Graphlet, shortest path etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-variants&quot;&gt;Model Variants&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GraphSage
    &lt;ul&gt;
      &lt;li&gt;Mean pool + Diff pool (3 or 2 layers)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Structure2Vec + Diffpool&lt;/li&gt;
  &lt;li&gt;Diffpool-Det
    &lt;ul&gt;
      &lt;li&gt;The assignment matrix &lt;em&gt;S&lt;/em&gt; are generated using graph clustering algorithms.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Diffpool-NoLP
    &lt;ul&gt;
      &lt;li&gt;The link prediction objective function is turned off.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time. But given that now there are 2 GNN models per layer, the size of the model should increase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DiffPool can capture hierarchical community structure even when trained on just the graph classification loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Imagination-Augmented Agents for Deep Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Imagination-Augmented-Agents-for-Deep-Reinforcement-Learning"/>
   <updated>2018-08-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Imagination-Augmented Agents for Deep Reinforcement Learning</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I2A agent uses the predictions from a learned environment model as an additional context in deep policy networks. This leads to improved data efficiency and robustness to imperfect models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.06203&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I2A agent has two main modules - Imagination module and the Policy module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Imagination Module&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Environment Model&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;This is a recurrent model, trained in an unsupervised manner using the agent trajectories. It can be used to predict the future state given the current state and action.&lt;/li&gt;
          &lt;li&gt;The environment model can be rolled out multiple times to obtain a simulated trajectory or an “imagined” trajectory.&lt;/li&gt;
          &lt;li&gt;During each rollout, the actions are chosen using a rollout policy π&lt;sub&gt;r&lt;/sub&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Rollout Encoder&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;A rollout encoder &lt;em&gt;E&lt;/em&gt; (LSTM) is used to process the entire imagined rollout.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The imagination module is used to generate &lt;em&gt;n&lt;/em&gt; trajectories. Each trajectory is a sequence of outputs of the environment model.&lt;/li&gt;
      &lt;li&gt;These &lt;em&gt;n&lt;/em&gt; trajectories are concatenated into a single “imagination” vector.&lt;/li&gt;
      &lt;li&gt;The training data for the environment model is generated from trajectories of a partially trained model-free agent.&lt;/li&gt;
      &lt;li&gt;Pretraining the environment model (instead of joint training with policy) leads to faster runtime.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Policy Module&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This module uses the output of both model-based path and model-free path as its input. It generates the policy vector and value function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rollout Strategy&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;One rollout is performed for each possible action in the environment ie, the first action in the i&lt;sup&gt;th&lt;/sup&gt; rollout is the i&lt;sup&gt;th&lt;/sup&gt; action in the action set.&lt;/li&gt;
      &lt;li&gt;Subsequent actions are generated using a shared rollout policy π&lt;sub&gt;’&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;An effective strategy was to create a small model-free network π&lt;sub&gt;’&lt;/sub&gt;(o&lt;sub&gt;t&lt;/sub&gt;) and then add a KL loss component that encourages π&lt;sub&gt;’&lt;/sub&gt;(o&lt;sub&gt;t&lt;/sub&gt;)to be similar to the imagination augmented policy π(o&lt;sub&gt;t&lt;/sub&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baselines&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Model-free agent&lt;/li&gt;
      &lt;li&gt;Copy-model agent - same as I2A but the environment model is replaced by a “copy” model that just returns the input observations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Sokoban
        &lt;ul&gt;
          &lt;li&gt;Task is to push a number of boxes onto given target locations.&lt;/li&gt;
          &lt;li&gt;I2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate).&lt;/li&gt;
          &lt;li&gt;In case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate.&lt;/li&gt;
          &lt;li&gt;Monte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder.&lt;/li&gt;
          &lt;li&gt;Predicting the reward along with value function and action seems to speed up training.&lt;/li&gt;
          &lt;li&gt;If a near-perfect model is available, I2A agent’s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy. The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;MiniPacman&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;I2A agent is evaluated to see if a single model can be used to solve multiple tasks.&lt;/li&gt;
          &lt;li&gt;A new environment is designed to define multiple tasks in an environment with shared state transitions.&lt;/li&gt;
          &lt;li&gt;Each task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost.&lt;/li&gt;
          &lt;li&gt;A single environment model is trained to predict both observations (frames) and events (eg “eating a pill”). This way, the environment model is shared across all tasks.&lt;/li&gt;
          &lt;li&gt;Baseline agents and I2As are trained on each task separately. I2A architecture outperforms the standard agent in all tasks and the copy-model
baseline in all but one task.&lt;/li&gt;
          &lt;li&gt;The improvement in performance is higher for tasks where rewards are sparse and where the anticipation
of ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Kronecker Recurrent Units</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Kronecker-Recurrent-Units"/>
   <updated>2018-07-19T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Kronecker Recurrent Units</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent Neural Networks have two key issues:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Over parameterization&lt;/strong&gt; which increases the time for training and inference.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Ill conditioned&lt;/strong&gt; recurrent weight matrix which makes training difficult due to vanishing or exploding gradients.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.10142&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;h3 id=&quot;existing-solutions-for-overparameterization&quot;&gt;Existing solutions for overparameterization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low-rank decomposition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training a neural network on the soft targets predicted by a big pre-trained network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low-bit precision training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hashing.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;existing-solutions-for-vanishing-and-exploding-gradients&quot;&gt;Existing solutions for vanishing and exploding gradients&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Gating mechanism like in LSTMs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient Clipping.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Orthogonal Weight Initialization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Parameterizing recurrent weight matrix.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kru&quot;&gt;KRU&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vanishing and exploding gradients are taken care of by using a soft unitary constraint.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why not use strict unitary constraint:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Restricts the search space and makes learning process unstable.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Makes forgetting (irrelevant) information difficult.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Relaxing the strict constraint has shown to improve the convergence speed and generalization performance.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KRU can be easily plugged into RNNs, LSTMs and other variants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The recurrent matrix &lt;em&gt;W&lt;/em&gt; is paramterized as a kronecker product of &lt;em&gt;F&lt;/em&gt; matrices &lt;em&gt;W&lt;sub&gt;0&lt;/sub&gt;, …, W&lt;sub&gt;F-1&lt;/sub&gt;&lt;/em&gt; where each &lt;em&gt;W&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; is a complex matrix of shape &lt;em&gt;P&lt;sub&gt;f&lt;/sub&gt; x Q&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; and the product of all &lt;em&gt;P&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; and producto of all &lt;em&gt;Q&lt;sub&gt;f&lt;/sub&gt;&lt;/em&gt; are both equal to &lt;em&gt;N&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why is &lt;em&gt;W&lt;/em&gt; a complex matrix?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In the real space, the set of all unitary matrices have the determinant as 1 or -1.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Given that determinant is a continuous function, the unitary set in the real space is disconnected.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The unitary set in the complex space is connected as its determinants are points on the unit circle.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;soft-unitary-constraint&quot;&gt;Soft Unitary Constraint&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;A soft unitary constraint is introduced in the form of regularization term&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;W&lt;sub&gt;f&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt;W&lt;sub&gt;f&lt;/sub&gt; - I&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt;&lt;sup&gt;2&lt;/sup&gt; (per kronecker factored recurrent matrix).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If each of the Kronecker factors is unitary, the resulting matrix &lt;em&gt;W&lt;/em&gt; would also be unitary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is computationally inefficient to apply this constraint over the recurrent matrix &lt;em&gt;W&lt;/em&gt; itself as the complexity of the regularizer is given as &lt;em&gt;O(N&lt;sup&gt;3&lt;/sup&gt;)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Use of Kronecker factorisation makes it computationally feasible to use this regulariser.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output. The recurrent dynamics should be implemented via a low capacity model.s per task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning Independent Causal Mechanisms</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-Independent-Causal-Mechanisms"/>
   <updated>2018-07-11T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning Independent Causal Mechanisms</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.00961&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We start with a given data distribution &lt;em&gt;P&lt;/em&gt; (say the MNIST dataset) where each x ε R&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider N transformations M&lt;sub&gt;1&lt;/sub&gt;, …, M&lt;sub&gt;N&lt;/sub&gt; (functions that map input x to transformed input x’). Note that N need not be known before hand.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These transformations can be thought of as independent (from other transformations) causal mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Applying these transformation would give N new distributions Q&lt;sub&gt;1&lt;/sub&gt;, …, Q&lt;sub&gt;N&lt;/sub&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At training time, two datasets are created. One dataset corresponds to untransformed objects (sampled from &lt;em&gt;P&lt;/em&gt;), referred to as &lt;em&gt;D&lt;sub&gt;P&lt;/sub&gt;&lt;/em&gt;. The other dataset corresponds to samples from the transformed distribution &lt;em&gt;Q&lt;/em&gt; and is referred to as &lt;em&gt;D&lt;sub&gt;Q&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that all the samples in &lt;em&gt;D&lt;sub&gt;P&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;D&lt;sub&gt;Q&lt;/sub&gt;&lt;/em&gt; are sampled independently and no supervising information is needed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A series of N’ parametric models, called as experts, are initialized and would be trained to learn the different mechanisms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For simplicity, assume that N = N’. If N &amp;gt; N’, some experts would learn more than one transformation or certain transformations would not be learnt. If N &amp;lt; N’, some experts would not learn anything or some experts would learn the same distribution. All of these cases can be diagnosed and corrected by changing the number of experts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The experts are trained with the goal of maximizing an objective parameter &lt;em&gt;c&lt;/em&gt;: R&lt;sup&gt;d&lt;/sup&gt; to R. &lt;em&gt;c&lt;/em&gt; takes high values on the support of  &lt;em&gt;P&lt;/em&gt; and low values outside.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, an example x&lt;sub&gt;Q&lt;/sub&gt; (from D&lt;sub&gt;Q&lt;/sub&gt;) is fed to all the experts at the same time. Each expert produces a value &lt;em&gt;c&lt;sub&gt;j&lt;/sub&gt; = c(E&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;Q&lt;/sub&gt;))&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The winning expert is the one whose output is the max among all the outputs. Its parameters are updated to maximise its output while the other experts are not updated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This forces the best performing model to become even better and hence specialize.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The objective &lt;em&gt;c&lt;/em&gt; comes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each expert can be thought of as a GAN that conditions on the input x&lt;sub&gt;Q&lt;/sub&gt; (and not on a noise vector). The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximate-identity-initialization&quot;&gt;Approximate Identity Initialization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This ensures that the experts start from a similar level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In practice, it seems necessary for the success of the proposed approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The approximate quality of reconstructed output was also evaluated using a downstream task.
    &lt;ul&gt;
      &lt;li&gt;3 type of inputs were created:
        &lt;ul&gt;
          &lt;li&gt;Untransformed images&lt;/li&gt;
          &lt;li&gt;Transformed images&lt;/li&gt;
          &lt;li&gt;Transformed images a being processed by experts.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;These inputs are fed to a pretrained MNISTN classifier.&lt;/li&gt;
      &lt;li&gt;The classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The experts E&lt;sub&gt;i&lt;/sub&gt; generalize on the data points from a different dataset as well.
    &lt;ul&gt;
      &lt;li&gt;To test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST).&lt;/li&gt;
      &lt;li&gt;Each expert consistently applies the same transformation even though the inputs are outside the training domain.&lt;/li&gt;
      &lt;li&gt;This suggests that the experts have generalized to different transformations irrespective of the underlying dataset.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Appendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all. This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Memory-based Parameter Adaptation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Memory-Based-Parameter-Adaption"/>
   <updated>2018-07-04T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Memory-Based Parameter Adaption</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Standard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.10542&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA consists of 3 components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Embedding Network &lt;em&gt;f&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Memory &lt;em&gt;M&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Output network &lt;em&gt;g&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;f&lt;/em&gt; and &lt;em&gt;g&lt;/em&gt; are parametric components while &lt;em&gt;M&lt;/em&gt; is a non-parametric component.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;M&lt;/em&gt; is a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a new training tuple (x&lt;sub&gt;j&lt;/sub&gt;, y&lt;sub&gt;j&lt;/sub&gt;) is fed as input to the model, a key-value pair (h&lt;sub&gt;j&lt;/sub&gt;, v&lt;sub&gt;j&lt;/sub&gt;) is added to the memory. h&lt;sub&gt;j&lt;/sub&gt; = f(x&lt;sub&gt;j&lt;/sub&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The memory has a fixed size and acts as a circular buffer. When it gets filled up, earlier examples are dropped.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When accessing the memory using a key &lt;em&gt;h&lt;sub&gt;key&lt;/sub&gt;&lt;/em&gt;, the k-nearest neighbours (in terms of distance from the given key) are retrieved.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training-phase&quot;&gt;Training Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;During the training phase, the memory is only used to store the input examples and does not interfere with the training procedure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;testing-phase&quot;&gt;Testing Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, the memory is used to adapt the parameters of the output network &lt;em&gt;g&lt;/em&gt; while the embedding network &lt;em&gt;f&lt;/em&gt; remains the same.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each retrived neighbour is a tuple of the form (h&lt;sub&gt;k&lt;/sub&gt;, v&lt;sub&gt;k&lt;/sub&gt;, w&lt;sub&gt;k&lt;/sub&gt;) where w&lt;sub&gt;k&lt;/sub&gt; is propotional to the closeness between the input query and the key corresponding to the retrived example.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The collection of all the retrieved examples are referred to as the context &lt;em&gt;C&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parameters of the output network &lt;em&gt;g&lt;/em&gt; are adapted from θ to θ&lt;sub&gt;x&lt;/sub&gt; where θ&lt;sub&gt;x&lt;/sub&gt; = θ + δ&lt;sub&gt;M&lt;/sub&gt;(x, θ)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;δ&lt;sub&gt;M&lt;/sub&gt;(x, θ) is referred to as the contextual update of parameters of the output network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;interpretation-of-mbpa&quot;&gt;Interpretation of MbPA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The expression corresponding to  δ&lt;sub&gt;M&lt;/sub&gt;(x, θ) can be obtained by performing gradient descent to minimise the max a posterior over the context C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This idea can be thought of as a generalisation of attention. Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model. Refer appendix E in the paper for further details.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continual Learning:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setting, the model encounters a sequence of tasks and cannot revisit a previous task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Permuted MNIST dataset was used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incremental Learning:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes. The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Imagenet dataset with Resnet V1 model is used. It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unbalanced Dataset:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Language Modelling:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;MbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks. PTB and WikiText datasets were used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Born Again Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Born-Again-Neural-Networks"/>
   <updated>2018-06-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Born Again Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.04770&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The standard KD setting is as follows:
    &lt;ul&gt;
      &lt;li&gt;Start with an untrained network (or ensemble of networks) and train them for the given task. This network is referred to as the teacher network.&lt;/li&gt;
      &lt;li&gt;Now start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network. This network is referred to as the student network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks. The student tried to predict the correct answer while matching the output distribution of the teacher.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting student network is referred to as BAN - Born Again Network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dark-knowledge&quot;&gt;Dark Knowledge&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network&quot;&gt;Hinton et al&lt;/a&gt; suggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes. This information is referred to as the “dark knowledge”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor. This sample specific weight is defined by the value of the teacher’s max output.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This suggests distillation may be performing some kind of importance weighing. To explore this further, the paper considers 2 cases:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Confidence Weighted By Teacher Max (CWTM) - where each example in the student’s loss function is weighted by the confidence that the teacher has on the prediction for that sample. The student incurs a higher loss if the teacher was more confident about the example.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Dark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher’s predictive distribution are permuted thus destroying the information about which output classes are related.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;image-data&quot;&gt;Image Data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets
    &lt;ul&gt;
      &lt;li&gt;CIFAR10&lt;/li&gt;
      &lt;li&gt;CIFAR100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;ResNets&lt;/li&gt;
      &lt;li&gt;DenseNets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BAN Variants
    &lt;ul&gt;
      &lt;li&gt;BAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets. Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc.&lt;/li&gt;
      &lt;li&gt;Two settings with CWTM and DKPP as explained earlier.&lt;/li&gt;
      &lt;li&gt;BAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-data&quot;&gt;Text Data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets:
    &lt;ul&gt;
      &lt;li&gt;PTB Dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;CNN-LSTM model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BAN Variant
    &lt;ul&gt;
      &lt;li&gt;LSTM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;BAN student models improved over their teachers in most of the configurations.&lt;/li&gt;
  &lt;li&gt;Training BANs across multiple generations leads to saturating improvements.&lt;/li&gt;
  &lt;li&gt;The student models exhibit improvements even in the control settings (CWTM and DKPP).
    &lt;ul&gt;
      &lt;li&gt;One reason could be that the permutation procedure did not remove the higher order moments of output distribution.&lt;/li&gt;
      &lt;li&gt;Improvements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher’s output distribution is more spread.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Net2Net-Accelerating Learning via Knowledge Transfer</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Net2Net-Accelerating-Learning-via-Knowledge-Transfer"/>
   <updated>2018-05-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Net2Net - Accelerating Learning via Knowledge Transfer</id>
   <content type="html">&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.05641&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/paengs/Net2Net&quot;&gt;Link to an implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach works as follows - Let us say that the teacher network was represented by the transformation &lt;em&gt;y = f(x, θ)&lt;/em&gt; where &lt;em&gt;θ&lt;/em&gt; refer to the parameters of the network. The task is to choose a new set of parameters &lt;em&gt;θ’&lt;/em&gt; for the student network &lt;em&gt;g(x, θ’)&lt;/em&gt; such that for all &lt;em&gt;x, f(x, θ) = g(x, θ’)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To start, we can assume that &lt;em&gt;f&lt;/em&gt; and &lt;em&gt;g&lt;/em&gt; are composed of standard linear layers. Layer &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;i+1&lt;/em&gt; are represented by weights &lt;em&gt;W&lt;sub&gt;mxn&lt;/sub&gt;&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;W&lt;sub&gt;nxp&lt;/sub&gt;&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We want to grow layer &lt;em&gt;i&lt;/em&gt; to have &lt;em&gt;q&lt;/em&gt; output units (where &lt;em&gt;q&lt;/em&gt; &amp;gt; &lt;em&gt;n&lt;/em&gt;) and layer &lt;em&gt;i+1&lt;/em&gt; to have &lt;em&gt;q&lt;/em&gt; input units. The new weight matrix would be &lt;em&gt;U&lt;sub&gt;mxq&lt;/sub&gt;&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;U&lt;sub&gt;qxp&lt;/sub&gt;&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The first &lt;em&gt;q&lt;/em&gt; columns (rows) of &lt;em&gt;W&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; (&lt;em&gt;W&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;) would be copied as it is into &lt;em&gt;U&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt;(&lt;em&gt;U&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For filling the remaining &lt;em&gt;n-q&lt;/em&gt; slots, columns (rows) would be sampled randomly from &lt;em&gt;W&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; (&lt;em&gt;W&lt;sup&gt;i+1&lt;/sup&gt;&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, each layer in &lt;em&gt;U&lt;sup&gt;i&lt;/sup&gt;&lt;/em&gt; is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The benefits of using this approach are the following:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The newly created student network performs at least as good as the teacher network.&lt;/li&gt;
      &lt;li&gt;Any changes to the network are guaranteed to be an improvement.&lt;/li&gt;
      &lt;li&gt;It is safe to optimize all the parameters in the network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The variant discussed above is called the &lt;strong&gt;Net2WiderNet&lt;/strong&gt; variant. There is another variant called&lt;strong&gt;Net2DeeperNet&lt;/strong&gt; that enables the network to grow in depth.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In that case, a new matrix, &lt;em&gt;U&lt;/em&gt;, initialized as the identity matrix, is added to the network. Note that unlike the &lt;strong&gt;Net2WiderNet&lt;/strong&gt;, this approach would not work with arbitrary activation function between the layers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;strengths&quot;&gt;Strengths&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The function preserving transformations need to be worked out manually. Extra care needs to be taken when operations like concatenation or batch norm are present.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning to Count Objects in Natural Images for Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-to-Count-Objects-in-Natural-Images-for-Visual-Question-Answering"/>
   <updated>2018-05-06T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning to Count Objects in Natural Images for Visual Question Answering</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Most of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image. The main reasons are:
    &lt;ul&gt;
      &lt;li&gt;Most VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector. These aggregated features helps in most category of questions but seems to hurt for counting based questions.&lt;/li&gt;
      &lt;li&gt;For the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image. This limits the scope of supervision.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate &lt;strong&gt;count&lt;/strong&gt; module.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05766&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object. This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image.&lt;/p&gt;

&lt;p&gt;Even multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to). Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis.&lt;/p&gt;

&lt;p&gt;The proposed &lt;strong&gt;count&lt;/strong&gt; module is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions.&lt;/p&gt;

&lt;p&gt;The inputs to the &lt;strong&gt;count&lt;/strong&gt; module are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question.&lt;/p&gt;

&lt;p&gt;The top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap). The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily.&lt;/p&gt;

&lt;p&gt;To explain their methodology, the paper simplifies the setting by making two assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal).&lt;/li&gt;
  &lt;li&gt;The second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the &lt;strong&gt;count&lt;/strong&gt; module.&lt;/p&gt;

&lt;p&gt;Given the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects.&lt;/p&gt;

&lt;p&gt;As the first step, the attention weights (&lt;strong&gt;a&lt;/strong&gt;) are used to generate an attention matrix (&lt;strong&gt;A&lt;/strong&gt;) by performing an outer product between &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;a&lt;sup&gt;T&lt;/sup&gt;&lt;/strong&gt;. This corresponds to the step of creating a graph from the input.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt; corresponds to the adjacency matrix of that graph. The attention weight for the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; proposal corresponds to the &lt;em&gt;i&lt;sup&gt;th&lt;/sup&gt;&lt;/em&gt; node in the graph and the edge between the nodes &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; has the weight &lt;strong&gt;a&lt;sub&gt;i&lt;/sub&gt;*a&lt;sub&gt;j&lt;/sub&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition &lt;strong&gt;a&lt;sub&gt;i&lt;/sub&gt;&lt;/strong&gt; = 1 is a complete directed graph with self-loops. Given such a graph, the number of vertices, &lt;em&gt;V = sqrt(E)&lt;/em&gt; where &lt;em&gt;E&lt;/em&gt; could be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix.&lt;/p&gt;

&lt;p&gt;The objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph. This requires removing two type of duplicate edges - intra-object edges and inter-object edges.&lt;/p&gt;

&lt;p&gt;Intra-object edges can be removed by computing a distance matrix, &lt;strong&gt;D&lt;/strong&gt;, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix. A modified adjacency matrix &lt;strong&gt;A’&lt;/strong&gt; is obtained by performing the element-wise product between f&lt;sub&gt;1&lt;/sub&gt;(&lt;strong&gt;A&lt;/strong&gt;) and f&lt;sub&gt;2&lt;/sub&gt;(&lt;strong&gt;D&lt;/strong&gt;) where f&lt;sub&gt;1&lt;/sub&gt; and f&lt;sub&gt;2&lt;/sub&gt; are piece-wise linear functions that are learnt via backpropogation.&lt;/p&gt;

&lt;p&gt;The inter-object edges are removed in the following manner:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Count the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number.&lt;/li&gt;
  &lt;li&gt;This creates the effect of reducing the weights of multiple proposals equivalent to a single proposal.&lt;/li&gt;
  &lt;li&gt;The number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights &lt;strong&gt;a&lt;/strong&gt;, adjacency matrix &lt;strong&gt;A&lt;/strong&gt; and distance matrix &lt;strong&gt;D&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;The matrix corresponding to the similarity between proposals  (&lt;strong&gt;sim&lt;sub&gt;i, j&lt;/sub&gt;&lt;/strong&gt;) is transformed into a vector corresponding to the scaling factor of each node (&lt;strong&gt;s&lt;sub&gt;i&lt;/sub&gt;&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;s&lt;/strong&gt; can be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges. The self edges (which were removed while computing &lt;strong&gt;A’&lt;/strong&gt; are added back (after scaling with &lt;strong&gt;s&lt;/strong&gt;) to obtain a new transformed matrix &lt;strong&gt;C&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The transformed matrix &lt;strong&gt;C&lt;/strong&gt; is a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals. The actual count can be obtained from &lt;strong&gt;C&lt;/strong&gt; by performing a sum over all its values as described earlier. The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues. The network produces a &lt;strong&gt;k&lt;/strong&gt;-hot &lt;strong&gt;n&lt;/strong&gt;-dimensional vector called &lt;strong&gt;o&lt;/strong&gt; where &lt;strong&gt;n&lt;/strong&gt; is the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count). In the ideal setting, &lt;strong&gt;k&lt;/strong&gt; should be one, as the network would produce an integer value but in practice, the network produces a real number so &lt;strong&gt;k&lt;/strong&gt; can be upto 2. If &lt;strong&gt;c&lt;/strong&gt; is an exact integer, the output is a 1-hot vector with the value in index corresponding to &lt;strong&gt;c&lt;/strong&gt; set to 1. If &lt;strong&gt;c&lt;/strong&gt; is a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which &lt;strong&gt;c&lt;/strong&gt; lies).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;count&lt;/strong&gt; module supports computing the confidence of a prediction by defining two variables p&lt;sub&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/sub&gt; and p&lt;sub&gt;&lt;strong&gt;D&lt;/strong&gt;&lt;/sub&gt; which compute the average distance of f&lt;sub&gt;6&lt;/sub&gt;(&lt;strong&gt;a&lt;/strong&gt;) and $f&lt;sub&gt;7&lt;/sub&gt;(&lt;strong&gt;D&lt;/strong&gt;) from 0.5. The final output &lt;strong&gt;o’&lt;/strong&gt; is defined as f&lt;sub&gt;8&lt;/sub&gt;(p&lt;sub&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;/sub&gt; + p&lt;sub&gt;&lt;strong&gt;D&lt;/strong&gt;&lt;/sub&gt;) . &lt;strong&gt;o&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the different f functions are piece wise linear functions and are learnt via backpropagation.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;The authors created a new category of count-based questions by filtering the number-type questions to remove questions like “What is the time right now”. These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the &lt;strong&gt;count&lt;/strong&gt; model.&lt;/p&gt;

&lt;p&gt;The authors augmented a state of the art &lt;a href=&quot;https://arxiv.org/abs/1704.03162&quot;&gt;VQA model&lt;/a&gt; with their &lt;strong&gt;count&lt;/strong&gt; module and show substantial gains over the count-type questions for the &lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;&gt;VQA-v2 dataset&lt;/a&gt;. This augmentation does not drastically impact the performance on non-count questions.&lt;/p&gt;

&lt;p&gt;The overall idea is quite crisp and intutive and the paper is easy to follow. It would be even better if there were some more abalation studies. For example, why are the piece-wise linear functions assumed to have 16 linear components? Would a smaller or larger number be better?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Neural Message Passing for Quantum Chemistry</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Message-Passing-for-Quantum-Chemistry"/>
   <updated>2018-04-08T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Message Passing for Quantum Chemistry</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Variants of the MPNN model achieve very good performance on the task of predicting the property of the molecules.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.01212&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mpnn&quot;&gt;MPNN&lt;/h1&gt;

&lt;h2 id=&quot;setting&quot;&gt;Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The input to the model is an undirected graph &lt;em&gt;G&lt;/em&gt; where node features are represented as &lt;em&gt;x&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; (corresponding to node &lt;em&gt;v&lt;/em&gt;) and edge features are &lt;em&gt;e&lt;sub&gt;v, w&lt;/sub&gt;&lt;/em&gt; (corresponding to edge between nodes &lt;em&gt;v, w&lt;/em&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model can be easily extended to the setting of directed graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model works in 2 phases:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-passing-phase&quot;&gt;Message Passing Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;All nodes send a &lt;em&gt;message&lt;/em&gt; to their neighbouring nodes. The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes. The feature vectors can be combined to form the message using the &lt;em&gt;message function&lt;/em&gt; which can be implemented as a neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message. The function used to aggregate and update the feature vector is called as the &lt;em&gt;update function&lt;/em&gt; and can be implemented as a neural network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After updating the feature vectors, the graph could initiate another round of message passing. After a sufficient number of message passing rounds, the Readout phase is invoked.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;readout-phase&quot;&gt;Readout Phase&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the &lt;em&gt;readout function&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;readout function&lt;/em&gt; can also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;existing-variants-in-literature&quot;&gt;Existing Variants in literature&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;The paper provides various examples where the existing architectures could be explained in terms of the message passing framework. This includes examples like &lt;a href=&quot;https://arxiv.org/abs/1509.09292&quot;&gt;Convolutional Networks on Graphs for Learning Molecular Fingerprints&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1511.05493&quot;&gt;
Gated Graph Sequence Neural Networks&lt;/a&gt;, &lt;a href=&quot;http://tkipf.github.io/graph-convolutional-networks/&quot;&gt;Graph Convolutional Networks&lt;/a&gt; etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Broadly speaking, the task is to predict the properties of given molecules (regression problem).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Properties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two benchmarks for error:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;DFT Error - Estimated average error of DFT approximation&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Chemical Accuracy - As established by the chemistry community&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Following variants of &lt;em&gt;message function&lt;/em&gt; are explored:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Matrix multiplication between &lt;em&gt;A&lt;sub&gt;evw&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;h&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; where &lt;em&gt;A&lt;/em&gt; is the adjacency matrix &lt;em&gt;h&lt;sub&gt;v&lt;/sub&gt;&lt;/em&gt; is the feature corresponding to node &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Edge Network which is same as matrix multiplication case with the difference that &lt;em&gt;A&lt;/em&gt; is a learned matrix for each edge type.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;virtual-elements&quot;&gt;Virtual Elements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Since all messages are shared via edges, it could take a long time for the message to move between two ends of the graph. To fasten this process, virtual elements are provided.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the first setting, “virtual edges” are inserted between nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the second setting, a “master” node connects to all the other nodes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-passing-complexity&quot;&gt;Message Passing Complexity&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In a graph with &lt;em&gt;n&lt;/em&gt; nodes and &lt;em&gt;d&lt;/em&gt; dimensional feature vectors, a single step of message passing would have the worst case time complexity of &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;d&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This complexity can be reduced by breaking the &lt;em&gt;d&lt;/em&gt; dimensional embedding into &lt;em&gt;k&lt;/em&gt; different groups of &lt;em&gt;d/k&lt;/em&gt; embeddings which can be updated in parallel. The complexity of the modified approach is &lt;em&gt;O(n&lt;sup&gt;2&lt;/sup&gt;d&lt;sup&gt;2&lt;/sup&gt;/k&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Best performing MPNN model uses edge network as the &lt;em&gt;message function&lt;/em&gt; and &lt;a href=&quot;https://arxiv.org/abs/1511.06391&quot;&gt;set2set&lt;/a&gt; as the &lt;em&gt;readout function&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using group of embeddings helps to improve generalization. This effect could also be because of ensemble-like nature of the modified architecture.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model performs worse without the virtual elements.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Long range interaction between vertices is necessary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Unsupervised Learning by Predicting Noise</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Unsupervised-Learning-By-Predicting-Noise"/>
   <updated>2018-04-02T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Unsupervised Learning By Predicting Noise</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Convolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.05310&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;h3 id=&quot;unsupervised-setting&quot;&gt;Unsupervised Setting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given a collection of image X (x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, …, x&lt;sub&gt;n&lt;/sub&gt;), we want to learn a parameterized mapping &lt;em&gt;f&lt;/em&gt; such that &lt;em&gt;f(x&lt;sub&gt;i&lt;/sub&gt;)&lt;/em&gt; gives the features of image &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;. We would jointly learn the target vectors &lt;em&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; (more on it later).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Squared L2 norm is used as the distance measure while making sure that final activations are unit normalized.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fixed-target-representation&quot;&gt;Fixed Target Representation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation. No discriminative features are learned in this case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is an assumption that k &amp;gt; n so that each image is assigned a different target.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other. But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation. That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimisation&quot;&gt;Optimisation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3). An optimisation is to take a batch of b images and update the square matrix P&lt;sub&gt;B&lt;/sub&gt; for dimension bXb (made of the images and their corresponding targets). This reduces the overall complexity of O(nb^2).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-detail&quot;&gt;Implementation Detail&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Used AlexNet with NATs to train the unsupervised model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An MLP is trained on these features to learn the classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard preprocessing techniques like random cropping/flipping are used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experimental-details&quot;&gt;Experimental Details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ImageNet for training the AlexNet architecture with the proposed approach.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Pascal VOC 2007 for transfer learning experiments.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Baselines&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Unsupervised approaches like autoencoder, GAN, BiGAN&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Self-supervised&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;SOTA models using hand-made features SIFT with Fisher Vector.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observation&quot;&gt;Observation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using squared loss instead of softmax does not deteriorate the performance too much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors compare the effect of using discrete vs continuous target representations for transfer learning. For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used. Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task. The test accuracy increases up to around 100 iterations then saturate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline. But it is still far behind the model using handcrafted features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality. It is interesting to see that such a simple strategy is able to give such good results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>The Lottery Ticket Hypothesis - Training Pruned Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/The-Lottery-Ticket-Hypothesis-Training-Pruned-Neural-Networks"/>
   <updated>2018-03-25T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/The Lottery Ticket Hypothesis - Training Pruned Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Empirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper purposes a hypothesis called the &lt;em&gt;lottery ticket hypothesis&lt;/em&gt; to explain this behaviour.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is the following - Successful training of a neural network depends on a &lt;em&gt;lucky&lt;/em&gt; random initialization of a subcomponent of the network. Such components are referred to as &lt;em&gt;lottery tickets&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Larger networks are more likely to have these &lt;em&gt;lottery tickets&lt;/em&gt; and hence are easier to train.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.03635&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Various aspects of the hypothesis are explored empirically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two tasks are considered - MNIST and XOR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting network is the &lt;em&gt;lottery-ticket&lt;/em&gt; in the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size. Further, it is more likely to match the original, larger network in terms of performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores different aspects of this experiment:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Pruning Strategies:
        &lt;ul&gt;
          &lt;li&gt;One-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively.&lt;/li&gt;
          &lt;li&gt;Though the latter is computationally more intensive, it is more likely to find a lottery ticket.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Size of the pruned network affects the speed of convergence when training the &lt;em&gt;lottery ticket&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;If only the architecture or only the initial weights of the &lt;em&gt;lottery ticket&lt;/em&gt; are used, the resulting network tends to converge more slowly and achieves a lower level of performance.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;This indicates that the lottery ticket depends on both the network architecture and the weight initialization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper includes some more interesting experiments. For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again. This performance should be compared with the performance of the initial large network and the performance of the &lt;em&gt;lottery ticket&lt;/em&gt; after training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis. The proposition itself is very interesting and could enhance our understanding of how the neural networks work.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Cyclical Learning Rates for Training Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Cyclical-Learning-Rates-for-Training-Neural-Networks"/>
   <updated>2018-03-18T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Cyclical Learning Rates for Training Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Conventional wisdom says that when training neural networks, learning rate should monotonically decrease. This insight forms the basis of the different type of adaptive learning rates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also provides a simple approach to estimate the lower and upper bound for CLR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/bckenstler/CLR&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;intution&quot;&gt;Intution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Difficulty in minimizing the loss arises from saddle points and not from local minima. &lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;[Ref]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increasing the learning rate allows for rapid traversal of saddle points.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;parameter-estimation&quot;&gt;Parameter Estimation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;step_size should be set to 2-10 times the number of iterations in an epoch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Estimating the CLR boundary values:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Run the model for several epochs while increasing the learning rate between the allowed low and high values.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;This gives a good candidate value for upper and lower bound. Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound. But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.&lt;/li&gt;
  &lt;li&gt;The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Improving-Information-Extraction-by-Acquiring-External-Evidence-with-Reinforcement-Learning"/>
   <updated>2018-03-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Information Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes the use of Reinforcement Learning (RL) to solve this task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.07954&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/karthikncode/DeepRL-InformationExtraction&quot;&gt;Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-aspect&quot;&gt;Key Aspect&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use of Reinforcement Learning to resolve the ambiguity inherent in the textual documents.&lt;/li&gt;
  &lt;li&gt;Given a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine). It would further resolve and combine the result for the query from the set of retrieved documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Database of Mass Shootings in the United States.&lt;/li&gt;
  &lt;li&gt;Food Shield database of illegal food adulteration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Information extraction task is modelled as a Markov Decision Process (MDP) &amp;lt;S, A, T, R&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S&lt;/strong&gt; - Set of all possible states
    &lt;ul&gt;
      &lt;li&gt;The state consists of:
        &lt;ul&gt;
          &lt;li&gt;Extractor’s confidence in predicted entity values.&lt;/li&gt;
          &lt;li&gt;Context from which values are extracted.&lt;/li&gt;
          &lt;li&gt;Similarity between the new document (extracted just now from the search engine) and the original document accompanying the given query.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; - Set of all possible actions
    &lt;ul&gt;
      &lt;li&gt;Reconciliation decision - d
        &lt;ul&gt;
          &lt;li&gt;Accept all entities values.&lt;/li&gt;
          &lt;li&gt;Reject all entities values.&lt;/li&gt;
          &lt;li&gt;Stop the current episode.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Query choice - q
        &lt;ul&gt;
          &lt;li&gt;Choose the next query from a set of automatically generated alternatives.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;R&lt;/strong&gt; - Rewards
    &lt;ul&gt;
      &lt;li&gt;Maximise the final extraction accuracy while minimising the number of queries.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; - Queries
    &lt;ul&gt;
      &lt;li&gt;Generated using a template.&lt;/li&gt;
      &lt;li&gt;The query is searched on a search engine and the top k links are retrieved.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transition&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Start with a single source article x&lt;sub&gt;i&lt;/sub&gt; and extract the initial set of entities.&lt;/li&gt;
      &lt;li&gt;At each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q). The episode stops whenever the action is a stop action.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep Q Network is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameters are learned using SGD and RMSProp.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h2&gt;

&lt;h3 id=&quot;extraction-model&quot;&gt;Extraction Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Max Entropy Classifier is used as the base extraction system.&lt;/li&gt;
  &lt;li&gt;First, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Basic Extractors&lt;/li&gt;
  &lt;li&gt;Aggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values.&lt;/li&gt;
  &lt;li&gt;Meta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN.&lt;/li&gt;
  &lt;li&gt;Oracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rl-models&quot;&gt;RL Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RL Basic - Only reconciliation decision.&lt;/li&gt;
  &lt;li&gt;RL Query - Only query decision with a fixed reconciliation strategy.&lt;/li&gt;
  &lt;li&gt;RL Extract - the full system with both reconciliation and query decision.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RL Extract obtains substantial gains eg up to 11% over Maxnet.&lt;/li&gt;
  &lt;li&gt;Simple aggregation schemes do not handle the task well.&lt;/li&gt;
  &lt;li&gt;In terms of reward structure, providing rewards after each step works better than a single delayed reward.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/An-Empirical-Investigation-of-Catastrophic-Forgetting-in-Gradient-Based-Neural-Networks"/>
   <updated>2018-03-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Catastrophic Forgetting&lt;/em&gt; refers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper investigates this behaviour for different learning activations in presence and absence of dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6211&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/goodfeli/forgetting&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiment-formulation&quot;&gt;Experiment Formulation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For each experiment, two tasks are defined - “old” task and “new” task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The network is first trained on the “old” task until the validation set error has not improved for the last 100 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The “best” performing model is then trained for the “new” task until the combined error on the “old” and the “new” validation datasets has not improved in the last 100 epochs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All the tasks used the same model architecture - 2 hidden layers followed by a softmax layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Following activations were tested:
    &lt;ul&gt;
      &lt;li&gt;Sigmoid&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;Hard Local Winner Takes It All&lt;/li&gt;
      &lt;li&gt;Maxout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models were trained using SGD with or without dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The authors took care to keep the hyperparams and other settings consistent and comparable across different experiments. Deviations, wherever applicable, and their reasons were documented.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In terms of the relationship between the “old” and the “new” tasks, three kinds of settings are considered:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The tasks are very very similar but the input is processed in a different format. For this setting, MNIST dataset was used with a different permutation of pixels for the “old” and the “new” task.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The tasks are similar but not exactly the same. For this setting, the task was to predict sentiments of reviews across 2 different product categories.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In the last setting, 2 dissimilar tasks were used. One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Dropout improved the overall validation performance for all the models for all the tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting. It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting. The paper recommends performing cross-validation for the choice of the activation function. If that is not feasible, maxout activation function with dropout could be used.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning an SAT Solver from Single-Bit Supervision</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-a-SAT-Solver-from-Single-Bit-Supervision"/>
   <updated>2018-02-24T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning a SAT Solver from Single-Bit Supervision</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved. As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.03685&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The expression itself can be written as a conjunction of disjunctions (“and” over “or”) where each conjunct is called a clause and each variable within a clause is called a literal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Invariants&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The variables or clauses or literals (within the clauses) can be permuted.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Every occurrence of a variable can be negated.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Put an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform message passing between nodes to obtain vector representations corresponding to each node. Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings. Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After T iterations, the nodes vote to decide the prediction of the model as a whole.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is trained end-to-end using the cross-entropy loss between logit and the true label.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Permutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoding-satisfying-assignment&quot;&gt;Decoding Satisfying Assignment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the early iterations, all the nodes vote “unsolvable” with low confidence. Then a few nodes start voting “solvable” and then a phase transition happens where most of the nodes start voting “solvable” with high confidence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model never becomes highly confident that problem is “unsolvable” and almost never guesses “solvable” on an “unsolvable” problem. So in some sense, the model is looking for the combination of literals that actually solves the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Based on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The other strengths of the paper includes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Generalizing to longer and more difficult SAT problems (than those seen during training).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Generalizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the “unsatisfiability” of an expression at test time. Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Neural Relational Inference for Interacting Systems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Relational-Inference-for-Interacting-Systems"/>
   <updated>2018-02-17T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Relational Inference for Interacting Systems</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For instance, consider a simulated system where the particles are connected to each other by springs. The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.04687&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ethanfetaya/nri&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model starts by assuming that a full connected interaction graph exists between the objects in the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For this latent graph &lt;strong&gt;z&lt;/strong&gt;, &lt;em&gt;z&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; denotes the (discrete) edge type between object &lt;em&gt;v&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; with the assumption that there are &lt;em&gt;K&lt;/em&gt; edge types.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The object &lt;em&gt;v&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; has a feature vector &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;t&lt;/sup&gt;&lt;/em&gt; associated with it at time &lt;em&gt;t&lt;/em&gt;. This feature vector captures information like location and velocity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A Graph Neural Network (GNN) acts on the fully connected latent graph &lt;em&gt;z&lt;/em&gt;, performs message passing from node to node via edges and predicts the discrete label for each edge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns &lt;em&gt;p&lt;sub&gt;θ&lt;/sub&gt;(x|z)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The overall model is a VAE that optimizes the ELBO given as:&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E&lt;sub&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/sub&gt;[log p&lt;sub&gt;θ&lt;/sub&gt;(x|z)] − KL[q&lt;sub&gt;φ&lt;/sub&gt;(z|x)||p&lt;sub&gt;θ&lt;/sub&gt;(z)]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;p&lt;sub&gt;θ&lt;/sub&gt;(x)&lt;/em&gt; is the prior which is assumed to be uniform distribution over the edge types.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future. This ensures that the interactions can have a significant effect on the dynamics of the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the dynamical system, run the encoder to obtain &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sample &lt;em&gt;z&lt;sub&gt;i, j&lt;/sub&gt;&lt;/em&gt; from &lt;em&gt;q&lt;sub&gt;φ&lt;/sub&gt;(z|x)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the decoder to predict the future dynamics for the next T timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimise the ELBO loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Experiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Stylistic Transfer in Natural Language Generation Systems Using Recurrent Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Stylistic-Transfer-in-Natural-Language-Generation-Systems-Using-Recurrent-Neural-Networks"/>
   <updated>2018-02-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Stylistic Transfer in Natural Language Generation Systems Using Recurrent Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclweb.org/anthology/W/W16/W16-6010.pdf&quot;&gt;This workshop paper&lt;/a&gt; explores the problem of style transfer in natural language generation (NLG).&lt;/li&gt;
  &lt;li&gt;One possible manifestation would be rewriting technical articles in an easy-to-understate manner.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identifying relevant stylistic cues and using them to control text generation in NLG systems.&lt;/li&gt;
  &lt;li&gt;Absence of a large amount of training data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pitch&quot;&gt;Pitch&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Using Recurrent Neural Networks (RNNs) to disentangle the style from semantic content.&lt;/li&gt;
  &lt;li&gt;Autoencoder model with two components - one for learning style and another for learning content.&lt;/li&gt;
  &lt;li&gt;This allows for “style” component to be replaced while keeping the “content” component same, resulting in a style transfer.&lt;/li&gt;
  &lt;li&gt;One way to think about this is - the encoder generates a 100-dimensional vector. In this, the first 50 entries, correspond to the “style” component and remaining to the “content” component.&lt;/li&gt;
  &lt;li&gt;The proposal is that the loss function should be modified to include a cross-covariance term for ensuring disentanglement.&lt;/li&gt;
  &lt;li&gt;I think one way of doing this is to have two loss functions:
    &lt;ul&gt;
      &lt;li&gt;The &lt;strong&gt;first loss&lt;/strong&gt; function ensures that the input sentence is decoded properly into the target sentence. This loss is computed for each sentence.&lt;/li&gt;
      &lt;li&gt;The &lt;strong&gt;second loss&lt;/strong&gt; ensures that the first 50 entries across all the encoded represenations are are correlated. This loss operates at the batch level.&lt;/li&gt;
      &lt;li&gt;The &lt;strong&gt;total loss&lt;/strong&gt; is the weighted sum of these 2 losses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-datasets&quot;&gt;Possible Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://norvig.com/ngrams/shakespeare.txt&quot;&gt;Complete works of Shakespeare&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/wikichallenge/data&quot;&gt;Wikpedia Kaggle dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ota.ox.ac.uk/&quot;&gt;Oxford Text Archive&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Twitter data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;possible-metrics&quot;&gt;Possible Metrics&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Soundness - is the generated text entailed with the input sentence.&lt;/li&gt;
  &lt;li&gt;Coherence - free of grammatical errors, proper word usage etc.&lt;/li&gt;
  &lt;li&gt;Effectiveness - how effective was the style transfer&lt;/li&gt;
  &lt;li&gt;Since some of the metrics are subjective, human evaluators also need to be employed.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Get To The Point - Summarization with Pointer-Generator Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"/>
   <updated>2018-02-05T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Get To The Point-Summarization with Pointer-Generator Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f&quot;&gt;Sequence-to-Sequence models&lt;/a&gt; have made abstract summarization viable but they still suffer from issues like &lt;em&gt;out of vocabulary&lt;/em&gt; words and repetitive sentences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a &lt;em&gt;coverage&lt;/em&gt; vector that keeps track of content that has already been summarized so as to discourage repetition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/abisee/pointer-generator&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;h3 id=&quot;pointer-generator-network&quot;&gt;Pointer Generator Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is a hybrid model between the Sequence-to-Sequence network and &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Pointer-Networks&quot;&gt;Pointer Network&lt;/a&gt; such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the model can choose a word from the source vocabulary, the issue of &lt;em&gt;out of vocabulary&lt;/em&gt; words is handled.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;coverage-mechanism&quot;&gt;Coverage Mechanism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model maintains a &lt;em&gt;coverage&lt;/em&gt; vector which is the sum of attention distributions over all previous decoder timesteps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This &lt;em&gt;coverage&lt;/em&gt; vector is fed as an input to the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;coverage loss&lt;/em&gt; is added to prevent the model from repeatedly attending to the same word.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The idea is to capture how much coverage different words have already received from the attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observation&quot;&gt;Observation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model is initially trained without coverage and then finetuned with the coverage loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During training, the model first learns how to copy words and then how to generate words (p&lt;sup&gt;gen&lt;/sup&gt; starts from 0.3 and converges to 0.53).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During testing, the model strongly prefers copying over generating (p&lt;sup&gt;gen&lt;/sup&gt; = 0.17).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The overall model is very simple, neat and interpretable and also performs well in practice.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>StarSpace - Embed All The Things!</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/StarSpace-Embed-All-The-Things"/>
   <updated>2018-01-29T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/StarSpace - Embed All The Things</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A similarity function is learnt to compare these entities in a meaningful way and score their similarity. The definition of the similarity function could depend on the downstream task where the embeddings are used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.03856&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/StarSpace&quot;&gt;Link to the implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Each entity is described as a set of discrete features. For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked. For the search use case, the document may be described as a bag-of-words of words they are made up of.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a dataset and a task at hand, generate a set of positive samples &lt;em&gt;E = (a, b)&lt;/em&gt; such that &lt;em&gt;a&lt;/em&gt; is the input to the task (from the dataset) and &lt;em&gt;b&lt;/em&gt; is the expected label(answer/entity) for the given task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similarly, generate another set of negative samples &lt;em&gt;E &lt;sup&gt;-&lt;/sup&gt; = (a, b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; such that &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;&lt;/em&gt; is one of the incorrect label(answer/entity) for the given task. The incorrect entity can be sampled randomly from the set of candidate entities. Multiple incorrect samples could be generated for each positive example. These incorrect samples are indexed using &lt;em&gt;i&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, in case of supervised learning problem like document classification, &lt;em&gt;a&lt;/em&gt; would be one of the documents (probably described in terms of words), &lt;em&gt;b&lt;/em&gt; is the correct label and &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; is one of the randomly sampled label from set of all the labels (excluding the correct label).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case of collaborative filtering, &lt;em&gt;a&lt;/em&gt; would be the user (either described as a discrete entity like a userid or in terms of items purchased so far), &lt;em&gt;b&lt;/em&gt; is the next item the user purchases and &lt;em&gt;b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt; is one of the randomly sampled item from the set of all the items.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A similarity function is chosen to compare the representation of entities of type &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;. The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A loss function compares the similarity between positive pairs &lt;em&gt;(a, b)&lt;/em&gt; and &lt;em&gt;(a, b&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;)&lt;/em&gt;. The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The norm of embeddings is capped at 1.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Emotional Chatting Machine - Emotional Conversation Generation with Internal and External Memory</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Emotional-Chatting-Machine-Emotional-Conversation-Generation-with-Internal-and-External-Memory"/>
   <updated>2018-01-22T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Emotional Chatting Machine-Emotional Conversation Generation with Internal and External Memory</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.01074&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Much of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words). This is the so-called seq2seq family of models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ECM model can sit within this framework and introduces 3 new components:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Emotion Category Embedding&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Embed the emotion categories into a real-valued, low-dimensional vector space.&lt;/li&gt;
          &lt;li&gt;These embeddings are used as input to the decoder and are learnt along with rest of the model.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Internal Memory&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Physiological, emotional responses are relatively short-lived and involve changes.&lt;/li&gt;
          &lt;li&gt;ECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding.&lt;/li&gt;
          &lt;li&gt;It starts with “full” emotions in the beginning and keeps decaying the emotion value over time.&lt;/li&gt;
          &lt;li&gt;How much of the emotion value is to be decayed is determined by a sigmoid gate.&lt;/li&gt;
          &lt;li&gt;By the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;External Memory&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Emotional responses are expected to carry emotionally strong words along with generic, neutral words.&lt;/li&gt;
          &lt;li&gt;An external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies - &lt;em&gt;generic&lt;/em&gt; vocabulary and the &lt;em&gt;emotion&lt;/em&gt; vocabulary (read from the external memory).&lt;/li&gt;
          &lt;li&gt;Both these vocabularies are assigned different generation probabilities and an output gate controls the weights of &lt;em&gt;generic&lt;/em&gt; and &lt;em&gt;emotion&lt;/em&gt; words.&lt;/li&gt;
          &lt;li&gt;This way the &lt;em&gt;emotion&lt;/em&gt; words are included in an otherwise neutral response.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Loss function&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first component is the cross-entropy loss between predicted and target token distribution.&lt;/li&gt;
      &lt;li&gt;A regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process.&lt;/li&gt;
      &lt;li&gt;Another regularization term on external memory to supervise the probability of selection of a &lt;em&gt;generic&lt;/em&gt; vs &lt;em&gt;emotion&lt;/em&gt; word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;*Dataset&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;STC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier. Any error on the part of the classifier degrades the quality of the training dataset.&lt;/li&gt;
      &lt;li&gt;NLPCC Dataset - Emotion classification dataset with 23105 sentences.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Perplexity to evaluate the model at the content level.&lt;/li&gt;
      &lt;li&gt;Emotion accuracy to evaluate the model at the emotional level.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ECM achieves a perplexity of 65.9 and emotional accuracy of 0.773.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Based on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Notes&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It is an interesting idea to let the sigmoid gate decide how the emotion “value” be spent while decoding. It seems similar to the idea of how much do we want to “attend” to the emotion value the key difference being that your total attention is limited. It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step. If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Exploring Models and Data for Image Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Exploring-Models-and-Data-for-Image-Question-Answering"/>
   <updated>2018-01-14T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Exploring Models and Data for Image Question Answering</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Problem Statement&lt;/strong&gt;: Given an image, answer a given question about the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1505.02074&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vis-lstm-model&quot;&gt;VIS-LSTM Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Treat the input image as the first word in the question.&lt;/li&gt;
  &lt;li&gt;Obtain the vector representation (skip-gram) for words in the question.&lt;/li&gt;
  &lt;li&gt;Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.&lt;/li&gt;
  &lt;li&gt;Keep image embedding frozen during training and use an LSTM to combine the word vectors.&lt;/li&gt;
  &lt;li&gt;LSTM outputs are fed into a softmax layer which generates the answer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DAtaset for QUestion Ansering on Real-world images (DAQUAR)
    &lt;ul&gt;
      &lt;li&gt;1300 images and 7000 questions with 37 object classes.&lt;/li&gt;
      &lt;li&gt;Downside is that even guess work can yield good results.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The paper proposed an algorithm for generating questions using MS-COCO dataset.
    &lt;ul&gt;
      &lt;li&gt;Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;object&lt;/em&gt; questions, &lt;em&gt;number&lt;/em&gt; questions, &lt;em&gt;colour&lt;/em&gt; questions and &lt;em&gt;location&lt;/em&gt; questions can be generated by searching for nouns, numbers, colours and prepositions respectively.&lt;/li&gt;
      &lt;li&gt;Resulting dataset has ~120K questions across above 4 semantic types.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;VIS+LSTM - explained above&lt;/li&gt;
  &lt;li&gt;2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM&lt;/li&gt;
  &lt;li&gt;IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).&lt;/li&gt;
  &lt;li&gt;FULL - Simple average of above 2 models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.&lt;/li&gt;
  &lt;li&gt;Also includes a KNN model where the system finds the nearest (image, question) pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Accuracy&lt;/li&gt;
  &lt;li&gt;Wu-Palmer similarity measure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.&lt;/li&gt;
  &lt;li&gt;Some useful information seems to be lost when downsizing the VGG vectors.&lt;/li&gt;
  &lt;li&gt;Fine tuning the word vectors helps with performance.&lt;/li&gt;
  &lt;li&gt;Normalising CNN hidden image features into zero mean and unit variance leads to faster training.&lt;/li&gt;
  &lt;li&gt;Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How transferable are features in deep neural networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/How-transferable-are-features-in-deep-neural-networks"/>
   <updated>2018-01-06T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/How transferable are features in deep neural networks</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs). The first layer features are “general” irrespective of the task/optimizer etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final layer features tend to be “specific” in the sense that they strongly depend on the task.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper studies the transition of generalization property across layers in the network. This could be useful in the domain of transfer learning where features are reused across tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;setup&quot;&gt;Setup&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Degree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B). Each group has 500 classes and half the total number of examples.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now choose a layer numbered n from {1, 2…7}.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For each layer n, train the following two networks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Selffer Network BnB&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Copy (and freeze) first n layers from baseB. The remaining layers are initialized randomly and trained on B.&lt;/li&gt;
          &lt;li&gt;This serves as the control group.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Transfer Network AnB&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Copy (and freeze) first n layers from baseA. The remaining layers are initialized randomly and trained on B.&lt;/li&gt;
          &lt;li&gt;This corresponds to transferring features from A to B.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If AnB performs well, n&lt;sup&gt;th&lt;/sup&gt; layer features are “general”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In another setting, the transferred layers are also fine-tuned (BnB&lt;sup&gt;+&lt;/sup&gt; and AnB&lt;sup&gt;+&lt;/sup&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;observation&quot;&gt;Observation&lt;/h1&gt;

&lt;h2 id=&quot;dataset-a-and-b-are-similar&quot;&gt;Dataset A and B are similar&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For n = {1, 2}, the performance of the BnB model is same as baseB model. For n = {3, 4, 5, 6}, the performance of BnB model is worse.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This indicates the presence of “fragile co-adaption” features on successive layers where features interact with each other in a complex way and can not be easily separated across layers. This is more prominent across middle layers and less across the first and the last layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For model AnB, the performance of baseB for n = {1, 2}. Beyond that, the performance begins to drop.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transfer learning of features followed by fine-tuning gives better results than training the network from scratch.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-a-and-b-are-dissimilar&quot;&gt;Dataset A and B are dissimilar&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Effectiveness of feature transfer decreases as the two tasks become less similar.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;random-weights&quot;&gt;Random Weights&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Instead of using transferred weights in BnB and BnA, the first n layers were initialized randomly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The performance falls for layer 1 and 2. It further drops to near-random level for layers 3 and beyond.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another interesting insight is that even for dissimilar tasks, transferring features is better than using random features.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Distilling the Knowledge in a Neural Network</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Distilling-the-Knowledge-in-a-Neural-Network"/>
   <updated>2017-12-31T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Distilling the Knowledge in a Neural Network</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to transfer the knowledge from such “cumbersome” models into a single, “simpler” model which is more suitable for deployment. This transfer of knowledge is referred to as “distillation”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;idea&quot;&gt;Idea&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Train the cumbersome model using the given training data in the usual way.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target. Thus, the simpler model is trained to generalise the same way as the cumbersome model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model. This approach was pursued by &lt;a href=&quot;https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf&quot;&gt;Buciluǎ et al.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes a more general solution which they name “distillation”. The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer). These soft targets are then used to train the simpler model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also shows that the proposed approach is, in fact, a more general case of the first approach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model. The temperature is set to 1 when making predictions using the simpler model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels. The second objective function should be given a much lower weight though. Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper reports favourable results for distillation task for the following domains:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Image Classification (on MNIST dataset)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;An extra experiment is performed where the simpler model is not shown any images of “3” but the model fails for only 133 cases out of 1010 cases involving “3”.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Automatic Speech Recognition (ASR)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;An extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively. Further, only 3% of the total dataset is used.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;The model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy. This shows the regularizing effect of soft targets.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Training ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;The experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train).&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>PTE - Predictive Text Embedding through Large-scale Heterogeneous Text Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/PTE-Predictive-Text-Embedding-through-Large-scale-Heterogeneous-Text-Networks"/>
   <updated>2017-12-24T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/PTE - Predictive Text Embedding through Large-scale Heterogeneous Text Networks</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised text embeddings can be generalized for different tasks but they have weaker predictive powers (as compared to end-to-end trained deep learning methods) for any particular task. But the deep learning techniques are expensive and need a large amount of supervised data and a large number of parameters to tune.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces Predictive Text Embedding (PTE) - a semi-supervised approach which learns an effective low dimensional representation using a large amount of unsupervised data and a small amount of supervised data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The work can be extended to general information networks as well as classic techniques like MDS, Iso-map, Laplacian EigenMaps etc do not scale well for large graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, this model can be applied to heterogeneous networks as well unlike the previous works &lt;a href=&quot;https://arxiv.org/abs/1503.03578&quot;&gt;LINE&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1403.6652&quot;&gt;DeepWalk&lt;/a&gt; which work on homogeneous networks only.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.00200&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes 3 different kinds of networks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Word-Word Network&lt;/strong&gt; which captures the word co-occurrence information (local level).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Word-Document Network&lt;/strong&gt; which captures the word-document co-occurrence information (local + document level).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Word-Label Network&lt;/strong&gt; which captures the word-label co-occurrence information (bipartite graph).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All 3 graphs are integrated into one heterogeneous text network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;First, the authors extend their previous work, LINE, for heterogenous bipartite text networks as explained:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given a bipartite graph &lt;em&gt;G = (V&lt;sub&gt;A&lt;/sub&gt; \bigcup V&lt;sub&gt;B&lt;/sub&gt;, E)&lt;/em&gt; , where &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt; and V&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; are disjoint set of vertices, the conditional probability of &lt;em&gt;v&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; (in set &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt;&lt;/em&gt;) being generated by &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; (in set &lt;em&gt;V&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;) is given as the softmax score between embeddings of &lt;em&gt;v&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; and normalised by the sum of exponentials of dot products between &lt;em&gt;v&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt;  and all nodes in &lt;em&gt;V&lt;sub&gt;A&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;The second order proximity can be determined by the conditional distributions *p(.&lt;/td&gt;
              &lt;td&gt;v&lt;sub&gt;j&lt;/sub&gt;)*p(.&lt;/td&gt;
              &lt;td&gt;v&lt;sub&gt;j&lt;/sub&gt;)*.&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The objective to be minimised the KL divergence between the conditional distribution &lt;em&gt;p(.\v&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; and the emperical distribution &lt;em&gt;p&lt;sup&gt;^&lt;/sup&gt;(.\v&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; (given as w&lt;sub&gt;i, j&lt;/sub&gt;/deg&lt;sub&gt;j&lt;/sub&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;The objective can be further simplified and optimised using SGD with edge sampling and negative sampling.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now, the 3 individual networks can all be interpreted as bipartite networks. So node representation of all the 3 individual networks is obtained as described above.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the word-label network, since the training data is sparse, one could either train the unlabelled networks first and then the labelled network or they all could be trained jointly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the case of joint training, the edges are sampled from the 3 networks alternatively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the fine-tuning case, the edges are first sampled from the unlabelled network and then from the labelled network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once the word embeddings are obtained, the text embeddings may be obtained by simply averaging the word embeddings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Baseline Models&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Local word co-occurence based methods - SkipGram, LINE(Gww)&lt;/li&gt;
      &lt;li&gt;Document word co-occurence based methods - LINE(Gwd), PV-DBOW&lt;/li&gt;
      &lt;li&gt;Combined method - LINE (Gww + Gwd)&lt;/li&gt;
      &lt;li&gt;CNN&lt;/li&gt;
      &lt;li&gt;PTE&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For long documents, PTE (joint) outperforms CNN and other PTE variants and is around 10 times faster than CNN model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For short documents, PTE (joint) does not always outperform CNN model probably because the word sense ambiguity is more relevant in the short documents.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Revisiting Semi-Supervised Learning with Graph Embeddings</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Revisiting-Semi-Supervised-Learning-with-Graph-Embeddings"/>
   <updated>2017-12-11T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Revisiting Semi-Supervised Learning with Graph Embeddings</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context. Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The framework is called &lt;a href=&quot;https://github.com/kimiyoung/planetoid&quot;&gt;Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.08861&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-setting&quot;&gt;Problem Setting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph G = (V, E) and x&lt;sub&gt;L&lt;/sub&gt; and x&lt;sub&gt;U&lt;/sub&gt; as feature vectors for labelled and unlabelled nodes and y&lt;sub&gt;L&lt;/sub&gt; as labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -&amp;gt; y&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two settings possible:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Transductive&lt;/strong&gt; - Predictions are made only for those nodes which are already observed in the graph at training time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Inductive&lt;/strong&gt; - Predictions are made for nodes whether they have been observed in the graph at training time or not.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The general semi-supervised learning loss would be &lt;em&gt;L&lt;sub&gt;S&lt;/sub&gt; + λL&lt;sub&gt;U&lt;/sub&gt;&lt;/em&gt; where &lt;em&gt;L&lt;sub&gt;S&lt;/sub&gt;&lt;/em&gt; is the supervised learning loss while &lt;em&gt;L&lt;sub&gt;U&lt;/sub&gt;&lt;/em&gt; is the unsupervised learning loss.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The unsupervised loss is a variant of the Skip-gram loss with negative edge sampling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, first a random walk sequence S is sampled. Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transductive-formulation&quot;&gt;Transductive Formulation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two separate fully connected networks are applied over the node features and node embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;These 2 representations are then concatenated and fed to a softmax classifier to predict the class label.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inductive-formulation&quot;&gt;Inductive Formulation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the inductive setting, it is difficult to obtain the node embeddings at test time. One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This provides a simple way to extend the original approach to the inductive setting.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension"/>
   <updated>2017-11-28T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is the following:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;We have a domain D&lt;sub&gt;S&lt;/sub&gt; for which we have labelled dataset of question-answer pairs and another domain D&lt;sub&gt;T&lt;/sub&gt; for which we do not have any labelled dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We use the data for domain D&lt;sub&gt;S&lt;/sub&gt; to train SynNet and use that to generate synthetic question-answer pairs for domain D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Now we can train a machine comprehension model M on D&lt;sub&gt;S&lt;/sub&gt; and finetune using the synthetic data for D&lt;sub&gt;T&lt;/sub&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/two-stage-synthesis-networks-transfer-learning-machine-comprehension/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;synnet&quot;&gt;SynNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Works in two stages:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Answer Synthesis - Given a text paragraph, generate an answer.&lt;/li&gt;
      &lt;li&gt;Question Synthesis - Given a text paragraph and an answer, generate a question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;answer-synthesis-network&quot;&gt;Answer Synthesis Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given the labelled dataset for D&lt;sub&gt;S&lt;/sub&gt;, generate a labelled dataset of &amp;lt;word, tag&amp;gt; pair such that each word in the given paragraph is assigned one of the 4 tags:
    &lt;ul&gt;
      &lt;li&gt;IOB&lt;sub&gt;start&lt;/sub&gt; - if it is the starting word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;mid&lt;/sub&gt; - if it is the intermediate word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;end&lt;/sub&gt; - if it is the ending word of an answer&lt;/li&gt;
      &lt;li&gt;IOB&lt;sub&gt;none&lt;/sub&gt; - if it is not part of any answer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For training, map the words to their GloVe embeddings and pass through a Bi-LSTM. Next, pass them through two-FC layers followed by a softmax layer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For the target domain D&lt;sub&gt;T&lt;/sub&gt;, all the consecutive word spans where no label is IOB&lt;sub&gt;none&lt;/sub&gt; are returned as candidate answers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;question-synthesis-network&quot;&gt;Question Synthesis Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Map each word in the paragraph to their GloVe embedding. After the word vector, append a ‘1’ if the word was part of the candidate answer else append a ‘0’.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far. Decoding is stopped when “END” token is produced.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary. To account for such words, a copying mechanism is also incorporated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At each time step, a Pointer Network (C&lt;sub&gt;P&lt;/sub&gt;) and a Vocabulary Predictor (V&lt;sub&gt;P&lt;/sub&gt;) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;machine-comprehension-model&quot;&gt;Machine Comprehension Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given any MC model, first train it over domain D&lt;sub&gt;S&lt;/sub&gt; and then fine-tune using the artificial questions generated using D&lt;sub&gt;T&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Regularization&lt;/strong&gt; - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Checkpoint Averaging&lt;/strong&gt; - Use the different checkpointed models to average the answer likelihood before running DP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -&amp;gt; NewsQA). Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Higher-order organization of complex networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Higher-order-organization-of-complex-networks"/>
   <updated>2017-11-19T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Higher-order organization of complex networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://science.sciencemag.org/content/353/6295/163&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/Network-Motifs-Simple-Building-Blocks-of-Complex-Networks&quot;&gt;motif M&lt;/a&gt;, the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mathematically, the aim is to minimise the motif conductance metric given as &lt;em&gt;cut&lt;sub&gt;M&lt;/sub&gt;(S, S’) / min[vol&lt;sub&gt;M&lt;/sub&gt;(S), vol&lt;sub&gt;M&lt;/sub&gt;(S’)]&lt;/em&gt; where &lt;em&gt;S’&lt;/em&gt; is complement of &lt;em&gt;S&lt;/em&gt;, &lt;em&gt;cut&lt;sub&gt;M&lt;/sub&gt;(S, S’)&lt;/em&gt; = number of instances of M which have atleast one node from both &lt;em&gt;S&lt;/em&gt; and &lt;em&gt;S’&lt;/em&gt; and &lt;em&gt;vol&lt;sub&gt;M&lt;/sub&gt;(S)&lt;/em&gt; = Number of nodes in instances of M that belong only to S.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the network and motif M, form a motif adjacency matrix W&lt;sub&gt;M&lt;/sub&gt; where W&lt;sub&gt;M&lt;/sub&gt;(i, j) is the number of instances of M that contains i and j.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute spectral ordering of the nodes from normalized motif laplacian matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute prefix set of spectral ordering with small motif conductance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Worst case &lt;em&gt;O(m&lt;sup&gt;1.5&lt;/sup&gt;)&lt;/em&gt;, based on experiments &lt;em&gt;O(m&lt;sup&gt;1.2&lt;/sup&gt;)&lt;/em&gt; where &lt;em&gt;m&lt;/em&gt; is the number of edges.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case the motif is not known beforehand, the framework can be used to compute significant motifs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Network Motifs - Simple Building Blocks of Complex Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Network-Motifs-Simple-Building-Blocks-of-Complex-Networks"/>
   <updated>2017-11-12T00:00:00-05:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Network Motifs-Simple Building Blocks of Complex Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper presents the concept of “network motifs” to understand the structural design of a network or a graph.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://science.sciencemag.org/content/298/5594/824&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A network motif is defined as “a pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Real-life networks exhibit properties like “small world” property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and “scale-free” property (fraction of nodes having k edges decays as a power-law).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Motifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering. Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Word Representations via Gaussian Embedding</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Word-Representations-via-Gaussian-Embedding"/>
   <updated>2017-11-05T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Word Representations via Gaussian Embedding</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Existing word embedding models like &lt;a href=&quot;https://gist.github.com/shagunsodhani/176a283e2c158a75a0a6&quot;&gt;Skip-Gram&lt;/a&gt;, &lt;a href=&quot;https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8&quot;&gt;GloVe&lt;/a&gt; etc map words to fixed sized vectors in a low dimensional vector space.&lt;/li&gt;
  &lt;li&gt;This fixed point setting cannot capture uncertainty about representation.&lt;/li&gt;
  &lt;li&gt;Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.&lt;/li&gt;
  &lt;li&gt;The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.&lt;/li&gt;
  &lt;li&gt;This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6623&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/seomoz/word2gauss&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;KL divergence is used as the asymmetric distance function for comparing the distributions.&lt;/li&gt;
  &lt;li&gt;Unlike the Word2Vec model, the proposed model uses ranking-based loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;similarity-measures-used&quot;&gt;Similarity Measures used&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Symmetric Similarity&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For two gaussian distributions, &lt;em&gt;P&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;P&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;, compute the inner product &lt;em&gt;E(P&lt;sub&gt;i&lt;/sub&gt;, P&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt; as &lt;em&gt;N(0; mean&lt;sub&gt;i&lt;/sub&gt; - mean&lt;sub&gt;j&lt;/sub&gt;, sigma&lt;sub&gt;i&lt;/sub&gt; + sigma&lt;sub&gt;j&lt;/sub&gt;)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Compute the gradient of &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;sigma&lt;/em&gt; with respect to &lt;em&gt;log(E)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Asymmetric Similarity&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Use KL divergence to encode the context distribution.&lt;/li&gt;
  &lt;li&gt;The benefit over the symmetric setting is that now entailment type relations can also be modeled.&lt;/li&gt;
  &lt;li&gt;For example, a low KL divergence from x to y indicates that y can be encoded as x or that y “entails” x.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning&quot;&gt;Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;One of the two notions of similarity is chosen and max-margin is used as the loss function.&lt;/li&gt;
  &lt;li&gt;Mean is regularized by adding a simple constraint on the L2-norm.&lt;/li&gt;
  &lt;li&gt;For covariance matrix, the eigenvalues are constrained to lie within a hypercube. This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Polysemous words have higher variance in their word embeddings as compared to specific words.&lt;/li&gt;
  &lt;li&gt;KL divergence (with diagonal covariance) outperforms other models.&lt;/li&gt;
  &lt;li&gt;Simple tree hierarchies can also be modeled by embedding into the Gaussian space. A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.&lt;/li&gt;
  &lt;li&gt;For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use combinations of low rank and diagonal matrices for covariances.&lt;/li&gt;
  &lt;li&gt;Improved optimisation strategies.&lt;/li&gt;
  &lt;li&gt;Trying other distributions like Student’s-t distribution.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>HARP - Hierarchical Representation Learning for Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/HARP-Hierarchical-Representation-Learning-for-Networks"/>
   <updated>2017-10-28T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/HARP - Hierarchical Representation Learning for Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.07845&quot;&gt;Link to the paper&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Given a graph &lt;em&gt;G = (V, E)&lt;/em&gt;, compute a series of successively smaller (coarse) graphs &lt;em&gt;G&lt;sub&gt;0&lt;/sub&gt;, …, G&lt;sub&gt;L&lt;/sub&gt;&lt;/em&gt;. Learn the node representations in &lt;em&gt;G&lt;sub&gt;L&lt;/sub&gt;&lt;/em&gt; and successively refine the embeddings for larger graphs in the series.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The architecture is independent of the algorithms used to embed the nodes or to refine the node representations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graph coarsening technique that preserves global structure&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Collapse edges and stars to preserve first and second order proximity.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Edge collapsing&lt;/strong&gt; - select the subset of &lt;em&gt;E&lt;/em&gt; such that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Star collapsing&lt;/strong&gt; - given star structure, collapse the pairs of neighboring nodes (of the central node).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;In practice, first apply star collapsing, followed by edge collapsing.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Extending node representation from coarse graph to finer graph&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Lets say &lt;em&gt;node1&lt;/em&gt; and &lt;em&gt;node2&lt;/em&gt; were merged into &lt;em&gt;node12&lt;/em&gt; during coarsening. First copy the representation of &lt;em&gt;node12&lt;/em&gt; into &lt;em&gt;node1&lt;/em&gt;, &lt;em&gt;node2&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Additionally, if hierarchical softmax was used, extend the B-tree such that &lt;em&gt;node12&lt;/em&gt; is replaced by 2 child nodes &lt;em&gt;node1&lt;/em&gt; and &lt;em&gt;node2&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Time complexity for HARP + DeepWalk is &lt;em&gt;O(number of walks * |V|)&lt;/em&gt; while for HARP + LINE is &lt;em&gt;O(number of iterations * |E|)&lt;/em&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The asymptotic complexity remains the same as the HARP-less version for the two cases.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Swish - a Self-Gated Activation Function</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Swish-A-self-gated-activation-function"/>
   <updated>2017-10-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Swish-A self gated activation function</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a new activation function called Swish with formulation &lt;em&gt;f(x) = x.sigmod(x)&lt;/em&gt; and its parameterised version called Swish-β where &lt;em&gt;f(x, β) = 2x.sigmoid(β.x)&lt;/em&gt; and β is a training parameter.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.05941&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;properties-of-swish&quot;&gt;Properties of Swish&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/shagunsodhani/papers-I-read/master/assets/Swish/plot.png&quot; alt=&quot;Plot Of Swish&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smooth, non-monotonic function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Swish-β can be thought of as a smooth function that interpolates between a linear function and RELU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Uses self-gating mechanism (that is, it uses its own value to gate itself). Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Being unbounded on the x&amp;gt;0 side, it avoids saturation when training is slow due to near 0 gradients.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the Swish function is smooth, the output landscape and the loss landscape are also smooth. A smooth landscape should be more traversable and less sensitive to initialization and learning rates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;criticism&quot;&gt;Criticism&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Reading Wikipedia to Answer Open-Domain Questions</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Reading-Wikipedia-to-Answer-Open-Domain-Questions"/>
   <updated>2017-10-15T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Reading Wikipedia to Answer Open-Domain Questions</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.00051&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unique-aspects-of-the-dataset&quot;&gt;Unique Aspects of the dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Existing machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans). MARCO questions are sampled from real, anonymized user queries.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most datasets would provide a comparatively small and clean context to answer the question. In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents. As such the questions and the context documents are noisy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In general, the answer to the questions are restricted to an entity or text span within the document. In case of MARCO, the human judges are encouraged to generate complete sentences as answers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset-description&quot;&gt;Dataset Description&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First release consists of 100K questions with the aim of releasing 1M questions in the future releases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;All questions are tagged with segment information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A subset of questions has multiple answers and another subset has no answers at all.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each record in the dataset contains the following information:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Query&lt;/strong&gt; - The actual question&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Passage&lt;/strong&gt; - Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Document URLs&lt;/strong&gt; - URLs for the top documents (which are the source of the contextual passages).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Answer&lt;/strong&gt; - Answer synthesised by human evaluators.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Segment&lt;/strong&gt; - Query type, description, neumeric, entity, location, person.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Accuracy and precision/recall for numeric questions&lt;/li&gt;
      &lt;li&gt;ROGUE-L/paraphrasing aware evaluation framework for long, textual answers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Among generative models, Memory Networks performed better than seq-to-seq.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the cloze-style test, &lt;a href=&quot;https://arxiv.org/abs/1609.05284&quot;&gt;ReasoNet&lt;/a&gt; achieved an accuracy of approx. 59% while &lt;a href=&quot;ASR&quot;&gt;Attention Sum Reader&lt;/a&gt; achieved an accuracy of approx 55%.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Current QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Imagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy. Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition. Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Task-Oriented Query Reformulation with Reinforcement Learning</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Task-Oriented-Query-Reformulation-with-Reinforcement-Learning"/>
   <updated>2017-10-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Task-Oriented Query Reformulation with Reinforcement Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper introduces a query reformulation system that rewrites a query to maximise the number of “relevant” documents that are extracted from a given black box search engine.&lt;/li&gt;
  &lt;li&gt;A Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04572&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nyu-dl/QueryReformulator&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-aspect&quot;&gt;Key Aspect&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval. This means relevant documents could be missed if there is no exactly matching words between the query and the document.&lt;/li&gt;
  &lt;li&gt;This problem can be handled at two levels: First, the search engine itself takes care of query semantics. Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation).&lt;/li&gt;
  &lt;li&gt;The paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;TREC - Complex Answer Retrieval (TREC-CAR)&lt;/li&gt;
  &lt;li&gt;Jeopardy Q&amp;amp;A dataset&lt;/li&gt;
  &lt;li&gt;Microsoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;framework&quot;&gt;Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Query Reformulation task is modeled as an RL problem where:
    &lt;ul&gt;
      &lt;li&gt;Environment is the search engine.&lt;/li&gt;
      &lt;li&gt;Actions are whether a word is to be added to the query or not and if yes, then what word is added.&lt;/li&gt;
      &lt;li&gt;Reward is the retrieval accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The input to the system is a query q&lt;sub&gt;0&lt;/sub&gt; consisting of a sequence of words w&lt;sub&gt;1&lt;/sub&gt;, …, w&lt;sub&gt;n&lt;/sub&gt; and a candidate term t&lt;sub&gt;i&lt;/sub&gt; with some context words.&lt;/li&gt;
  &lt;li&gt;Candidate terms are all the terms that appear in the original query and the documents retrieved using the query.&lt;/li&gt;
  &lt;li&gt;The words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN’s or RNNs.&lt;/li&gt;
  &lt;li&gt;Similarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs.&lt;/li&gt;
  &lt;li&gt;Finally, a sigmoidal score is computed for all the candidate words.&lt;/li&gt;
  &lt;li&gt;An RNN sequentially applies this model to emit query words till an end token is emitted.&lt;/li&gt;
  &lt;li&gt;Vocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The model is trained using REINFORCE algorithm which minimizes the &lt;em&gt;C&lt;sub&gt;a&lt;/sub&gt; = (R − R~) * sum(log(P(t|q))) where R~ is the baseline.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Value network minimises &lt;em&gt;C&lt;sub&gt;b&lt;/sub&gt; = &amp;amp;\alpha(||R-R~||&lt;sup&gt;2&lt;/sup&gt;)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;C&lt;sub&gt;a&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;C&lt;sub&gt;b&lt;/sub&gt;&lt;/em&gt; are minimised using SGD.&lt;/li&gt;
  &lt;li&gt;An entropy regulation term is added to prevent the probability distribution from reaching the peak.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;baseline-methods&quot;&gt;Baseline Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Raw&lt;/strong&gt; - Original query is fed to the search engine without any modification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pseudo-Relevance Feedback (PRF-TFIDF)&lt;/strong&gt; - The query is expanded using the top-N TF-IDF terms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PRF-Relevance Model (PRF-RM)&lt;/strong&gt; - Probability of adding token &lt;em&gt;t&lt;/em&gt; to the query &lt;em&gt;q0&lt;/em&gt; is given by &lt;em&gt;P(t|q0) = (1 − λ)P′(t|q0) + λ sum (P(d)P(t|d)P(q0|d))&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-methods&quot;&gt;Proposed Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Assumes that the query words contribute indepently to the query retrival performace. (Too strong an assumption).&lt;/li&gt;
      &lt;li&gt;A term is marked as relevant if &lt;em&gt;(R(new_query) - R(old_query))/R(old_query) &amp;gt; 0.005&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;RL-RNN/CNN - RL Framework + RNN/CNN to encode the input features.&lt;/li&gt;
      &lt;li&gt;RL-RNN-SEQ - Add a sequential generator.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall@K&lt;/li&gt;
      &lt;li&gt;Precision@K&lt;/li&gt;
      &lt;li&gt;Mean Average Precision@K&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt; - The paper uses Recall@K as a reward when training the RL-based models with the argument that the “metric has shown to be effective in improving the other metrics as well”, without any justification though.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SL-Oracle&lt;/strong&gt; - classifier that perfectly selects terms that will increase performance based on the supervised learning approach.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RL-Oracle&lt;/strong&gt; - Produces a conservative upper-bound for the performance of the RL Agent. It splits the test data into N subsets and trains an RL agent for each subset. Then, the reward is averaged over all the N subsets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Reformulation based methods &amp;gt; original query&lt;/li&gt;
  &lt;li&gt;RL methods &amp;gt; Supervised methods &amp;gt; unsupervised methods&lt;/li&gt;
  &lt;li&gt;RL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries).&lt;/li&gt;
  &lt;li&gt;RL-based model benefits from more candidate terms while the classical PRF method quickly saturates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Interestingly, for each raw query, they carried out the reformulation step just once and not multiple times. The number of times a query is reformulated could also have become a part of the RL framework.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Refining Source Representations with Relation Networks for Neural Machine Translation</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Refining-Source-Representations-with-Relation-Networks-for-Neural-Machine-Translation"/>
   <updated>2017-09-22T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Refining Source Representations with Relation Networks for Neural Machine Translation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).&lt;/li&gt;
  &lt;li&gt;This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.03980&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations-of-existing-nmt-models&quot;&gt;Limitations of existing NMT models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The RNN encoder-decoder architecture is the standard choice for NMT systems. But the RNNs are prone to forgetting old information.&lt;/li&gt;
  &lt;li&gt;In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.&lt;/li&gt;
  &lt;li&gt;While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contributions-of-the-paper&quot;&gt;Contributions of the paper&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Learn the relationship between the source words using the context (neighboring words).&lt;/li&gt;
  &lt;li&gt;Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs. The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relation-network&quot;&gt;Relation Network&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Neural network which is desgined for relational reasoning.&lt;/li&gt;
  &lt;li&gt;Given a set of inputs * O = o&lt;sub&gt;1&lt;/sub&gt;, …, o&lt;sub&gt;n&lt;/sub&gt; *, RN is formed as a composition of inputs:
   RN(O) = f(sum(g(o&lt;sub&gt;i&lt;/sub&gt;, o&lt;sub&gt;j&lt;/sub&gt;))), f and g are functions used to learn the relations (feed forward networks)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;g&lt;/em&gt; learns how the objects are related hence the name “relation”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;CNN Layer
        &lt;ul&gt;
          &lt;li&gt;Extract information from the words surrounding the given word (context).&lt;/li&gt;
          &lt;li&gt;The final output of this layer is the sequence of vectors for different kernel width.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Graph Propagation (GP) Layer
        &lt;ul&gt;
          &lt;li&gt;Connect all the words with each other in the form of a graph.&lt;/li&gt;
          &lt;li&gt;Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.&lt;/li&gt;
          &lt;li&gt;The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Multi-Layer Perceptron (MLP) Layer
        &lt;ul&gt;
          &lt;li&gt;The representation from the GP Layer is fed to the MLP layer.&lt;/li&gt;
          &lt;li&gt;The layer uses residual connections from previous layers in form of concatenation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;IWSLT Data - 44K sentences from tourism and travel domain.&lt;/li&gt;
  &lt;li&gt;NIST Data - 1M Chinese-English parallel sentence pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MOSES - Open source translation system - http://www.statmt.org/moses/&lt;/li&gt;
  &lt;li&gt;NMT - Attention based NMT&lt;/li&gt;
  &lt;li&gt;NMT+ - NMT with improved decoder&lt;/li&gt;
  &lt;li&gt;TRANSFORMER - Google’s new NMT&lt;/li&gt;
  &lt;li&gt;RNMT+ - Relation Network integrated with NMT+&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation-metric&quot;&gt;Evaluation Metric&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;case-insensitive 4-gram BLEU score&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;As sentences become larger (more than 50 words), RNMT clearly outperforms other baselines.&lt;/li&gt;
  &lt;li&gt;Qualitative evaluation shows that RNMT+ model captures the word alignment better than the NMT+ models.&lt;/li&gt;
  &lt;li&gt;Similarly, NMT+ system tends to miss some information from the source sentence (more so for longer sentences). While both CNNs and RNNs are weak at capturing long-term dependency, using the relation layer mitigates this issue to some extent.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Pointer Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Pointer-Networks"/>
   <updated>2017-08-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Pointer Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Such a problem can not be solved using &lt;a href=&quot;https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f&quot;&gt;Seq2Seq&lt;/a&gt; or Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Traditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector. This attention vector is used to compute a fixed size softmax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;So Pointer Net is a very simple modification of the attention model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;application&quot;&gt;Application&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Any problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;eg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The paper considers the following 3 problems:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Convex Hull&lt;/li&gt;
      &lt;li&gt;Delaunay triangulations&lt;/li&gt;
      &lt;li&gt;Travelling Salesman Problem (TSP)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, the order in which the inputs are fed to the system affects its performance. The authors discussed this apsect in their subsequent paper titled &lt;a href=&quot;https://arxiv.org/pdf/1511.06391v4.pdf&quot;&gt;Order Matters: Sequence To Sequence for Sets&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Learning to Compute Word Embeddings On the Fly</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Learning-to-Compute-Word-Embeddings-On-the-Fly"/>
   <updated>2017-08-21T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Learning to Compute Word Embeddings On the Fly</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning representations for OOV words directly on the end task often results in poor representation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a rare word &lt;em&gt;w&lt;/em&gt;, let &lt;em&gt;d(w) = &amp;lt;x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;…&amp;gt;&lt;/em&gt; denote its defination where &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; are words.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;d(w)&lt;/em&gt; is fed to a &lt;em&gt;defination reader&lt;/em&gt; network &lt;em&gt;f&lt;/em&gt; (LSTM) and its last state is used as the &lt;em&gt;defination embedding e&lt;sub&gt;d&lt;/sub&gt;(w)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In case &lt;em&gt;w&lt;/em&gt; has multiple definitions, the embeddings are combined using mean pooling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The approach can be extended to in-vocabulary words as well by using the &lt;em&gt;definition embedding&lt;/em&gt; of such words to update their original embeddings.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Auxiliary data sources
    &lt;ul&gt;
      &lt;li&gt;Word definitions from WordNet&lt;/li&gt;
      &lt;li&gt;Spelling of words&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The proposed approach was tested on following tasks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Extractive Question Answering over SQuAD
        &lt;ul&gt;
          &lt;li&gt;Base model from &lt;a href=&quot;https://arxiv.org/abs/1611.01604&quot;&gt;Xiong et al. 2016&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Entailment Prediction over SNLI corpus
        &lt;ul&gt;
          &lt;li&gt;Base models from &lt;a href=&quot;https://nlp.stanford.edu/pubs/snli_paper.pdf&quot;&gt;Bowman et al. 2015&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1609.06038&quot;&gt;Chen et al. 2016&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;One Billion Words Language Modelling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-token words like “San Francisco” are not accounted for now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model does not handle the rare words which appear in the definition and just replaces them by the &lt;UNK&gt; token. Making the model recursive would be a useful addition.&lt;/UNK&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>R-NET - Machine Reading Comprehension with Self-matching Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/R-NET-Machine-Reading-Comprehension-with-Self-matching-Networks"/>
   <updated>2017-08-07T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/R-NET - Machine Reading Comprehension with Self-matching Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;R-NET is an end-to-end trained neural network model for machine comprehension.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lastly, it uses pointer networks to determine the position of the answer in the passage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/mrc/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MS-MARCO&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Question / Passage Encoder&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Concatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gated Attention based RNN&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Given question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Self Matching Attention&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The passage representation obtained so far would not capture most of the context.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;So the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output Layer&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Use pointer network (initialized using attention pooling over answer representation) to predict the position of the answer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Loss function is the sum of negative log probabilities of start and end positions.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Results&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;R-NET is ranked second on &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD Leaderboard&lt;/a&gt; as of 7th August, 2017 and achieves best-published results on MS-MARCO dataset.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Using ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>ReasoNet - Learning to Stop Reading in Machine Comprehension</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/ReasoNet-Learning-to-Stop-Reading-in-Machine-Comprehension"/>
   <updated>2017-07-24T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/ReasoNet - Learning to Stop Reading in Machine Comprehension</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached. If termination state is reached, the answer module is triggered to generate the answer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since the termination state is discrete and not connected to the final output, RL approach is used.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.05284&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNN, DailyMail Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQuAD&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Graph Reachability Dataset&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;2 synthetic datasets to test if the network can answer questions like “Is node_1 connected to node_12”?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Memory (M)&lt;/strong&gt; - Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; - Attention vector (&lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;) is a function of current internal state &lt;strong&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; and external memory &lt;strong&gt;M&lt;/strong&gt;. The state and memory are passed through FCs and fed to a similarity function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Internal State (s&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; - Vector representation of the question state computed by a RNN using the previous internal state and the attention vector &lt;strong&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Termination Gate (T&lt;sub&gt;t&lt;/sub&gt;)&lt;/strong&gt; - Uses a logistic regression model to generate a random binary variable using the current internal state &lt;strong&gt;s&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Answer&lt;/strong&gt; - Answer module is triggered when &lt;strong&gt;T&lt;sub&gt;t&lt;/sub&gt; = 1&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;For CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities.&lt;/li&gt;
      &lt;li&gt;For SQuAD, the position of the first and the last word from the answer span are predicted.&lt;/li&gt;
      &lt;li&gt;For Graph Reachability, a logistic regression module is used to predict yes/no as the answer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt; - For the RL setting, reward at time &lt;strong&gt;t&lt;/strong&gt;, &lt;strong&gt;r&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; = 1 if &lt;strong&gt;T&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt; = 1 and answer is correct. Otherwise &lt;strong&gt;r&lt;sub&gt;t&lt;/sub&gt; = 0&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Workflow&lt;/strong&gt; - Given a passage p, query q and answer a:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Extract memory using p&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Extract initial hidden state using q&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;These episodes generate actions and answers that are used to train the ReasoNet.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;CNN, DailyMail Corpus&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;SQuAD&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;At the time of submission, ReasoNet was ranked 2nd on the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD leaderboard&lt;/a&gt; and as of 9th July 2017, it is ranked 4th.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Graph Reachability Dataset&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet - Standard ReasoNet as described above.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet-Last - Use the prediction from the &lt;strong&gt;T&lt;sub&gt;max&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet &amp;gt; ReasoNet-Last &amp;gt; Deep LSTM Reader&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;ReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;As such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage.&lt;/li&gt;
      &lt;li&gt;In fact, the modal value of the number of passes = upper bound on the number of passes.&lt;/li&gt;
      &lt;li&gt;This effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes.&lt;/li&gt;
      &lt;li&gt;It would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Principled Detection of Out-of-Distribution Examples in Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Principled-Detection-of-Out-of-Distribution-Examples-in-Neural-Networks"/>
   <updated>2017-07-17T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Principled Detection of Out of Distribution Examples in Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper proposes &lt;em&gt;ODIN&lt;/em&gt; which can detect such out-of-distribution examples without changing the pre-trained model itself.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.02690&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;odin&quot;&gt;ODIN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Uses 2 major techniques&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Temperature Scaling&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Softmax classifier for the classification network can be written as:&lt;/p&gt;

            &lt;p&gt;&lt;em&gt;p&lt;sub&gt;i&lt;/sub&gt;(x, T) = exp(f&lt;sub&gt;i&lt;/sub&gt;(x)/T) / sum(exp(f&lt;sub&gt;j&lt;/sub&gt;(x)/T))&lt;/em&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;where &lt;em&gt;x&lt;/em&gt; is the input, &lt;em&gt;p&lt;/em&gt; is the softmax probability and &lt;em&gt;T&lt;/em&gt; is the temperature scaling parameter.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Increasing &lt;em&gt;T&lt;/em&gt; (up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Input Preprocessing&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Add small perturbations to the input (image) before feeding it into the network.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;em&gt;x_perturbed = x - ε * sign(-δ&lt;sub&gt;x&lt;/sub&gt;log(p&lt;sub&gt;y&lt;/sub&gt;(x, T)))&lt;/em&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;where ε is the perturbation magnitude&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;The perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Given an input (image), first perturb the input.&lt;/li&gt;
  &lt;li&gt;Feed the perturbed input to the network to get its softmax score.&lt;/li&gt;
  &lt;li&gt;If the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification.&lt;/li&gt;
  &lt;li&gt;Otherwise, mark the input as out-of-distribution.&lt;/li&gt;
  &lt;li&gt;For detailed mathematical treatment, refer section 6 and appendix in the &lt;a href=&quot;https://arxiv.org/abs/1706.02690&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Code available on &lt;a href=&quot;https://github.com/ShiyuLiang/odin-pytorch&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Models&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;DenseNet with depth L = 100 and growth rate k = 12&lt;/li&gt;
      &lt;li&gt;Wide ResNet with depth = 28 and widen factor = 10&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In-Distribution Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;CIFAR-10&lt;/li&gt;
      &lt;li&gt;CIFAR-100&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Out-of-Distribution Datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;TinyImageNet&lt;/li&gt;
      &lt;li&gt;LSUN&lt;/li&gt;
      &lt;li&gt;iSUN&lt;/li&gt;
      &lt;li&gt;Gaussian Noise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metrics&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;False Positive Rate at 95% True Positive Rate&lt;/li&gt;
      &lt;li&gt;Detection Error - minimum misclassification probability over all thresholds&lt;/li&gt;
      &lt;li&gt;Area Under the Receiver Operating Characteristic Curve&lt;/li&gt;
      &lt;li&gt;Area Under the Precision-Recall Curve&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ODIN outperforms the baseline across all datasets and all models by a good margin.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Very simple and straightforward approach with theoretical justification under some conditions.&lt;/li&gt;
  &lt;li&gt;Limited to examples from Vision so can not judge its applicability for NLP tasks.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Ask Me Anything -  Dynamic Memory Networks for Natural Language Processing</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Ask-Me-Anything-Dynamic-Memory-Networks-for-Natural-Language-Processing"/>
   <updated>2017-07-09T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Ask Me Anything- Dynamic Memory Networks for Natural Language Processing</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.07285&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;input-module&quot;&gt;Input Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Concatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;question-module&quot;&gt;Question Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Similarly, feed the question to a GRU to obtain its representation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;episodic-memory-module&quot;&gt;Episodic Memory Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Episodic memory consists of an attention mechanism and a recurrent network with which it updates its memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;During each iteration, the network generates an episode &lt;em&gt;e&lt;/em&gt; by attending over the representation of the sentences, question and the previous memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The episodic memory is updated using the current episode and the previous memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Depending on the amount of supervision available, the network may perform multiple passes. eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass. For others, a fixed number of passes are made.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple passes allow the network to perform transitive inference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-mechanism&quot;&gt;Attention Mechanism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given the input representation &lt;em&gt;c&lt;/em&gt;, memory &lt;em&gt;m&lt;/em&gt; and question &lt;em&gt;q&lt;/em&gt;, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A separate GRU encodes the input representation and weights it by the attention.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Final state of the GRU is fed to the answer module.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;answer-module&quot;&gt;Answer Module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Use a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There are two possible losses:
    &lt;ul&gt;
      &lt;li&gt;Cross-entropy loss of the predicted answer (all datasets)&lt;/li&gt;
      &lt;li&gt;Cross-entropy loss of the attention supervision (for datasets like bAbI)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;question-answering&quot;&gt;Question Answering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bAbI Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For most tasks, DMN either outperforms or performs as good as Memory Networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;text-classification&quot;&gt;Text Classification&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stanford Sentiment Treebank Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DMN outperforms all the baselines for both binary and fine-grained sentiment analysis.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequence-tagging&quot;&gt;Sequence Tagging&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Wall Street Journal Dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DMN archives state of the art accuracy of 97.56%&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multiple passes help in reasoning tasks but not so much for sentiment/POS tags.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted. A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Alternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again. So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed. If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>One Model To Learn Them All</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/One-Model-To-Learn-Them-All"/>
   <updated>2017-07-01T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/One Model To Learn Them All</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The current trend in deep learning is to design, train and fine tune a separate model for each problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-philosophy&quot;&gt;Design Philosophy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The joint representation is to be of variable size.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Different tasks from the same domain share the modality net.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder and decoder use the following computational blocks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Convolutional Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Attention Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Multihead, dot product based attention mechanism.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Mixture-of-Experts (MoE) Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For further details, refer the &lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Encoder&lt;/strong&gt; consists of 6 conv blocks with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I/O mixer&lt;/strong&gt; consists of an attention block and 2 conv blocks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt; consists of 4 blocks of convolution and attention with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Modality Nets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Language Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Input is the sequence of tokens ending in a termination token.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This sequence is mapped to correct dimensionality using a learned embedding.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt; and &lt;strong&gt;Categorical Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Uses residual convolution blocks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Similar to the exit flow for &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception Network&lt;/a&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Audio Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ speech corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ImageNet dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;COCO image captioning dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ parsing dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-German translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-English translation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-French translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-French translation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experimental section is not very rigorous with many details skipped (would probably be added later).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While MultiModel does not beat the state of the art models, it does outperform some recent models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Two/Too Simple Adaptations of Word2Vec for Syntax Problems</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Two-Too-Simple-Adaptations-of-Word2Vec-for-Syntax-Problems"/>
   <updated>2017-06-26T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Two-Too Simple Adaptations of Word2Vec for Syntax Problems</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In the original Skip-Gram setting, the model predicts the &lt;em&gt;2c&lt;/em&gt; words in the context window (&lt;em&gt;c&lt;/em&gt; is the size of the context window). But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.&lt;/li&gt;
  &lt;li&gt;Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.&lt;/li&gt;
  &lt;li&gt;The paper proposes to use a set of &lt;em&gt;2c&lt;/em&gt; matrices each for a different word in the context window for both Skip-Gram and CBOW models.&lt;/li&gt;
  &lt;li&gt;This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.&lt;/li&gt;
  &lt;li&gt;The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Decomposable Attention Model for Natural Language Inference</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Decomposable-Attention-Model-for-Natural-Language-Inference"/>
   <updated>2017-06-17T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Decomposable Attention Model for Natural Language Inference</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.&lt;/li&gt;
  &lt;li&gt;Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01933&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given two sentences &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;, the model has to predict whether they have an “entailment” relationship, “neutral” relationship or “contradiction” relationship.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;All the words are mapped to their corresponding word vector representation. In subsequent steps, “word” refers to the word vector representation of the actual word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attend&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For each word &lt;em&gt;i&lt;/em&gt; in &lt;strong&gt;a&lt;/strong&gt; and &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;, obtain unnormalized attention weights *e(i, j)=F(i)&lt;sup&gt;T&lt;/sup&gt;F(j) where F is a feed-forward neural network.&lt;/li&gt;
      &lt;li&gt;For &lt;em&gt;i&lt;/em&gt;, compute a β&lt;sub&gt;i&lt;/sub&gt; by performing softmax-like normalization of &lt;em&gt;j&lt;/em&gt; using &lt;em&gt;e(i, j)&lt;/em&gt; as the weight and normalizing for all words &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;β&lt;sub&gt;i&lt;/sub&gt; captures the subphrase in &lt;strong&gt;b&lt;/strong&gt; that is softly aligned to &lt;em&gt;a&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Similarly compute α&lt;sub&gt;j&lt;/sub&gt; for &lt;em&gt;j&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compare&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Create two set of comparison vectors, one for &lt;strong&gt;a&lt;/strong&gt; and another for &lt;strong&gt;b&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;For &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;1, i&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(i, β&lt;sub&gt;i&lt;/sub&gt;)).&lt;/li&gt;
      &lt;li&gt;Similarly for &lt;strong&gt;b&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;2, j&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(j, α&lt;sub&gt;j&lt;/sub&gt;))&lt;/li&gt;
      &lt;li&gt;G is another feed-forward neural network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aggregate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Aggregate over the two set of comparison vectors to obtain &lt;strong&gt;v&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;v&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Feed the aggregated results through the final classifier layer.&lt;/li&gt;
      &lt;li&gt;Multi-class cross-entropy loss function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-complexity&quot;&gt;Computational Complexity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Computationally, the proposed model is asymptotically as good as LSTM with attention.&lt;/li&gt;
  &lt;li&gt;Assuming that dimensionality of word vectors &amp;gt; length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.&lt;/li&gt;
  &lt;li&gt;Further, the model has the advantage of being parallelizable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.&lt;/li&gt;
  &lt;li&gt;Adding intra-sentence attention further improve the test accuracy by 0.5 percent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation. &lt;a href=&quot;https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs&quot;&gt;Quora Duplicate Question Detection Challenege&lt;/a&gt;  would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Fast and Accurate Dependency Parser using Neural Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/A-Fast-and-Accurate-Dependency-Parser-using-Neural-Networks"/>
   <updated>2017-06-03T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/A Fast and Accurate Dependency Parser using Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.&lt;/li&gt;
  &lt;li&gt;Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;description-of-the-system&quot;&gt;Description of the system&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The system described in the paper uses &lt;a href=&quot;http://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-056-R1-07-027&quot;&gt;&lt;strong&gt;arc-standard&lt;/strong&gt; system&lt;/a&gt; (a greedy, transition-based dependency parsing system).&lt;/li&gt;
  &lt;li&gt;Words, POS tags and arc labels are represented as d dimensional vectors.&lt;/li&gt;
  &lt;li&gt;S&lt;sup&gt;w&lt;/sup&gt;, S&lt;sup&gt;t&lt;/sup&gt;, S&lt;sup&gt;l&lt;/sup&gt; denote the set of words, POS and labels respectively.&lt;/li&gt;
  &lt;li&gt;Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.&lt;/li&gt;
  &lt;li&gt;Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such.&lt;/li&gt;
  &lt;li&gt;Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).&lt;/li&gt;
  &lt;li&gt;Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.&lt;/li&gt;
  &lt;li&gt;Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.&lt;/li&gt;
  &lt;li&gt;This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.&lt;/li&gt;
  &lt;li&gt;L2-regularization term is also added to the loss.&lt;/li&gt;
  &lt;li&gt;During inference, a greedy decoding strategy is used and transition with the highest score is chosen.&lt;/li&gt;
  &lt;li&gt;The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;English Penn Treebank (PTB)&lt;/li&gt;
      &lt;li&gt;Chinese Penn Treebank (CTB)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two dependency representations used:
    &lt;ul&gt;
      &lt;li&gt;CoNLL Syntactic Dependencies (CD)&lt;/li&gt;
      &lt;li&gt;Stanford Basic Dependencies (SD)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Metrics:
    &lt;ul&gt;
      &lt;li&gt;Unlabeled Attached Scores (UAS)&lt;/li&gt;
      &lt;li&gt;Labeled Attached Scores (LAS)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Benchmarked against:
    &lt;ul&gt;
      &lt;li&gt;Greedy arc-eager parser&lt;/li&gt;
      &lt;li&gt;Greedy arc-standard parser&lt;/li&gt;
      &lt;li&gt;Malt-Parser&lt;/li&gt;
      &lt;li&gt;MSTParser&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;The system proposed in the paper outperforms all other parsers in both speed and accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Cube function gives a 0.8-1.2% improvement over tanh.&lt;/li&gt;
  &lt;li&gt;Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.&lt;/li&gt;
  &lt;li&gt;Using POS and labels gives an improvement of 1.7% and 0.4% respectively.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Neural Module Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Neural-Module-Networks"/>
   <updated>2017-05-23T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Neural Module Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For the task of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;Visual Question Answering&lt;/a&gt;, decompose a question into its linguistic substructures and train a neural network module for each substructure.&lt;/li&gt;
  &lt;li&gt;Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.&lt;/li&gt;
  &lt;li&gt;Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.&lt;/li&gt;
  &lt;li&gt;The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.02799&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Questions tend to be compositional.&lt;/li&gt;
  &lt;li&gt;Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.&lt;/li&gt;
  &lt;li&gt;Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-module-network-for-vqa&quot;&gt;Neural Module Network for VQA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Training samples of form &lt;em&gt;(w, x, y)&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;w&lt;/em&gt; - Natural Language Question&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;x&lt;/em&gt; - Images&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;y&lt;/em&gt; - Answer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model specified by collection of modules &lt;em&gt;{m}&lt;/em&gt; and a network layout predictor &lt;em&gt;P&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model instantiates a network based on &lt;em&gt;P(w)&lt;/em&gt; and uses that to encode a distribution &lt;em&gt;P(y|w, x, model_params)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;modules&quot;&gt;Modules&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find: Finds objects of interest.&lt;/li&gt;
  &lt;li&gt;Transform: Shift regions of attention.&lt;/li&gt;
  &lt;li&gt;Combine: Merge two attention maps into a single one.&lt;/li&gt;
  &lt;li&gt;Describe: Map a pair of attention and input image to a distribution over the labels.&lt;/li&gt;
  &lt;li&gt;Measure: Map attention to a distribution over the labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;natural-language-question-to-networks&quot;&gt;Natural Language Question to Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Map question to the layout which specifies the set of modules and connections between them.&lt;/li&gt;
  &lt;li&gt;Assemble the final network using the layout.&lt;/li&gt;
  &lt;li&gt;Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.&lt;/li&gt;
  &lt;li&gt;eg “what is the colour of the truck?” becomes “colour(truck)”&lt;/li&gt;
  &lt;li&gt;The symbolic representation is mapped to a layout:
    &lt;ul&gt;
      &lt;li&gt;All leaves become &lt;em&gt;find&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All internal nodes become &lt;em&gt;transform/combine&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All root nodes become &lt;em&gt;describe/measure&lt;/em&gt; module.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;answering-natural-language-question&quot;&gt;Answering Natural Language Question&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Final model combines output from a simple LSTM question encoder with the output of the neural module network.&lt;/li&gt;
  &lt;li&gt;This helps in modelling the syntactic and semantic regularities of the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Since some modules are updated more frequently than others, adaptive per weight learning rates are better.&lt;/li&gt;
  &lt;li&gt;The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).&lt;/li&gt;
  &lt;li&gt;Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.&lt;/li&gt;
  &lt;li&gt;Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Making-the-V-in-VQA-Matter-Elevating-the-Role-of-Image-Understanding-in-Visual-Question-Answering"/>
   <updated>2017-05-14T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question.&lt;/li&gt;
  &lt;li&gt;For example, if the question starts with “Do you see a …”, it is more likely to be “yes” than “no”.&lt;/li&gt;
  &lt;li&gt;To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.&lt;/li&gt;
  &lt;li&gt;The authors present a balanced version of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;VQA dataset&lt;/a&gt; where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.&lt;/li&gt;
  &lt;li&gt;The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset-collection&quot;&gt;Dataset Collection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I’ which is similar to I but for which the answer to question Q is A’ (different from A).&lt;/li&gt;
  &lt;li&gt;To facilitate the search for I’, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A. In case none of the 24 images qualifies, the worker may select “not possible”.&lt;/li&gt;
  &lt;li&gt;In the second round, the workers were asked to answer Q for I’.&lt;/li&gt;
  &lt;li&gt;This 2-stage protocol results in a significantly more balanced dataset than the previous dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;observation&quot;&gt;Observation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.&lt;/li&gt;
  &lt;li&gt;Training on balanced dataset improves performance on the unbalanced dataset.&lt;/li&gt;
  &lt;li&gt;Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;counter-example-explanations&quot;&gt;Counter-example Explanations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.&lt;/li&gt;
  &lt;li&gt;Supervising signal is provided by the data collection procedure where humans pick the image I’ from the same set of candidate images.&lt;/li&gt;
  &lt;li&gt;For each image in the candidate set, compute the inner product of question-image embedding and answer embedding.&lt;/li&gt;
  &lt;li&gt;The K inner product values are passed through a fully connected layer to generate K scores.&lt;/li&gt;
  &lt;li&gt;Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).&lt;/li&gt;
  &lt;li&gt;The proposed explanation model achieves a recall@5 of 43.49%&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Conditional Similarity Networks</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Conditional-Similarity-Networks"/>
   <updated>2017-05-07T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Conditional Similarity Networks</id>
   <content type="html">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.&lt;/li&gt;
  &lt;li&gt;But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.&lt;/li&gt;
  &lt;li&gt;What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.&lt;/li&gt;
  &lt;li&gt;The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.&lt;/li&gt;
  &lt;li&gt;It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vision.cornell.edu/se3/conditional-similarity-networks/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conditional-similarity-networks&quot;&gt;Conditional Similarity Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image, &lt;em&gt;x&lt;/em&gt;, learn a non-linear feature embedding &lt;em&gt;f(x)&lt;/em&gt; such that for any 2 images &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, the euclidean distance between &lt;em&gt;f(x&lt;sub&gt;1&lt;/sub&gt;)&lt;/em&gt; and &lt;em&gt;f(x&lt;sub&gt;2&lt;/sub&gt;)&lt;/em&gt; reflects their similarity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conditional-similarity-triplets&quot;&gt;Conditional Similarity Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given a triplet of images &lt;em&gt;(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;)&lt;/em&gt; and a condition &lt;em&gt;c&lt;/em&gt; (the notion of similarity), an oracle (say crowd) is used to determmine if &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is more similar to &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; or &lt;em&gt;x&lt;sub&gt;3&lt;/sub&gt;&lt;/em&gt; as per the given criteria &lt;em&gt;c&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;In general, for images &lt;em&gt;i, j, l&lt;/em&gt;, the triplet &lt;em&gt;t&lt;/em&gt; is ordered {i, j, l | c} if &lt;em&gt;i&lt;/em&gt; is more similar to &lt;em&gt;j&lt;/em&gt; than &lt;em&gt;l&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-from-triplets&quot;&gt;Learning From Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Define a loss function &lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;()&lt;/em&gt; to model the similarity structure over the triplets.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;(i, j, l) = max{0, D(i, j) - D(i, l) + h}&lt;/em&gt; where &lt;em&gt;D&lt;/em&gt; is the euclidean distance function and &lt;em&gt;h&lt;/em&gt; is the similarity scalar margin to prevent trivial solutions.&lt;/li&gt;
  &lt;li&gt;To model conditional similarities, masks &lt;em&gt;m&lt;/em&gt; are defined as &lt;em&gt;m = σ(β)&lt;/em&gt; where σ is the RELU unit and β is a set of parameters to be learnt.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt; denotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.&lt;/li&gt;
  &lt;li&gt;The euclidean function &lt;em&gt;D&lt;/em&gt; now computes the masked distance (&lt;em&gt;f(i, c)m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt;) between the two given images.&lt;/li&gt;
  &lt;li&gt;Two regularising terms are also added - L2 norm for &lt;em&gt;D&lt;/em&gt; and L1 norm for &lt;em&gt;m&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fonts dataset by Bernhardsson
    &lt;ul&gt;
      &lt;li&gt;3.1 million 64 by 64-pixel grey scale images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zappos50k shoe dataset
    &lt;ul&gt;
      &lt;li&gt;Contains 50,000 images of individual richly annotated shoes.&lt;/li&gt;
      &lt;li&gt;Characteristics of interest:
        &lt;ul&gt;
          &lt;li&gt;Type of the shoes (i.e., shoes, boots, sandals or slippers)&lt;/li&gt;
          &lt;li&gt;Suggested gender of the shoes (i.e., for women, men, girls or boys)&lt;/li&gt;
          &lt;li&gt;Height of the shoes’ heels (0 to 5 inches)&lt;/li&gt;
          &lt;li&gt;Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Initial model for the experiments is a ConvNet pre-trained on ImageNet&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standard Triplet Network&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learn from all available triplets jointly as if they have the same notion of similarity.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Set of Task Specific Triplet Networks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Train n separate triplet networks such that each is trained on a single notion of similarity.&lt;/li&gt;
      &lt;li&gt;Needs far more parameters and compute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - fixed disjoint masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.&lt;/li&gt;
      &lt;li&gt;Aims to learn a fully disjoint embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - learned masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learns all the components - conv filters, embedding and the masks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Refer paper for details on hyperparameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.&lt;/li&gt;
  &lt;li&gt;The learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.&lt;/li&gt;
  &lt;li&gt;Order of performance:
    &lt;ul&gt;
      &lt;li&gt;CSNs with learned masks &amp;gt; CSNs with fixed masks &amp;gt; Task-specific networks &amp;gt; standard triplet network.&lt;/li&gt;
      &lt;li&gt;Though CSNs with learned masks require more training data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.&lt;/li&gt;
  &lt;li&gt;This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple Baseline for Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/Simple-Baseline-for-Visual-Question-Answering"/>
   <updated>2017-04-28T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/Simple Baseline for Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/li&gt;
  &lt;li&gt;The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1512.02167.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text Features&lt;/strong&gt; - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Features&lt;/strong&gt; - Last layer activations from GoogLeNet.&lt;/li&gt;
  &lt;li&gt;Text features are concatenated with image features and fed into a softmax.&lt;/li&gt;
  &lt;li&gt;Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interpretation-of-the-model&quot;&gt;Interpretation of the model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive.&lt;/li&gt;
  &lt;li&gt;The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.&lt;/li&gt;
  &lt;li&gt;Question words generally can influence the answer given the bias in images occurring in COCO dataset.&lt;/li&gt;
  &lt;li&gt;Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.&lt;/li&gt;
  &lt;li&gt;The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.&lt;/li&gt;
  &lt;li&gt;While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>VQA-Visual Question Answering</title>
   <link href="https://shagunsodhani.github.io/papers-I-read/VQA-Visual-Question-Answering"/>
   <updated>2017-04-27T00:00:00-04:00</updated>
   <id>https://shagunsodhani.github.io/papers-I-read/VQA Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1505.00468v6&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vqa-challenge-and-workshop&quot;&gt;&lt;a href=&quot;http://www.visualqa.org/&quot;&gt;VQA Challenge and Workshop&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.&lt;/li&gt;
  &lt;li&gt;Interestingly, the second version is starting on 27th April 2017 (today).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;benefits-over-tasks-like-image-captioning&quot;&gt;Benefits over tasks like image captioning:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Simple, &lt;em&gt;n-gram&lt;/em&gt; statistics based methods are not sufficient.&lt;/li&gt;
  &lt;li&gt;Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.&lt;/li&gt;
  &lt;li&gt;Since only short answers are expected, evaluation is easier.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Created a new dataset of 50000 realistic, abstract images.&lt;/li&gt;
  &lt;li&gt;Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (&amp;gt;200K images) and abstract images.&lt;/li&gt;
  &lt;li&gt;Three questions per image and ten answers per question (along with their confidence) were collected.&lt;/li&gt;
  &lt;li&gt;The entire dataset contains over 760K questions and 10M answers.&lt;/li&gt;
  &lt;li&gt;The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-of-data-collection-methodology&quot;&gt;Highlights of data collection methodology&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Emphasis on questions that require an image, and not just common sense, to be answered correctly.&lt;/li&gt;
  &lt;li&gt;Workers were shown previous questions when writing new questions to increase diversity.&lt;/li&gt;
  &lt;li&gt;Answers collected from multiple users to account for discrepancies in answers by humans.&lt;/li&gt;
  &lt;li&gt;Two modalities supported:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Open-ended&lt;/strong&gt; - produce the answer&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;multiple-choice&lt;/strong&gt; - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-from-data-analysis&quot;&gt;Highlights from data analysis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Most questions range from four to ten words while answers range from one to three words.&lt;/li&gt;
  &lt;li&gt;Around 40% questions are “yes/no” questions.&lt;/li&gt;
  &lt;li&gt;Significant (&amp;gt;80%) inter-human agreement for answers.&lt;/li&gt;
  &lt;li&gt;The authors performed a study where human evaluators were asked to answer the questions without looking at the images.&lt;/li&gt;
  &lt;li&gt;Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.&lt;/li&gt;
  &lt;li&gt;The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline-models&quot;&gt;Baseline Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;random&lt;/strong&gt; selection&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;prior (“yes”)&lt;/strong&gt; - always answer as yes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;per Q-type prior&lt;/strong&gt; - pick the most popular answer per question type.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;nearest neighbor&lt;/strong&gt; - find the k nearest neighbors for the given (image, question) pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;I&lt;/strong&gt; - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;norm I&lt;/strong&gt; - : l2 normalized version of &lt;strong&gt;I&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Question Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q&lt;/strong&gt; - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q&lt;/strong&gt; - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Deeper LSTM Q&lt;/strong&gt; - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-Layer Perceptron (MLP)&lt;/strong&gt; - Combine image and question embeddings to obtain a single embedding.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q + I&lt;/strong&gt; method - concatenate BoW Q and I embeddings.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q + I, deeper LSTM Q + norm I&lt;/strong&gt; methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.&lt;/li&gt;
  &lt;li&gt;Cross-entropy loss with VGGNet parameters frozen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (&amp;gt;80% and &amp;gt;90% respectively).&lt;/li&gt;
  &lt;li&gt;The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.&lt;/li&gt;
  &lt;li&gt;Vision only model performs even worse than the model which always produces “yes” as the answer.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
