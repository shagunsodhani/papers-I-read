<!DOCTYPE html>
<html lang="en-us">
  
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      On the Difficulty of Warm-Starting Neural Network Training &middot; Papers I Read
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/poole.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/syntax.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/lanyon.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/css/style.css">
  <link rel="stylesheet" href="https://shagunsodhani.github.io/papers-I-read/public/font-awesome-4.7.0/css/font-awesome.css">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://shagunsodhani.github.io/papers-I-read/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="https://shagunsodhani.github.io/papers-I-read/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68140113-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-68140113-4');
</script>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>I am trying a new initiative - <i>A Paper A Week</i>. This blog will hold all the notes and summaries.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/archieve">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/tags">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read/archive/v1.0.0.zip">Download</a> -->
    <a class="sidebar-nav-item" href="https://github.com/shagunsodhani/papers-I-read">GitHub project</a>
    <a class="sidebar-nav-item" href="https://shagunsodhani.github.io/papers-I-read/atom.xml">Feed</a>
    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="https://shagunsodhani.github.io/papers-I-read/" title="Home">Papers I Read</a>
            <small>Notes and Summaries</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">On the Difficulty of Warm-Starting Neural Network Training</h1>
  <p class="entry-tags"><a href="https://shagunsodhani.github.io/papers-I-read/tags.html#2019" title="Pages tagged 2019" rel="tag">2019</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Incremental+Learning" title="Pages tagged Incremental Learning" rel="tag">Incremental Learning</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Online+Learning" title="Pages tagged Online Learning" rel="tag">Online Learning</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Transfer+Learning" title="Pages tagged Transfer Learning" rel="tag">Transfer Learning</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#AI" title="Pages tagged AI" rel="tag">AI</a> &bull; <a href="https://shagunsodhani.github.io/papers-I-read/tags.html#Empirical" title="Pages tagged Empirical" rel="tag">Empirical</a></p>
  <span class="post-date">18 Jun 2020</span>
  <h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>The paper considers learning scenarios where the training data is available incrementally (and not at once).</p>
  </li>
  <li>
    <p>For example, in some applications, new data is available periodically (e.g., latest news articles come out every day).</p>
  </li>
  <li>
    <p>The paper highlights that, in such scenarios, the conventional wisdom of “warm start” does not apply.</p>
  </li>
  <li>
    <p>When new data is available, it is better to train a new model from scratch than to update the model trained on previously available data.</p>
  </li>
  <li>
    <p>While the two setups lead to similar training performance, the randomly initialized model has a much better generalization performance.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.08475">Link to the paper</a></p>
  </li>
</ul>

<h2 id="basic-batch-updating">Basic Batch Updating</h2>

<ul>
  <li>
    <p>Create two random, equally-sized partitions of the training data.</p>
  </li>
  <li>
    <p>Train the model till convergence on the first half of the data. Then train the model on the entire dataset.</p>
  </li>
  <li>
    <p>Models: ResNet18, MLPs, Logisitic Regression (LR)</p>
  </li>
  <li>
    <p>Dataset: CIFAR10, CIFAR100, SVHN</p>
  </li>
  <li>
    <p>Optimizers: Adam, SGD</p>
  </li>
  <li>
    <p>Warm starting hurts generalization in all the cases.</p>
  </li>
  <li>
    <p>The effect is more pronounced in the case of ResNets and MLPs (compared to LR) and harder CIFAR 10 dataset (as compared to SVHN dataset).</p>
  </li>
</ul>

<h2 id="online-learning">Online Learning</h2>

<h3 id="passive-online-learning">Passive Online Learning</h3>

<ul>
  <li>
    <p>The model is given access to k new learning examples at each iteration.</p>
  </li>
  <li>
    <p>A warm started model reuses the previously initialized model and trains (till convergence) on the new batch of k items.</p>
  </li>
  <li>
    <p>A “randomly initialized” model is trained on all the examples (seen so far) from scratch.</p>
  </li>
  <li>
    <p>Dataset: CIFAR10</p>
  </li>
  <li>
    <p>Model: ResNet18</p>
  </li>
  <li>
    <p>As more training data becomes available, the generalization gap between the two setups increases, and warmup starts hurting generalization.</p>
  </li>
</ul>

<h3 id="active-online-learning">Active Online Learning</h3>

<ul>
  <li>
    <p>In this setup, the learner is trained to sample k new examples to add to the training dataset (using margin-based sampling).</p>
  </li>
  <li>
    <p>Like the previous setup, warmup strategy still hurts generalization.</p>
  </li>
</ul>

<h2 id="transfer-learning">Transfer Learning</h2>

<ul>
  <li>
    <p>Train a Resnet18 model on the CIFAR10 dataset and use this model to warm start training on the SVHN dataset.</p>
  </li>
  <li>
    <p>When a small percentage of the SVHN dataset is used, the setup resembles pretraining / transfer learning and performs better than training from scratch.</p>
  </li>
  <li>
    <p>As the percentage of the SVHN dataset increases, the warmup approach starts underperforming.</p>
  </li>
</ul>

<h2 id="overcoming-warm-start-problem">Overcoming warm start problem</h2>

<ul>
  <li>
    <p>ResNet18 model on CIFAR10 dataset</p>
  </li>
  <li>
    <p>When performing a hyper-parameter sweep over the learning rate and batch size, it is possible to train warm start models to reach the same generalization performance as training from scratch.</p>
  </li>
  <li>
    <p>Though, in that case, there are no computational savings as the warm-started models take about the same time (to converge) as the randomly initialized model.</p>
  </li>
  <li>
    <p>The increased training time indicates that the warm started model probably needs to forget the knowledge from previous training rounds.</p>
  </li>
  <li>
    <p>Warm start Resnet models, that generalize well, have a low correlation to their initialization stage (measured via Pearson correlation coefficient between the model weights).</p>
  </li>
  <li>
    <p>Generalization is damaged even when using a model trained on incomplete data for only a few epochs.</p>
  </li>
  <li>
    <p>For warm start models, the gradient (corresponding to the “new” data) is higher than that for randomly initialized models. This hints that regularisation may help to close the generalization gap. But in practice, regularization helps both the warmup and randomly initialized model.</p>
  </li>
  <li>
    <p>Warm starting only a few layers also does not close the gap.</p>
  </li>
  <li>
    <p>Adding some noise to the warm started model (with the motivation of having a partially random initialization) does help somewhat but also increases the training time.</p>
  </li>
  <li>
    <p>Motivating the problem as an instance of catastrophic forgetting, the authors use the EWC algorithm but report that using EWC hurts model performance.</p>
  </li>
  <li>
    <p>The paper does not propose a solution to the problem but provides a thorough analysis of the problem setup, which is quite useful for understanding the phenomenon itself.</p>
  </li>
</ul>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Hints-for-Computer-System-Design">
            Hints for Computer System Design
            <small>07 Jan 2022</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Synthesized-Policies-for-Transfer-and-Adaptation-across-Tasks-and-Environments">
            Synthesized Policies for Transfer and Adaptation across Tasks and Environments
            <small>29 Mar 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="https://shagunsodhani.github.io/papers-I-read/Deep-Neural-Networks-for-YouTube-Recommendations">
            Deep Neural Networks for YouTube Recommendations
            <small>22 Mar 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://shagunsodhani.github.io/papers-I-read/On-the-Difficulty-of-Warm-Starting-Neural-Network-Training"  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "/On-the-Difficulty-of-Warm-Starting-Neural-Network-Training"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://papers-i-read.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>

  </body>
</html>
